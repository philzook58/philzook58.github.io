I"Ét<p>An apologetic warning: I‚Äôm not sure this all compiles without adjustment. I‚Äôve been churning on this too long and my mind has wandered onto other things, so I‚Äôm just gonna get it out there.</p>

<p>Check out Ed Kmett‚Äôs Linear, Conal Elliott‚Äôs Vector-spaces, and Gibbons‚Äô Naperian Functors for more in this vague line (although done better maybe)</p>

<h2 id="function-vectors">Function Vectors</h2>

<p>What is a vector? Is it a thing you can sum and scalar multiply? That is often the starting point in textbooks. Is it a tuple of numbers? A thing you can inner product?</p>

<p>One reasonable operational definition is that it is a thing that you give a basis element and it gives back the component of the vector in this element, in other words a function</p>

<div class="language-haskell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kr">type</span> <span class="kt">Vec</span> <span class="n">basis</span> <span class="n">number</span> <span class="o">=</span> <span class="n">basis</span> <span class="o">-&gt;</span> <span class="n">number</span>
</code></pre></div></div>

<p>As functional programmers, this is great news. Vectors are basically functions. We have so many elegant combinators for functions.</p>

<p>Is there a way to convert an ordinary indexed vector into this form? Yes. It is just the function that takes the index and gives the number at that position</p>

<div class="language-haskell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">convert2Vec</span> <span class="n">v</span> <span class="o">=</span> <span class="nf">\</span><span class="n">i</span> <span class="o">-&gt;</span> <span class="n">v</span> <span class="o">!!</span> <span class="n">i</span>
</code></pre></div></div>

<p>or more tersely</p>

<div class="language-haskell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">convert2Vec</span> <span class="n">v</span> <span class="o">=</span> <span class="n">flip</span> <span class="p">(</span><span class="o">!!</span><span class="p">)</span>
</code></pre></div></div>

<p>and vice versa it is conceivable to perform the opposite, asking a function vector for every basis element and storing them in some container type. More on this in a bit.</p>

<p>We can easily add functions and scalar multiply functions and we have a zero vector.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>vadd v w = \x -&gt; (v x) + (w x)
smult s w = \x -&gt; s * (w x)
vzero = const 0
</code></pre></div></div>

<p>The direct sum and direct product of function vectors can be also be easily built, which I think is quite cool.</p>

<div class="language-haskell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">directsum</span> <span class="o">::</span> <span class="kt">Vec</span> <span class="n">a</span> <span class="n">c</span> <span class="o">-&gt;</span> <span class="kt">Vec</span> <span class="n">b</span> <span class="n">c</span> <span class="o">-&gt;</span> <span class="kt">Vec</span> <span class="p">(</span><span class="kt">Either</span> <span class="n">a</span> <span class="n">b</span><span class="p">)</span> <span class="n">c</span>
<span class="n">directsum</span> <span class="o">=</span> <span class="n">either</span>

<span class="n">directproduct</span> <span class="o">::</span> <span class="p">(</span><span class="kt">Num</span> <span class="n">c</span><span class="p">)</span> <span class="o">=&gt;</span> <span class="kt">Vec</span> <span class="n">a</span> <span class="n">c</span> <span class="o">-&gt;</span> <span class="kt">Vec</span> <span class="n">b</span> <span class="n">c</span> <span class="o">-&gt;</span> <span class="kt">Vec</span> <span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span> <span class="n">c</span>
<span class="n">directproduct</span> <span class="n">u</span> <span class="n">v</span> <span class="o">=</span> <span class="nf">\</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="p">(</span><span class="n">u</span> <span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">v</span> <span class="n">y</span><span class="p">)</span> 
</code></pre></div></div>

<h3 id="representable-functors">Representable Functors</h3>
<p>What about vectors data types that aren‚Äôt literally function types? Well there is a way to transform back and forth between these vectors and functions.</p>

<p>To say that a vector-like thing is basically <code class="language-plaintext highlighter-rouge">(-&gt;) basis</code> is to say that vectors are Representable functors with the Representation type given by the basis. These are very special functors that inherit lots of properties.</p>

<p>There is an interesting idea that perhaps Rep should be called Ln. In Haskell, there are sum, product, and function types. There is funky algebra for these types. Let‚Äôs suppose you had a couple finite enums which you took the sums and products of. How do you calculate the number of possible enum values now? It turns out, very aptly, that you translate Sum types into $+$ and Product types into $\times$. For function types, if you think about it <code class="language-plaintext highlighter-rouge">(a -&gt; b)</code> gets counted as $b^a$. This is because you have b possible choices for every value of a that you give the function.
Product types have sum types as their representation type. Just like the log of a product is the sum of the logs of the factors. $\ln(abc) = \ln(a) + \ln(b) + \ln(c)$.</p>

<p>Vectors are basically big ol‚Äô Product types holding many numbers and hence they are represented by a sum type with as many options (index values) as numbers the vector holds.</p>

<h3 id="dual-vectors">Dual Vectors</h3>

<p>The Dual of vectors are linear valued functions of vectors. Mathematicians freak the hell out about dual vectors, so let‚Äôs give them the benefit of the doubt and take a look. They are machines you give vectors and they output numbers. Sometimes dual vectors are called one-forms.</p>

<div class="language-haskell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kr">type</span> <span class="kt">Dual</span> <span class="n">v</span> <span class="n">number</span> <span class="o">=</span> <span class="n">v</span> <span class="o">-&gt;</span> <span class="n">number</span>  
</code></pre></div></div>

<p>This type unfortunately does not enforce linearity. We are indeed free to square elements if you so wish. Please don‚Äôt.</p>

<p>Given that vectors are functions, the Dual is an interesting higher-order function</p>

<p>Taking the Dual converts the basis type from contravariant to covariant position. 
Contravariant functors consume their held objects while covariant functors in some sense produce or hold them. 
Every time you cross into the arguments of a higher order function, you flip between contravariant and covariant. Or more mechanically, every time you cross a <code class="language-plaintext highlighter-rouge">)</code> and then an <code class="language-plaintext highlighter-rouge">-&gt;</code> going from right to left you flip between contravariant and covariant position. Here are some examples.</p>

<div class="language-haskell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kr">type</span> <span class="kt">Bar</span> <span class="o">=</span> <span class="p">(</span><span class="n">a</span><span class="o">-&gt;</span><span class="n">b</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">c</span>
<span class="kr">type</span> <span class="kt">Foo</span> <span class="o">=</span> <span class="p">((</span><span class="n">a</span><span class="o">-&gt;</span><span class="n">b</span><span class="o">-&gt;</span><span class="n">c</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">d</span><span class="p">)</span><span class="o">-&gt;</span><span class="n">e</span><span class="o">-&gt;</span><span class="n">f</span>
</code></pre></div></div>

<p>In type Bar <code class="language-plaintext highlighter-rouge">a</code> and <code class="language-plaintext highlighter-rouge">c</code> are in covariant position, and <code class="language-plaintext highlighter-rouge">b</code> is in contravariant position
In the type Foo, <code class="language-plaintext highlighter-rouge">f</code>, <code class="language-plaintext highlighter-rouge">c</code> are in covariant position,  <code class="language-plaintext highlighter-rouge">a</code>, <code class="language-plaintext highlighter-rouge">b</code>, <code class="language-plaintext highlighter-rouge">d</code>, and <code class="language-plaintext highlighter-rouge">e</code> are in contravariant position.</p>

<p>The dual vector will produce basis elements to hand to the vector and then manipulate the produced coefficients linearly. Check out this Phil Freeman talk for more <sup><a href="#freemanprofunctor">1</a></sup>.</p>

<p>In index notation, a vector is written with upper indices $v^i$ and dual vectors are written with lower indices $w_i$. There is an intriguing connection between the location of these indices (which correspond to our basis types) and the contravariant and covraint nature of the indices.</p>

<h3 id="dual-dual--id-and-the-yoneda-lemma">Dual Dual ~ Id and the Yoneda Lemma</h3>
<p>The word dual implies there ought to be only two things hanging around, and this is true. There is a sense in which the Dual of the Dual gives you back the original thing, like taking the transpose twice. Taking the Dual twice brings the basis back into contravariant position, so it is not insane that there is again a way to pull the whole thing back into the original form.</p>

<p>This is an example of the use of the Yoneda Lemma which states that
<code class="language-plaintext highlighter-rouge">forall t. (a -&gt; t) -&gt; f t ~ f a</code>, in this case <code class="language-plaintext highlighter-rouge">f = Id</code>.
The hand wavy idea is that if this thing needs to work for all types <code class="language-plaintext highlighter-rouge">t</code> then basically you can only be fmapping under an <code class="language-plaintext highlighter-rouge">f a</code> because there is nowhere else you‚Äôre gonna get that <code class="language-plaintext highlighter-rouge">t</code> from.</p>

<p>Using this relation we can show that</p>

<p><code class="language-plaintext highlighter-rouge">forall t. ((a -&gt; t) -&gt; t) -&gt; n ~ (a -&gt; n)</code></p>

<p>We can actually derive the equivalence using Djinn, a thingy that derives implementations from type signatures sometimes.
http://www.hedonisticlearning.com/djinn/</p>
<div class="language-haskell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">doubleDual</span> <span class="o">::</span> <span class="n">forall</span> <span class="n">t</span><span class="o">.</span> <span class="p">(((</span><span class="n">a</span> <span class="o">-&gt;</span> <span class="n">t</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">t</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">n</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="p">(</span><span class="n">a</span> <span class="o">-&gt;</span> <span class="n">n</span><span class="p">)</span>
<span class="n">doubledual</span> <span class="n">a</span> <span class="n">b</span> <span class="o">=</span> <span class="n">a</span> <span class="p">(</span><span class="nf">\</span><span class="n">c</span> <span class="o">-&gt;</span> <span class="n">c</span> <span class="n">b</span><span class="p">)</span>

<span class="n">doubleDual'</span> <span class="o">::</span> <span class="n">forall</span> <span class="n">t</span><span class="o">.</span> <span class="p">(</span><span class="n">a</span> <span class="o">-&gt;</span> <span class="n">n</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="p">((</span><span class="n">a</span> <span class="o">-&gt;</span> <span class="n">t</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">t</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">n</span><span class="p">)</span>
<span class="n">doubledual'</span> <span class="n">a</span> <span class="n">b</span> <span class="o">=</span> <span class="n">b</span> <span class="n">a</span>
</code></pre></div></div>

<p>To be perfectly frank, I‚Äôm not sure this matches up with the idea that Dual Dual ~ Id, because I don‚Äôt really think we have <code class="language-plaintext highlighter-rouge">forall t.</code>. We specialize t to some kind of number. But the concepts line up tantalizingly close. Maybe I‚Äôll update this when I‚Äôm more confident.</p>

<h3 id="linear-operators">Linear Operators</h3>
<p>The most straightforward implementation of a linear operator is just as a plain function from vectors to vectors.</p>

<div class="language-haskell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kr">type</span> <span class="kt">LinOp</span> <span class="n">a</span> <span class="n">b</span> <span class="n">c</span> <span class="o">=</span> <span class="kt">Vec</span> <span class="n">a</span> <span class="n">c</span> <span class="o">-&gt;</span> <span class="kt">Vec</span> <span class="n">b</span> <span class="n">c</span>
</code></pre></div></div>

<p>Compared to the Linear monad, it is disappointing that this does not enforce linearity in the slightest, but it is very simple.</p>

<p>Note again that <code class="language-plaintext highlighter-rouge">b</code> is in contravariant position and that <code class="language-plaintext highlighter-rouge">a</code> is in covariant position. In the index analogy, this would like like $A_a^b$</p>

<p>There is always the identity matrix which is simply give by the <code class="language-plaintext highlighter-rouge">id</code> function.</p>

<p>The zero matrix is given by <code class="language-plaintext highlighter-rouge">const (const 0)</code></p>

<p>The simple 2x2 block matrix has the type</p>

<div class="language-haskell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kr">type</span> <span class="kt">Block</span> <span class="n">a</span> <span class="n">b</span> <span class="n">c</span> <span class="n">d</span> <span class="n">n</span> <span class="o">=</span> <span class="kt">LinOp</span> <span class="p">(</span><span class="kt">Either</span> <span class="n">a</span> <span class="n">b</span><span class="p">)</span> <span class="p">(</span><span class="kt">Either</span> <span class="n">c</span> <span class="n">d</span><span class="p">)</span> <span class="n">n</span>
</code></pre></div></div>

<p>We can take the kronecker product and direct sum of matrices.</p>

<p><code class="language-plaintext highlighter-rouge">kron :: LinOp a b n -&gt; LinOp c d n -&gt; LinOp (a,c) (b,d) n</code>
<code class="language-plaintext highlighter-rouge">kron x y acf = \(b,d) -&gt; (x \a -&gt; (y \c-&gt; acf (a,c)) d) b</code>
<code class="language-plaintext highlighter-rouge">dsum :: LinOp a b n -&gt; LinOp c d n -&gt; LinOp (Either a c) (Either b d) n</code></p>

<p>This is a block matrix with zero matrices as the off diagonals.</p>

<p>There are also linear operaitions on the dual space.</p>

<div class="language-haskell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kr">type</span> <span class="kt">DualOp</span> <span class="n">a</span> <span class="n">b</span> <span class="n">c</span> <span class="o">=</span> <span class="kt">Dual</span> <span class="p">(</span><span class="kt">Vec</span> <span class="n">a</span> <span class="n">c</span><span class="p">)</span> <span class="n">c</span> <span class="o">-&gt;</span> <span class="kt">Dual</span> <span class="p">(</span><span class="kt">Vec</span> <span class="n">b</span> <span class="n">c</span><span class="p">)</span> <span class="n">c</span>
</code></pre></div></div>

<p>These are isomorphic to LinOps with the bases reversed.</p>

<p><code class="language-plaintext highlighter-rouge">DualOp a b c ~ LinOp b a c</code></p>

<p>witnessed by</p>

<p><code class="language-plaintext highlighter-rouge">dualOp :: DualOp a b c -&gt; LinOp b a c</code>
<code class="language-plaintext highlighter-rouge">dualOp a vb = \x -&gt; a (\va -&gt; va x) vb</code></p>

<p><code class="language-plaintext highlighter-rouge">dualOp' :: LinOp b a c -&gt; DualOp a b c</code>
<code class="language-plaintext highlighter-rouge">dualOp' a dva = dva . a</code></p>

<p>Linear Operators inherit scalar multiplication and addition from their vectorial pieces. The apparent deficiency of this approach compared to the matrix representation is that matrix decompositions are at best clunky. This does suck since most of the fun stuff about matrices involves inverting them or taking eigenvalues or something.</p>

<h3 id="metrics">Metrics</h3>
<p>A dual vector does not need a defined dot product or metric in order to consume vectors. What you do need to define a dot product for is to convert vectors to dual vectors and vice versa.</p>

<p>The dualizing operator is the metric.
The metric is an operator that takes two vectors and gives their dot product.</p>

<div class="language-haskell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kr">type</span> <span class="kt">Metric</span> <span class="n">basis</span> <span class="n">number</span> <span class="o">=</span> <span class="kt">Vec</span> <span class="n">basis</span> <span class="n">number</span> <span class="o">-&gt;</span> <span class="kt">Vec</span> <span class="n">basis</span> <span class="n">number</span> <span class="o">-&gt;</span> <span class="n">number</span>
</code></pre></div></div>

<p>The metric in index notation is $g_{ij}$.</p>

<p>Currying this operation shows that it also can be viewed as an operation that takes a vector and produces a dual vector. I could just as well write the Metric type as</p>

<div class="language-haskell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kr">type</span> <span class="kt">Metric</span> <span class="n">basis</span> <span class="n">number</span> <span class="o">=</span> <span class="kt">Vec</span> <span class="n">basis</span> <span class="n">number</span> <span class="o">-&gt;</span> <span class="kt">Dual</span> <span class="p">(</span><span class="kt">Vec</span> <span class="n">basis</span> <span class="n">number</span><span class="p">)</span> <span class="n">number</span>
</code></pre></div></div>

<p>What this shows if if only use up one of the metric‚Äôs indices we can convert a vector into a dual vector. $w_j=g_{ij}v^i$</p>

<p>For finite, enumerable types like <code class="language-plaintext highlighter-rouge">Bool</code> it is easy enough to see how we could build a standard metric. We have the ability to probe the entirety of the vector space by holding a list of every possible basis element.</p>

<p>For example</p>
<div class="language-haskell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">g_ij</span> <span class="o">::</span> <span class="kt">Number</span> <span class="n">a</span> <span class="o">=&gt;</span> <span class="kt">Metric</span> <span class="kt">Bool</span> <span class="n">a</span>
<span class="n">g_ij</span> <span class="n">v1</span> <span class="n">v2</span> <span class="o">=</span> <span class="p">(</span><span class="n">v1</span> <span class="kt">True</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">v2</span> <span class="kt">True</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">v1</span> <span class="kt">False</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">v2</span> <span class="kt">False</span><span class="p">)</span>
</code></pre></div></div>

<p>As a fun little exercise, see if you can derive <code class="language-plaintext highlighter-rouge">g_ij</code> for any type of the Enum and Bounded class.</p>

<p>For quasi-infinite basis types like a float (corresponding roughly to functions on the Real domain like $\sin(x)$ and stuff), we need to specify an integration routine. This integration routine is the metric. Dual vectors will have the integration routine already built into them.</p>

<p>The dual metric
<code class="language-plaintext highlighter-rouge">g^ij :: Dual (Vec a n) n -&gt; Dual (Vec a n) n -&gt; n</code></p>

<p>This dual metric is equivalent to
<code class="language-plaintext highlighter-rouge">dMetric' :: Dual a n -&gt; Vec a n</code>
via the double dual identity from before</p>

<p>With the metric we can define the transpose of a LinOp. We can post and precompose a LinOp with the two kinds of metrics.</p>

<div class="language-haskell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">transpose</span> <span class="o">::</span> <span class="kt">LinOp</span> <span class="n">a</span> <span class="n">b</span> <span class="n">c</span> <span class="o">-&gt;</span> <span class="kt">DualOp</span> <span class="n">a</span> <span class="n">b</span> <span class="n">c</span>
<span class="n">transpose</span> <span class="n">a</span> <span class="o">=</span> <span class="n">g_ij</span> <span class="o">.</span> <span class="n">a</span> <span class="o">.</span> <span class="n">g</span><span class="o">^</span><span class="n">ij</span>

<span class="n">transpose'</span> <span class="o">::</span> <span class="kt">LinOp</span> <span class="n">a</span> <span class="n">b</span> <span class="n">c</span> <span class="o">-&gt;</span> <span class="kt">LinOp</span> <span class="n">b</span> <span class="n">a</span> <span class="n">c</span>
<span class="n">transpose'</span> <span class="o">=</span>  <span class="n">convert</span> <span class="o">.</span> <span class="n">transpose</span> 
</code></pre></div></div>

<p>As a side note, it is interesting that defining the Dual vector as a representable functor requires giving the metric as the tabulate function. This almost works if not for the typeclass constraint that the functor held type is a scalar which makes it no longer a vanilla Representable instance.</p>

<h3 id="existential-types-and-vector-summation">Existential types and Vector Summation</h3>

<p>What the existential does is it does not let us do anything anymore with the Vec other than feed it into the Dual. The type <code class="language-plaintext highlighter-rouge">a</code> cannot unify with anything else. The index becomes a dummy index that we cannot actually access anymore. The <code class="language-plaintext highlighter-rouge">exists a.</code> is very similar to the summation symbol $\sum_a$ which also turns an index into a dummy index.</p>

<p>Not that <code class="language-plaintext highlighter-rouge">exists</code> is not actual Haskell notation. Sorry. You can do the equivalent but you need to construct funky less illuminating types.</p>

<p>Returning to the perspective of functors on the basis, we can find and intersting analogy that has been noted before. I am uncertain whether this is useful or just pure numerology, seeing faces in the tree trunks. So far, it has just been poison to me frankly.</p>

<p><code class="language-plaintext highlighter-rouge">(Vec b n, Vec b n -&gt; n)</code></p>

<p>One way of writing this again to match</p>

<div class="language-haskell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">exists</span> <span class="n">a</span><span class="o">.</span> <span class="p">(</span><span class="kt">Vec</span> <span class="n">a</span> <span class="n">number</span><span class="p">,</span> <span class="kt">Dual</span> <span class="p">(</span><span class="kt">Vec</span> <span class="n">a</span> <span class="n">number</span><span class="p">)</span> <span class="n">number</span><span class="p">)</span>
</code></pre></div></div>

<p>If we want to guarantee that eventually that first vector will get plugged into that second vector, we can apply an existential qualifier to the front of the type</p>

<p><code class="language-plaintext highlighter-rouge">exists b. (Vec b n, Vec b n -&gt; n)</code></p>

<p>Now this type can only be consumed by a polymorphic function.</p>

<p><code class="language-plaintext highlighter-rouge">forall b. (Vec b n, Vec b n -&gt; n) -&gt; whatever</code></p>

<p>And basically there is nowhere to get a <code class="language-plaintext highlighter-rouge">b</code> type other than to plug the vector into the dual vector. Hence the existentialized pairing is basically delayed application.</p>

<p>This type <code class="language-plaintext highlighter-rouge">(Vec a n, Vec b n -&gt; n)</code> is a Profunctor. 
<sup><a href="#piponiprofunctor">1</a></sup>
It has one type parameter in covraint and one in contravariant position. Matching the two parameters and taking the existential is called taking the coend of the profunctor and it sometimes written $\int^c P(c,c)$. The coend possesses some interesting properties that interplay with Yoneda Lemma in a manner very evocative of calculations in linear algebra.</p>

<p><sup><a href="#bartoszcoend">2</a></sup></p>

<p>Next Time: 
I don‚Äôt know when I‚Äôll do the next article in this particular series, so let‚Äôs sketch out some of the neat things I has intended at least.</p>

<p>String diagrams are a way of drawing monoidal categories. One monoidal category is that of Functors. Each string corresponds to a functor and the diagram is read left to right, with left to right corresponding to functor composition. Natural transformations are nodes, with particular kinds appearing as splitting of lines or bending of lines.</p>

<p>Bending of lines in particular denotes adjunctions. The disappearance of the two functors corresponds to the ability to natural transform that ordering into the identity functor, which you often just leave out of the diagram.</p>

<p>I think that the left adjoint to vectory functors is a tuple of the index type (a,-). This is because one definition of adjunctions is that it is the relationship that makes L a -&gt; b ~ a -&gt; R b. This is closely related to currying.</p>

<p><code class="language-plaintext highlighter-rouge">sequenceA</code> is a way of flipping functors. This may be important for getting the ‚Äúwrong‚Äù cap, the one that adjunction doesn‚Äôt get you.</p>

<p>Functor composition of vectory functors corresponds to the Kronecker product. You can look at the the typeclass instance for Representable and see that the composition of functors indices the pairing of their indices.</p>

<p>Likewise, the functor Product of two vectory functors is their Direct Sum. Think about that Representable = Ln relationship.</p>

<p>These Functory String diagrams corresponds very well with the notation in quasi Feynman diagrams and quantum circuits and Penrose notation, where side to side composition corresponds to the Kronecker product and nodes are matrices. Cups and caps correspond to usees of the upper index or lower index metric. A cup and cap can be straightened out into a striaght line because those two metrics are inverse.</p>

<p>I say quasi Feynman diagrams because Feynman diagrams are for bosons and fermions, so they don‚Äôt really use the Kronecker product per say, and the left-right up-down ordering means nothing. They are totally topological. Goldstone diagrams are a variant where time is represented in up-down order, so they get closer.</p>

<p>You can index into this composite functor using fmap, but it kills sharing, which is a huge problem. I‚Äôm working on it. One basic possibility is to not use ordinary functor composition.
Instead use</p>

<p><code class="language-plaintext highlighter-rouge">type Kron f g a = [(f a, g a)]</code></p>

<p>This is sort of a ‚ÄúFree‚Äù Kronecker product that keeps them unexpanded. You can still work with it. It is related to Piponi‚Äôs Linear Monad. It is definitely related to doing perturbation theory on top of free particles.</p>

<p>Another important improvement I think is to use sharing by only referencing by integer or something a globally stored vector to prevent dumb duplication of effort. Also Memoizing.</p>

<p>See you next time, hot dog.</p>

<p>My other blog has a place to comment or reach out to me on the Twitsphere:
<a href="http://www.philipzucker.com/functor-vector-part-2-function-vectors/">http://www.philipzucker.com/functor-vector-part-2-function-vectors/</a></p>

<h2 id="footnotes">Footnotes</h2>

<p><a name="piponiprofunctor">2</a> <a href="http://blog.sigfpe.com/2011/07/profunctors-in-haskell.html">http://blog.sigfpe.com/2011/07/profunctors-in-haskell.html</a></p>

<p><a name="bartoszcoend">1</a>: <a href="https://bartoszmilewski.com/2017/03/29/ends-and-coends/">https://bartoszmilewski.com/2017/03/29/ends-and-coends/</a></p>

<p><a name="freemanprofunctor">1</a>: <a href="https://www.youtube.com/watch?v=OJtGECfksds">https://www.youtube.com/watch?v=OJtGECfksds</a></p>

<p><a name="gibbonsnaperian">1</a>: <a href="https://www.cs.ox.ac.uk/people/jeremy.gibbons/publications/aplicative.pdf">https://www.cs.ox.ac.uk/people/jeremy.gibbons/publications/aplicative.pdf</a></p>

<p><a name="piponiyoneda">1</a>: <a href="http://blog.sigfpe.com/2006/11/yoneda-lemma.html">http://blog.sigfpe.com/2006/11/yoneda-lemma.html</a></p>

:ET