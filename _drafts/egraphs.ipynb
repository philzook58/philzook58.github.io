{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rulesets\n",
    "https://github.com/yihozhang/szalinski-egglog\n",
    "https://github.com/philzook58/egglog-rec\n",
    "halide ruler\n",
    "herbie\n",
    "egg suite\n",
    "egglog suite\n",
    "termination-comp\n",
    "hlint\n",
    "metatheory\n",
    "https://github.com/yihozhang/egglog-pointer-analysis-benchmark\n",
    "KAT\n",
    "\n",
    "Lift/Rise?\n",
    "speq?\n",
    "Isaria\n",
    "casc ueq\n",
    "smtcomp maybe\n",
    "tensat\n",
    "glenside\n",
    "\n",
    "cvc5 has the RARE rule files\n",
    "https://github.com/cvc5/cvc5/blob/main/src/theory/bv/rewrites\n",
    "\n",
    "https://github.com/Z3Prover/z3/tree/master/src/ast/rewriter\n",
    "https://github.com/Z3Prover/z3/blob/master/src/ast/rewriter/rewriter.txt pretty interesting. Rewrite returns codes saying fail, done, rewritecdepth1 2 3 or full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Egglog\n",
    "Upstreaming stuff?\n",
    "using python egglog\n",
    "\n",
    "https://github.com/yihozhang/Halide\n",
    "\n",
    "proof kernel\n",
    "proof system for egg\n",
    "congruence closure rules\n",
    "z3 new proof format\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zipper Egraph\n",
    "There is an interesting trick for encoding\n",
    "\n",
    "Manually dealing with the trick kind of sucked.\n",
    "Having baked in support is kind of useful seeming.\n",
    "\n",
    "Contexts are probably orderable.\n",
    "Colored or contextual egraphs\n",
    "Maximal context.\n",
    "\n",
    "A hash cons has too much sharing.\n",
    "Hash conses in a certain sense are broken with respect to subtle context issues. Hash consing modulo alpha is perhaps an ill posed concept in this sense. The hash cons shares stuff that has no right to be shared. If you use locally nameless, then open trees under lambdas don't share though.\n",
    "\n",
    "You can break the sharing by giving every node in a tree a unique marker.\n",
    "\n",
    "\"Skolemizing\" the unique marker gives us zipper contexts.\n",
    "\n",
    "forall x, C1[x] -> C2[x]  gives us context rewrites (context equivalences)\n",
    "forall C, C[x] -> C[x] gives us context free rewrites (an observational equivalence)\n",
    "C[x] -> C[y] if C and x something give us guarded rewrites.\n",
    "\n",
    "A pattern in some sense is kind of a mini chunk of context spelled out.\n",
    "\n",
    "There was an idea for marking delimitting operations. Binder forms. if then elses.\n",
    "Delimitter forms are the ones that actually might be interesting from contexzt perspective, everything else is assumed not interesting.\n",
    "\n",
    "Paths from top instead of full zipper.\n",
    "\n",
    "\n",
    "\n",
    "Ctx are probably a different datatype than terms, so we could give them ctx ids rather than terms ids.\n",
    "\n",
    "ctx could also be a regular term instead of an egraphified thing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HashCons():\n",
    "    def __init__(self):\n",
    "\n",
    "enodes = {(ctx, \"add\", args) : 10} # every enode has a ctx\n",
    "\n",
    "\n",
    "class EGraph():\n",
    "    def __init__(self):\n",
    "    \n",
    "    def insert_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# autodiff egraph\n",
    "I think you can autodiff thorugh an egraph\n",
    "\n",
    "the tape is akin to acylic egraph\n",
    "\n",
    "x - x should require no autodiffing.\n",
    "\n",
    "x + x should be 2*x\n",
    "\n",
    "acyclic egraph approach enables domain specific jit. Online results. No fixed point needed\n",
    "pytroch selling point. The compilation step is an hindrance\n",
    "\n",
    "\n",
    "## Interval egraph\n",
    "Same for interval airthmetic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brute Force\n",
    "A set of terms. Apply equations in all ways. Dumb paramodulation.\n",
    "This allows lambdas, built in rules, etc.\n",
    "\n",
    "With a little beam search who knows.\n",
    "\n",
    "Could maintain a best set of terms for each eclass.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Equational Search\n",
    "In the kestrel 2023 egraphs work, they used performance guided extraction. They would porfile and then rexextract something a little different.\n",
    "\n",
    "Great idea.\n",
    "\n",
    "But, if we are doing that, do we really need an egraph? We could directly wiggle the terms.\n",
    "For bidirectional rules, we don't lose anything.\n",
    "\n",
    "For rules that lose pattern variables, the other direction needs to generate them\n",
    "\n",
    "1. keep them stored in the rule\n",
    "2. Keep them stored in some term bank,   maybe hash cons. Maybe egraph\n",
    "3. copy the term when we apply a non reversible rule.\n",
    "\n",
    "irreversible rules also tend to be really good. Deleting chunks of your tree.\n",
    "\n",
    "Also if we need to magic terms out of thin air (synthesis, rule, irreversible rules), random generation is an alternative to enumeration.\n",
    "\n",
    "\n",
    "Destructive rewriting is fast, buyt also easier to shallowly embed.\n",
    "Terms are easier to deal with binders.\n",
    "\n",
    "Sketchs seem like they'd still work. Could bias towards chunks that are meeting sketch\n",
    "simulated annealing\n",
    "\n",
    "Extraction is a lgobal optimization process. That's pretty cool.  The thing is, objectives are usually a little off. We know roughly we want terms small usually, but the exact weights are unclear. You hasve to sometimes mangle or approximate your objective to fit it into a framework like ILP.\n",
    "If this approximation is worse than the approximation that comes from a non optimal heurstic algorithm, there is no point.\n",
    "\n",
    "heuristic work really well. They are also usually pretty simple and explicable. You almost can't write papers on them.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Stochastic_optimization\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ruler\n",
    "So the basic idea of ruler is\n",
    "\n",
    "1. Have an interpretation into z3\n",
    "2. enumerate terms\n",
    "3. maybe concretly sample them for cadnaidate matches\n",
    "4. use z3 to check if they are equal\n",
    "5. if they are equal, add them to the set of rewrite rules. \n",
    "6. Run rules to compress egraph\n",
    "\n",
    "\n",
    "https://pypy.org/posts/2024/07/finding-simple-rewrite-rules-jit-z3.html\n",
    "\n",
    "There was also that UW paper\n",
    "\n",
    "https://arxiv.org/abs/2405.06127 https://aha.stanford.edu/generalizing-rewrite-rule-synthesis\n",
    "\n",
    "https://inst.eecs.berkeley.edu/~cs294-260/sp24/projects/charleshong/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "uf = {}\n",
    "ts = set()\n",
    "\n",
    "\n",
    "def rw(ts, rules):\n",
    "\n",
    "\n",
    "def expand(ts):\n",
    "    newts = set()\n",
    "    for t1 in ts:\n",
    "        newts.add(-t)\n",
    "        for t2 in ts:\n",
    "            newts.add(t1 + t2)\n",
    "\n",
    "def samples(ts):\n",
    "    {t : []}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Egraphs Modulo Theories\n",
    "\n",
    "Overlap and category theory. Graph rewriting is described using these puishout things and homomorphisms.\n",
    "String, ground term and rgular term, ground multiset can also be put in these terms?\n",
    "https://algebraicjulia.github.io/AlgebraicRewriting.jl/dev/generated/full_demo/\n",
    "What's the schema for a term? child -> parent ?\n",
    "A pattern match is a homomorphism. I know that via the database search method for finding patterns\n",
    "Kruskal theorem and that tree homomorphism embedding thing. Nah. That's jnot the pattern match embedding. That allows compression of edges\n",
    "The substring relation forms a partial order. So could be considered a category.\n",
    "\n",
    "semiring KB. You can expand semirings into polynomials. But now you can't subtract. What should overlap mean?\n",
    "2AC seems like a semiring to me. But we're missing distributivity...\n",
    "overlap(x,y) = Ax + B = Cy + D\n",
    "\n",
    "\n",
    "overlap is anti unification kind of?\n",
    "\n",
    "\n",
    "\n",
    "Lattice are \"fancy\" union finds\n",
    "Different notions of union find give theories.\n",
    "\n",
    "\n",
    "\n",
    "Rob Lewis thesis. Ping ping ponging linear log ineqautions  https://arxiv.org/pdf/1404.4410 https://github.com/avigad/polya \n",
    "they did it from scratch? That seems wrong headed.\n",
    "\n",
    "\"Normalization\" in the presense of linear inequalities.\n",
    "Eh.\n",
    "Differenmce logic?\n",
    "\n",
    "\n",
    "What about the Non grs version? Did I try that?\n",
    "\n",
    "What about normalizing all lambdas to use v.\n",
    "\n",
    "a uninfind dict of things we're not equal to..\n",
    "`dif : dict[int, set[int]]`\n",
    "\n",
    "\n",
    "Rewrite rule lab has AC Kb\n",
    "E had AC?\n",
    "RRL did theorem proving\n",
    "And as AC symbol.\n",
    "Or as AC symbol.\n",
    "can do sat solving?\n",
    "RRL m,an page says x^3 = x for rings implies commutativity \n",
    "huet opppen 1980 has sum example?\n",
    "rrl discovered symbol < order on line\n",
    "\n",
    "\n",
    "declare operators to be transitive4 / left right divisble. What does that mean?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok. How can I start compressing stuff.\n",
    "\n",
    "Bottom up matching with containers\n",
    "FuyncDeclRef\n",
    "\n",
    "Theory as union find replacement\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdataclasses\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dataclass\n\u001b[1;32m      2\u001b[0m \u001b[38;5;129m@dataclass\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mFoo\u001b[39;00m():\n\u001b[1;32m      4\u001b[0m     A : \u001b[38;5;28mtype\u001b[39m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfoo\u001b[39m(\u001b[38;5;28mself\u001b[39m, x : Self\u001b[38;5;241m.\u001b[39mA) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self\u001b[38;5;241m.\u001b[39mA:\n",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m, in \u001b[0;36mFoo\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;129m@dataclass\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mFoo\u001b[39;00m():\n\u001b[1;32m      4\u001b[0m     A : \u001b[38;5;28mtype\u001b[39m\n\u001b[0;32m----> 5\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfoo\u001b[39m(\u001b[38;5;28mself\u001b[39m, x : \u001b[43mSelf\u001b[49m\u001b[38;5;241m.\u001b[39mA) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self\u001b[38;5;241m.\u001b[39mA:\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Self' is not defined"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class Foo():\n",
    "    A : type\n",
    "    def foo(self, x : Self.A) -> Self.A:\n",
    "        return x \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and has to be hashable and eq.\n",
    "class Container():\n",
    "    def rebuild(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class ENode():\n",
    "    decl: FuncDeclRef\n",
    "    args: list[EId]\n",
    "    def rebuild(self):\n",
    "        for a in self.args:\n",
    "            th.find(a.rebuild())\n",
    "    def children(self):\n",
    "        yield from self.args\n",
    "\n",
    "class CommNode():\n",
    "    def rebuild(self):\n",
    "        self.args.sort()\n",
    "# Comm node could also reify permutation such that it goes into group union find.\n",
    "# group invariance.\n",
    "\n",
    "class SetNode():\n",
    "    pass\n",
    "class MultiSetNode():\n",
    "    pass\n",
    "class PolyNode():\n",
    "    pass\n",
    "\n",
    "class EId():\n",
    "    \"\"\"eids aka ground objects\n",
    "    structured eids\n",
    "    \"\"\"\n",
    "    def overlap(x : Self, y : Self) -> Optional[Self]:\n",
    "        pass\n",
    "    def order(x : Self, y : Self) -> tuple[x,y]:\n",
    "        # do not call on equal\n",
    "    # hashable and eqable. This is structural equality\n",
    "\n",
    "class Atomic(EId):\n",
    "    eid : int\n",
    "class MultiSet(EId):\n",
    "    pass\n",
    "class Poly(EId):\n",
    "    pass\n",
    "class Linear(EId):\n",
    "    pass\n",
    "class Sequence(EId):\n",
    "    pass\n",
    "class PrefixString(EId): # or suffix. This is basically and enode? fabc = fabc. Maybe useful for partial application\n",
    "    pass\n",
    "class \n",
    "class GroupAct(EId):\n",
    "    g : Group#oid\n",
    "    x : EId\n",
    "    def overlap(self,y):\n",
    "        z = self.x.overlap(y.x)\n",
    "        if z is None:\n",
    "            return None\n",
    "        else:\n",
    "            return GroupAct(self.g.ident, z)\n",
    "            #return GroupAct(self.g / y.g, z)\n",
    "\n",
    "# hmm. recursive Eids.\n",
    "# recursive calls of overlap and order.\n",
    "\n",
    "# Haskell style. Functors?\n",
    "\n",
    "class EqTheory():\n",
    "    eid : type\n",
    "    def makeset(self): # maybe makeset should be global atomic identifiers, shared across all theories\n",
    "        pass\n",
    "    def union(self, x, y):\n",
    "        pass\n",
    "    def find(self, x): #normalize\n",
    "        pass\n",
    "    def rebuild(self): # complete\n",
    "        self.huet()\n",
    "\n",
    "    #def overlap(self, x, y):\n",
    "    #   pass\n",
    "    #def order(self, x, y): maaaaaybe order should be part of the theory rather than eid\n",
    "    #    pass\n",
    "    \n",
    "    def huet(self):\n",
    "\n",
    "\n",
    "# UF\n",
    "class AtomicTheory():\n",
    "    def union():\n",
    "        pass\n",
    "\n",
    "class ACTheory():\n",
    "    pass\n",
    "\n",
    "class LinearTheory():\n",
    "    pass\n",
    "\n",
    "class GrobnerTheory():\n",
    "    pass\n",
    "\n",
    "class ProductTheory():\n",
    "    #ths: list[EqTheory]\n",
    "    th1 : EqTheory\n",
    "    th2 : EqTheory\n",
    "    def union(self, x, y):\n",
    "        x1,x2 = x\n",
    "        y1,y2 = y\n",
    "        self.th1.union(x1, y1)\n",
    "        self.th2.union(x2, y2)\n",
    "    def union(self, x, y):\n",
    "        pass\n",
    "\n",
    "class SumTheory():\n",
    "    ths : list[EqTheory]\n",
    "    transfers : dict[EId, EId] \n",
    "    def union(self, x, y):\n",
    "        xtag, x1 = x\n",
    "        ytag, y1 = y\n",
    "        if xtag == ytag:\n",
    "            self.ths[xtag].union(x1, y1)\n",
    "        else:\n",
    "            yy = self.ths[xtag].makeset()\n",
    "            self.ths[xtag].union(x1, yy)\n",
    "            # plus mediating union find?\n",
    "            # everything needs the ability to register\n",
    "            # an ordering on theories.\n",
    "            # theory transfer tables\n",
    "    def rebuild():\n",
    "        for th in self.ths:\n",
    "            th.rebuild()\n",
    "            for x, y in self.transfers:\n",
    "\n",
    "\"\"\"\n",
    "So we have transfers which are rewrite rules from high theoiries to low theories.\n",
    "They also work in completion.\n",
    "So, look for overlaps.\n",
    "Complete them in order. Normalize trasnfers. Then push overlaps as unions\n",
    "\n",
    "The order is the sum order of the underlying orders\n",
    "We are using total orders well founded orders to guarantee the thing can't fail.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class ExpTheory():\n",
    "    base_th : EqTheory\n",
    "    ctx_ths : dict[EId, Theory]\n",
    "# sum theory of 2 AC theories for + and *\n",
    "# a union find is kind of a giant sum theory of units\n",
    "# theory to theory tables are the theory to theory rewrites\n",
    "# create\n",
    "\n",
    "class FuncDeclRef():\n",
    "    name: str\n",
    "    args: list[SortRef]\n",
    "    ret: SortRef\n",
    "    ctx: Context\n",
    "    def __init__(self, name, args, ret):\n",
    "        self.name = name\n",
    "        self.args = args\n",
    "        self.ret = ret\n",
    "    def __call__(self, *args):\n",
    "        pass\n",
    "    def __getitem(self, *args):\n",
    "        pass\n",
    "\n",
    "\n",
    "class SortRef():\n",
    "    sid : int\n",
    "    pass\n",
    "\n",
    "\n",
    "class EGraph():\n",
    "    sorts: list[SortRef]\n",
    "    nodes: list[SortRef, Container]\n",
    "    ths: dict[SortRef, EqTheory]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EGraph():\n",
    "    enodes:set[ExprRef]\n",
    "    uf:dict[ExprRef,ExprRef]\n",
    "    def __init__(self):\n",
    "        self.enodes = set() # not a table anymore because term holds it's own children, fun_sym,  and id \n",
    "        self.uf = {}\n",
    "        self.solver = z3.Solver()\n",
    "    def add_term(self, e:ExprRef):\n",
    "        if e not in self.enodes:\n",
    "            self.enodes.add(e)\n",
    "            self.uf[e] = e\n",
    "            if z3.is_app(e):\n",
    "                for c in e.children():\n",
    "                    self.add_term(c)\n",
    "            return e\n",
    "        else:\n",
    "            return self.find(e)\n",
    "    def find(self, e:ExprRef):\n",
    "        while not self.uf[e].eq(e):\n",
    "            e = self.uf[e]\n",
    "        return e\n",
    "    def union(self, e1:ExprRef, e2:ExprRef):\n",
    "        e1 = self.find(e1)\n",
    "        e2 = self.find(e2)\n",
    "        if not e1.eq(e2):\n",
    "            self.solver.add(e1 == e2) # assert_and_track? unsat core gives a form of proof.\n",
    "            self.uf[e1] = e2\n",
    "    def rebuild(self):\n",
    "        for e in self.enodes:\n",
    "            # congruence\n",
    "            e1 = e.decl()(*map(self.find, e.children()))\n",
    "            self.add_term(e1)\n",
    "            self.union(e, e1) # z3 already knows though.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "from z3 import *\n",
    "v = Int(\"v\")\n",
    "w = Int(\"w\")\n",
    "x1 = Lambda([v], Lambda([v], v))\n",
    "x2 = Lambda([v], Lambda([v], v))\n",
    "x3 = Lambda([w], Lambda([v], v))\n",
    "print(x1.eq(x2))\n",
    "print(x1.eq(x3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = Lambda([v], z3.substitute(Lambda([v], w), (w, v)))\n",
    "x2\n",
    "x2.eq(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# huh. It's even worse\n",
    "from cvc5.pythonic import *\n",
    "v = Int(\"v\")\n",
    "w = Int(\"w\")\n",
    "x1 = Lambda([v], Lambda([v], v))\n",
    "x2 = Lambda([v], Lambda([v], v))\n",
    "x3 = Lambda([w], Lambda([v], v))\n",
    "print(x1.eq(x2))\n",
    "print(x1.eq(x3))\n",
    "x2\n",
    "x1\n",
    "v.eq(w)\n",
    "Lambda([v], v).eq(Lambda([v],v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from z3 import *\n",
    "@dataclass\n",
    "class EGraph():\n",
    "    E: list[tuple(ExprRef, ExprRef)]\n",
    "    T: dict[SortRef]\n",
    "    def __init__(self):\n",
    "        self.E = []\n",
    "        self.T = defaultdict(set)\n",
    "        self.solver = Solver()\n",
    "    def add_term(self, t):\n",
    "        self.T[t.sort()].add(t)\n",
    "        if is_app(t):\n",
    "            for t in t.children():\n",
    "                self.add_term(t)\n",
    "    def union(self, t1, t2, reason=None):\n",
    "        # use solver for subsumption check rather than reducing.\n",
    "        self.solver.push()\n",
    "        self.solver.add(t1 != t2)\n",
    "        res = self.solver.check()\n",
    "        self.solver.pop()\n",
    "        if res == sat: # non redundant info\n",
    "            self.solver.assert_and_track(t1 == t2, ) # add and track?\n",
    "            self.E.append((t1,t2, reason)) # add reason\n",
    "    def rw(self, sorts, f):\n",
    "        for xs in product(*[self.T[sort] for sort in sort]):\n",
    "            lhs, rhs = f(xs)\n",
    "            self.solver.push()\n",
    "            # this avoids the reduction step to check if in self.T\n",
    "            self.solver.add(Not(Or(lhs == t for t in self.T[lhs.sort()]))\n",
    "            res = self.solver.check() # can give unsat core reason for why lhs == pat[xs]\n",
    "            self.solver.pop()\n",
    "            if res == unsat:\n",
    "                core = self.get_unsat_core()\n",
    "\n",
    "                self.union(lhs, rhs, reason=(\"rule\", core))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## smt parse\n",
    "souffle datalog grammar might also be nice.\n",
    "Or egglog0 grammar\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set-logic\n",
      "[QF_LIA]\n",
      "declare-fun\n",
      "p\n",
      "Int\n",
      "()\n",
      "Int\n",
      "<bound method FNode.function_name of p>\n",
      "[p]\n",
      "declare-fun\n",
      "q\n",
      "Int\n",
      "()\n",
      "Int\n",
      "<bound method FNode.function_name of q>\n",
      "[q]\n",
      "declare-fun\n",
      "x\n",
      "Bool\n",
      "()\n",
      "Bool\n",
      "<bound method FNode.function_name of x>\n",
      "[x]\n",
      "declare-fun\n",
      "y\n",
      "Bool\n",
      "()\n",
      "Bool\n",
      "<bound method FNode.function_name of y>\n",
      "[y]\n",
      "declare-fun\n",
      "f\n",
      "Bool -> Int\n",
      "()\n",
      "Bool -> Int\n",
      "<bound method FNode.function_name of f>\n",
      "[f]\n",
      "define-fun\n",
      "['.def_1', [], Bool, (x & y)]\n",
      "assert\n",
      "[(x -> (q < p))]\n",
      "check-sat\n",
      "[]\n",
      "push\n",
      "[1]\n",
      "assert\n",
      "[(y -> (p < q))]\n",
      "check-sat\n",
      "[]\n",
      "assert\n",
      "[(x & y)]\n",
      "check-sat\n",
      "[]\n",
      "pop\n",
      "[1]\n",
      "check-sat\n",
      "[]\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/pysmt/pysmt/blob/master/examples/smtlib.py\n",
    "from io import StringIO\n",
    "\n",
    "from pysmt.smtlib.parser import SmtLibParser\n",
    "\n",
    "\n",
    "# To make the example self contained, we store the example SMT-LIB\n",
    "# script in a string.\n",
    "DEMO_SMTLIB=\\\n",
    "\"\"\"\n",
    "(set-logic QF_LIA)\n",
    "(declare-fun p () Int)\n",
    "(declare-fun q () Int)\n",
    "(declare-fun x () Bool)\n",
    "(declare-fun y () Bool)\n",
    "(declare-fun f (Bool) Int)\n",
    "(define-fun .def_1 () Bool (! (and x y) :cost 1))\n",
    "(assert (=> x (> p q)))\n",
    "(check-sat)\n",
    "(push)\n",
    "(assert (=> y (> q p)))\n",
    "(check-sat)\n",
    "(assert .def_1)\n",
    "(check-sat)\n",
    "(pop)\n",
    "(check-sat)\n",
    "\"\"\"\n",
    "\n",
    "parser = SmtLibParser()\n",
    "\n",
    "script = parser.get_script(StringIO(DEMO_SMTLIB))\n",
    "\n",
    "for cmd in script:\n",
    "    print(cmd.name)\n",
    "    if cmd.name == \"declare-fun\":\n",
    "        f = cmd.args[0]\n",
    "        #print(dir(cmd.args[0]))\n",
    "        print(f.symbol_name())\n",
    "        print(f.symbol_type())\n",
    "        print(f.args())\n",
    "        print(f.get_type())\n",
    "        print(f.function_name)\n",
    "    elif cmd.name == \"assert\":\n",
    "        pass\n",
    "    elif cmd.name == \"check-sat\":\n",
    "        pass\n",
    "\n",
    "    print(cmd.args)\n",
    "print(\"*\"*50)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "PysmtTypeError",
     "evalue": "Trying to redefine symbol 'p' with a new type. Previous type was 'Int' new type is 'Bool'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPysmtTypeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 59\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m     49\u001b[0m formula_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124m(declare-fun p () Bool)\u001b[39m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;124m(declare-fun q () Bool)\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124m(extract (and p q))\u001b[39m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 59\u001b[0m parsed_clauses \u001b[38;5;241m=\u001b[39m \u001b[43mparse_horn_clauses\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformula_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m print_parsed_clauses(parsed_clauses)\n",
      "Cell \u001b[0;32mIn[68], line 8\u001b[0m, in \u001b[0;36mparse_horn_clauses\u001b[0;34m(formula_str)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_horn_clauses\u001b[39m(formula_str):\n\u001b[1;32m      7\u001b[0m     parser \u001b[38;5;241m=\u001b[39m SmtLibParser()\n\u001b[0;32m----> 8\u001b[0m     script \u001b[38;5;241m=\u001b[39m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_script\u001b[49m\u001b[43m(\u001b[49m\u001b[43mStringIO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformula_str\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m cmd \u001b[38;5;129;01min\u001b[39;00m script:\n",
      "File \u001b[0;32mparser.py:854\u001b[0m, in \u001b[0;36mpysmt.smtlib.parser.parser.SmtLibParser.get_script\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparser.py:868\u001b[0m, in \u001b[0;36mget_command_generator\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparser.py:1169\u001b[0m, in \u001b[0;36mget_command\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparser.py:1255\u001b[0m, in \u001b[0;36mpysmt.smtlib.parser.parser.SmtLibParser._cmd_declare_fun\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparser.py:619\u001b[0m, in \u001b[0;36mpysmt.smtlib.parser.parser.SmtLibParser._get_var\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/sage/lib/python3.12/site-packages/pysmt/formula.py:144\u001b[0m, in \u001b[0;36mFormulaManager.Symbol\u001b[0;34m(self, name, typename)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mSymbol\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, typename\u001b[38;5;241m=\u001b[39mtypes\u001b[38;5;241m.\u001b[39mBOOL):\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_or_create_symbol\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypename\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/sage/lib/python3.12/site-packages/pysmt/formula.py:136\u001b[0m, in \u001b[0;36mFormulaManager.get_or_create_symbol\u001b[0;34m(self, name, typename)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_symbol(name, typename)\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s\u001b[38;5;241m.\u001b[39msymbol_type() \u001b[38;5;241m==\u001b[39m typename:\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PysmtTypeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrying to redefine symbol \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m with a new type\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    137\u001b[0m                          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. Previous type was \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m new type is \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m    138\u001b[0m                          (name, s\u001b[38;5;241m.\u001b[39msymbol_type(), typename))\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m s\n",
      "\u001b[0;31mPysmtTypeError\u001b[0m: Trying to redefine symbol 'p' with a new type. Previous type was 'Int' new type is 'Bool'"
     ]
    }
   ],
   "source": [
    "from pysmt.smtlib.parser import SmtLibParser\n",
    "from pysmt.shortcuts import Symbol, And, Implies, ForAll, Equals\n",
    "from pysmt.typing import BOOL\n",
    "from io import StringIO\n",
    "\n",
    "def parse_horn_clauses(formula_str):\n",
    "    parser = SmtLibParser()\n",
    "    script = parser.get_script(StringIO(formula_str))\n",
    "    \n",
    "    results = []\n",
    "    for cmd in script:\n",
    "        if cmd.name == \"assert\":\n",
    "            formula = cmd.args[0]\n",
    "            if formula.is_forall():\n",
    "                body = formula.arg(0)\n",
    "                if body.is_implies():\n",
    "                    antecedent, consequent = body.arg(0), body.arg(1)\n",
    "                    body_parts = antecedent.args() if antecedent.is_and() else [antecedent]\n",
    "                    results.append((\"horn_clause\", formula.quantifier_vars(), body_parts, consequent))\n",
    "                else:\n",
    "                    results.append((\"fact\", body))\n",
    "            elif formula.is_implies():\n",
    "                antecedent, consequent = formula.arg(0), formula.arg(1)\n",
    "                body_parts = antecedent.args() if antecedent.is_and() else [antecedent]\n",
    "                results.append((\"horn_clause\", [], body_parts, consequent))\n",
    "            else:\n",
    "                results.append((\"fact\", formula))\n",
    "        elif cmd.name == \"run\":\n",
    "            results.append((\"run\", None))\n",
    "        elif cmd.name == \"extract\":\n",
    "            results.append((\"extract\", cmd.args[0] if cmd.args else None))\n",
    "    \n",
    "    return results\n",
    "\n",
    "def print_parsed_clauses(clauses):\n",
    "    for clause_type, *args in clauses:\n",
    "        if clause_type == \"horn_clause\":\n",
    "            variables, body, head = args\n",
    "            print(f\"Horn Clause:\\n  Variables: {[str(v) for v in variables]}\\n  Body: {[str(b) for b in body]}\\n  Head: {str(head)}\")\n",
    "        elif clause_type == \"fact\":\n",
    "            print(f\"Fact: {str(args[0])}\")\n",
    "        elif clause_type == \"run\":\n",
    "            print(\"Run command encountered\")\n",
    "        elif clause_type == \"extract\":\n",
    "            print(f\"Extract command: {str(args[0])}\")\n",
    "        print()\n",
    "\n",
    "# Example usage\n",
    "formula_str = \"\"\"\n",
    "(declare-fun p () Bool)\n",
    "(declare-fun q () Bool)\n",
    "(declare-fun r () Bool)\n",
    "(assert (forall ((x Bool)) (=> (and p q) r)))\n",
    "(assert (= p q))\n",
    "(run)\n",
    "(extract (and p q))\n",
    "\"\"\"\n",
    "\n",
    "parsed_clauses = parse_horn_clauses(formula_str)\n",
    "print_parsed_clauses(parsed_clauses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pysmt\n",
      "  Downloading PySMT-0.9.6-py2.py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading PySMT-0.9.6-py2.py3-none-any.whl (326 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m326.1/326.1 kB\u001b[0m \u001b[31m781.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pysmt\n",
      "Successfully installed pysmt-0.9.6\n"
     ]
    }
   ],
   "source": [
    "! python3 -m pip install pysmt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tptp parse\n",
    "\n",
    "https://github.com/AndrzejKucik/tptp_python_parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/example.p\n"
     ]
    }
   ],
   "source": [
    "%%file /tmp/example.p\n",
    "% Example TPTP file\n",
    "\n",
    "fof(fact1, axiom, p(a)).\n",
    "fof(fact2, axiom, q(b)).\n",
    "fof(horn_clause1, axiom, (p(X) => q(X))).\n",
    "fof(horn_clause2, axiom, (~p(X) | q(X))).\n",
    "fof(non_horn_clause, axiom, (p(X) | q(X) | ~r(X))).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lark import Lark\n",
    "import os\n",
    "\n",
    "tptp_parser = Lark(r\"\"\"\n",
    "\ttptp_file : tptp_input*\n",
    "\ttptp_input : annotated_formula | include\n",
    "\n",
    "\tannotated_formula : thf_annotated | tfx_annotated | tff_annotated| tcf_annotated | fof_annotated | cnf_annotated | tpi_annotated\n",
    "\n",
    "\ttpi_annotated : \"tpi(\" NAME \",\" FORMULA_ROLE \",\" tpi_formula annotations* \").\"\n",
    "\ttpi_formula : fof_formula\n",
    "\tthf_annotated : \"thf(\" NAME \",\" FORMULA_ROLE \",\" thf_formula annotations* \").\"\n",
    "\ttfx_annotated : \"tfx(\"  NAME \",\" FORMULA_ROLE \",\" tfx_formula annotations* \").\"\n",
    "\ttff_annotated : \"tff(\" NAME \",\" FORMULA_ROLE \",\" tff_formula annotations* \").\"\n",
    "\ttcf_annotated : \"tcf(\" NAME \",\" FORMULA_ROLE \",\" tcf_formula annotations* \").\"\n",
    "\tfof_annotated : \"fof(\" NAME \",\" FORMULA_ROLE \",\" fof_formula annotations* \").\"\n",
    "\tcnf_annotated : \"cnf(\" NAME \",\" FORMULA_ROLE \",\" cnf_formula annotations* \").\"\n",
    "\tannotations : \",\" source (optional_info)*\n",
    "\n",
    "\tFORMULA_ROLE : \"axiom\" | \"hypothesis\" | \"definition\" | \"assumption\" | \"lemma\" | \"theorem\" | \"corollary\" | \"conjecture\"\n",
    "\t\t\t\t| \"negated_conjecture\" | \"plain\" | \"type\" | \"fi_domain\" | \"fi_functors\" | \"fi_predicates\" | \"unknown\" | LOWER_WORD\n",
    "\n",
    "\tthf_formula : thf_logic_formula | thf_sequent\n",
    "\tthf_logic_formula : thf_binary_formula | thf_unitary_formula | thf_type_formula | thf_subtype\n",
    "\tthf_binary_formula : thf_binary_pair | thf_binary_tuple | thf_binary_type\n",
    "\n",
    "\tthf_binary_pair : thf_unitary_formula thf_pair_connective thf_unitary_formula\n",
    "\tthf_binary_tuple : thf_or_formula | thf_and_formula | thf_apply_formula\n",
    "\tthf_or_formula : thf_unitary_formula \"|\" thf_unitary_formula | thf_or_formula \"|\" thf_unitary_formula\n",
    "\tthf_and_formula : thf_unitary_formula \"&\" thf_unitary_formula | thf_and_formula \"&\" thf_unitary_formula\n",
    "\n",
    "\tthf_apply_formula: thf_unitary_formula \"@\" thf_unitary_formula | thf_apply_formula \"@\" thf_unitary_formula\n",
    "\n",
    "\tthf_unitary_formula : thf_quantified_formula | thf_unary_formula | thf_atom | thf_conditional | thf_let | thf_tuple | \"(\" thf_logic_formula \")\"\n",
    "\n",
    "\tthf_quantified_formula : thf_quantification thf_unitary_formula\n",
    "\tthf_quantification : thf_quantifier \"[\" thf_variable_list \"] :\"\n",
    "\tthf_variable_list : thf_variable (\",\" thf_variable)*\n",
    "\tthf_variable : thf_typed_variable | VARIABLE\n",
    "\tthf_typed_variable : VARIABLE \":\" thf_top_level_type\n",
    "\n",
    "\tthf_unary_formula : thf_unary_connective \"(\" thf_logic_formula \")\"\n",
    "\tthf_atom : thf_function | VARIABLE | defined_term | thf_conn_term\n",
    "\n",
    "\tthf_function : ATOM | FUNCTOR \"(\" thf_arguments \")\" | DEFINED_FUNCTOR \"(\" thf_arguments \")\" | SYSTEM_FUNCTOR \"(\" thf_arguments \")\"\n",
    "\n",
    "\tthf_conn_term : thf_pair_connective | ASSOC_CONNECTIVE | thf_unary_connective\n",
    "\n",
    "\tthf_conditional : \"$ite(\" thf_logic_formula \",\" thf_logic_formula \",\" thf_logic_formula \")\"\n",
    "\n",
    "\tthf_let : \"$let(\" thf_unitary_formula \",\" thf_formula \")\" | \"$let(\" thf_let_defns \",\" thf_formula\")\"\n",
    "\tthf_let_defns : thf_let_defn | \"[\" thf_let_defn_list \"]\"\n",
    "\tthf_let_defn_list : thf_let_defn (\",\" thf_let_defn)*\n",
    "\tthf_let_defn : thf_let_quantified_defn | thf_let_plain_defn\n",
    "\tthf_let_quantified_defn: thf_quantification \"(\" thf_let_plain_defn \")\"\n",
    "\tthf_let_plain_defn: thf_let_defn_lhs ASSIGNMENT thf_formula\n",
    "\tthf_let_defn_lhs : CONSTANT | FUNCTOR \"(\" fof_arguments \")\" | thf_tuple\n",
    "\n",
    "\tthf_arguments : thf_formula_list\n",
    "\n",
    "\tthf_type_formula : thf_typeable_formula \":\" thf_top_level_type | CONSTANT \":\" thf_top_level_type\n",
    "\tthf_typeable_formula : thf_atom | \"(\" thf_logic_formula \")\"\n",
    "\tthf_subtype : thf_atom \"<<\" thf_atom\n",
    "\n",
    "\tthf_top_level_type : thf_unitary_type | thf_mapping_type | thf_apply_type\n",
    "\n",
    "\tthf_unitary_type : thf_unitary_formula\n",
    "\tthf_apply_type : thf_apply_formula\n",
    "\tthf_binary_type : thf_mapping_type | thf_xprod_type | thf_union_type\n",
    "\tthf_mapping_type : thf_unitary_type \">\" thf_unitary_type | thf_unitary_type \">\" thf_mapping_type\n",
    "\tthf_xprod_type : thf_unitary_type \"*\" thf_unitary_type | thf_xprod_type \"*\" thf_unitary_type\n",
    "\tthf_union_type : thf_unitary_type \"+\" thf_unitary_type | thf_union_type \"+\" thf_unitary_type\n",
    "\n",
    "\tthf_sequent : thf_tuple \"-->\" thf_tuple | \"(\" thf_sequent \")\"\n",
    "\n",
    "\tthf_tuple : \"[\" thf_formula_list? \"]\" | \"{\" thf_formula_list? \"}\"\n",
    "\tthf_formula_list : thf_logic_formula (\",\" thf_logic_formula)*\n",
    "\n",
    "\tlogic_defn_rule : logic_defn_lhs ASSIGNMENT logic_defn_rhs\n",
    "\tlogic_defn_lhs : LOGIC_DEFN_VALUE | thf_top_level_type | NAME | \"$constants\" | \"$quantification\" | \"$consequence\" | \"$modalities\"\n",
    "\n",
    "\tlogic_defn_rhs : LOGIC_DEFN_VALUE | thf_unitary_formula\n",
    "\tLOGIC_DEFN_VALUE : DEFINED_CONSTANT | \"$rigid\" | \"$flexible\" | \"$constant\" | \"$varying\" | \"$cumulative\" | \"$decreasing\" | \"$local\"\n",
    "\t\t\t\t\t| \"$global\" | \"$modal_system_K\" | \"$modal_system_T\" | \"$modal_system_D\" | \"$modal_system_S4\" | \"$modal_system_S5\"\n",
    "\t\t\t\t\t| \"$modal_axiom_K\" | \"$modal_axiom_T\" | \"$modal_axiom_B\" | \"$modal_axiom_D\" | \"$modal_axiom_4\" | \"$modal_axiom_5\"\n",
    "\n",
    "\ttfx_formula : tfx_logic_formula | thf_sequent\n",
    "\ttfx_logic_formula : thf_logic_formula\n",
    "\n",
    "\ttff_formula : tff_logic_formula | tff_typed_atom | tff_sequent\n",
    "\ttff_logic_formula : tff_binary_formula | tff_unitary_formula | tff_subtype\n",
    "\ttff_binary_formula : tff_binary_nonassoc | tff_binary_assoc\n",
    "\ttff_binary_nonassoc : tff_unitary_formula BINARY_CONNECTIVE tff_unitary_formula\n",
    "\ttff_binary_assoc : tff_or_formula | tff_and_formula\n",
    "\ttff_or_formula : tff_unitary_formula \"|\" tff_unitary_formula | tff_or_formula \"|\" tff_unitary_formula\n",
    "\ttff_and_formula : tff_unitary_formula \"&\" tff_unitary_formula | tff_and_formula \"&\" tff_unitary_formula\n",
    "\ttff_unitary_formula : tff_quantified_formula | tff_unary_formula | tff_atomic_formula | tff_conditional | tff_let | \"(\" tff_logic_formula \")\"\n",
    "\n",
    "\ttff_quantified_formula : FOF_QUANTIFIER \"[\" tff_variable_list \"] :\" tff_unitary_formula\n",
    "\ttff_variable_list : tff_variable (\",\" tff_variable)*\n",
    "\ttff_variable : tff_typed_variable | VARIABLE\n",
    "\ttff_typed_variable : VARIABLE \":\" tff_atomic_type\n",
    "\ttff_unary_formula : \"~\" tff_unitary_formula | fof_infix_unary\n",
    "\ttff_atomic_formula : fof_atomic_formula\n",
    "\ttff_conditional : \"$ite_f(\" tff_logic_formula \",\" tff_logic_formula \",\" tff_logic_formula \")\"\n",
    "\ttff_let : \"$let_tf(\" tff_let_term_defns \",\" tff_formula \")\" | \"$let_ff(\" tff_let_formula_defns \",\" tff_formula \")\"\n",
    "\n",
    "\ttff_let_term_defns : tff_let_term_defn | \"[\" tff_let_term_list \"]\"\n",
    "\ttff_let_term_list : tff_let_term_defn (\",\" tff_let_term_defn)*\n",
    "\ttff_let_term_defn : \"! [\" tff_variable_list \"] :\" tff_let_term_defn | tff_let_term_binding\n",
    "\ttff_let_term_binding : fof_plain_term \"=\" fof_term | \"(\" tff_let_term_binding \")\"\n",
    "\ttff_let_formula_defns : tff_let_formula_defn | \"[\" tff_let_formula_list \"]\"\n",
    "\ttff_let_formula_list : tff_let_formula_defn (\",\" tff_let_formula_defn)*\n",
    "\ttff_let_formula_defn : \"! [ \"tff_variable_list \"] :\" tff_let_formula_defn | tff_let_formula_binding\n",
    "\ttff_let_formula_binding : fof_plain_atomic_formula \"<=>\" tff_unitary_formula | \"(\" tff_let_formula_binding \")\"\n",
    "\ttff_sequent : tff_formula_tuple \"-->\" tff_formula_tuple | \"(\" tff_sequent \")\"\n",
    "\ttff_formula_tuple : \"[\" [tff_formula_tuple_list] \"]\"\n",
    "\ttff_formula_tuple_list : tff_logic_formula (\",\" tff_logic_formula)*\n",
    "\n",
    "\ttff_typed_atom : UNTYPED_ATOM \":\" tff_top_level_type | \"(\" tff_typed_atom \")\"\n",
    "\ttff_subtype : UNTYPED_ATOM \"<<\" ATOM\n",
    "\n",
    "\ttff_top_level_type : tff_atomic_type | tff_mapping_type | tf1_quantified_type | \"(\" tff_top_level_type \")\"\n",
    "\ttf1_quantified_type : \"!> [\" tff_variable_list \"] :\" tff_monotype\n",
    "\ttff_monotype : tff_atomic_type | \"(\" tff_mapping_type \")\"\n",
    "\ttff_unitary_type :  tff_atomic_type | \"(\" tff_xprod_type \")\"\n",
    "\ttff_atomic_type : TYPE_CONSTANT | DEFINED_TYPE | TYPE_FUNCTOR \"(\" tff_type_arguments \")\" | VARIABLE\n",
    "\ttff_type_arguments : tff_atomic_type (\",\" tff_atomic_type)*\n",
    "\n",
    "\ttff_mapping_type : tff_unitary_type \">\" tff_atomic_type\n",
    "\ttff_xprod_type : tff_unitary_type \"*\" tff_atomic_type | tff_xprod_type \"*\" tff_atomic_type\n",
    "\n",
    "\n",
    "\ttcf_formula : tcf_logic_formula | tff_typed_atom\n",
    "\ttcf_logic_formula : tcf_quantified_formula | cnf_formula\n",
    "\ttcf_quantified_formula : \"! [\" tff_variable_list \"] :\" cnf_formula\n",
    "\n",
    "\n",
    "\tfof_formula : fof_logic_formula | fof_sequent\n",
    "\tfof_logic_formula : fof_binary_formula | fof_unitary_formula\n",
    "\n",
    "\tfof_binary_formula :  fof_binary_nonassoc | fof_binary_assoc\n",
    "\t\n",
    "\tfof_binary_nonassoc : fof_unitary_formula BINARY_CONNECTIVE fof_unitary_formula\n",
    "\t\n",
    "\tfof_binary_assoc : fof_or_formula | fof_and_formula\n",
    "\tfof_or_formula : fof_unitary_formula \"|\" fof_unitary_formula | fof_or_formula \"|\" fof_unitary_formula\n",
    "\tfof_and_formula  : fof_unitary_formula \"&\" fof_unitary_formula | fof_and_formula \"&\"  fof_unitary_formula\n",
    "\t\n",
    "\tfof_unitary_formula : fof_quantified_formula | fof_unary_formula | fof_atomic_formula | \"(\" fof_logic_formula \")\"\n",
    "\t\n",
    "\tfof_quantified_formula : FOF_QUANTIFIER \"[\" fof_variable_list \"] :\" fof_unitary_formula\n",
    "\tfof_variable_list : VARIABLE (\",\" VARIABLE)*\n",
    "\tfof_unary_formula : \"~\" fof_unitary_formula | fof_infix_unary\n",
    "\t\n",
    "\tfof_infix_unary : fof_term INFIX_INEQUALITY fof_term\n",
    "\tfof_atomic_formula : fof_plain_atomic_formula | fof_defined_atomic_formula | fof_system_atomic_formula\n",
    "\tfof_plain_atomic_formula : fof_plain_term\n",
    "\tfof_defined_atomic_formula : fof_defined_plain_formula | fof_defined_infix_formula\n",
    "\tfof_defined_plain_formula : fof_defined_plain_term | DEFINED_PROPOSITION | DEFINED_PREDICATE \"(\" fof_arguments \")\"\n",
    "\tfof_defined_infix_formula : fof_term defined_infix_pred fof_term\n",
    "\n",
    "\tfof_system_atomic_formula : fof_system_term\n",
    "\n",
    "\tfof_plain_term : CONSTANT | FUNCTOR \"(\" fof_arguments \")\"\n",
    "\n",
    "\tfof_defined_term : defined_term | fof_defined_atomic_term\n",
    "\tfof_defined_atomic_term : fof_defined_plain_term\n",
    "\n",
    "\tfof_defined_plain_term : DEFINED_CONSTANT | DEFINED_FUNCTOR \"(\" fof_arguments \")\"\n",
    "\n",
    "\tfof_system_term : SYSTEM_CONSTANT | SYSTEM_FUNCTOR \"(\" fof_arguments \")\"\n",
    "\n",
    "\tfof_arguments : fof_term (\",\" fof_term)*\n",
    "\n",
    "\tfof_term : fof_function_term | VARIABLE | tff_conditional_term | tff_let_term | tff_tuple_term\n",
    "\tfof_function_term : fof_plain_term | fof_defined_term | fof_system_term\n",
    "\t\n",
    "\ttff_conditional_term : \"$ite_t(\" tff_logic_formula \",\" fof_term \",\" fof_term \")\"\n",
    "\ttff_let_term : \"let_ft(\" tff_let_formula_defns \",\" fof_term \")\" | \"$let_tt(\" tff_let_term_defns \",\"fof_term \")\"\n",
    "\ttff_tuple_term : \"{\" [fof_arguments] \"}\"\n",
    "\n",
    "\n",
    "\tfof_sequent : fof_formula_tuple \"-->\" fof_formula_tuple | \"(\" fof_sequent \")\"\n",
    "\tfof_formula_tuple : \"[\" [fof_formula_tuple_list] \"]\"\n",
    "\tfof_formula_tuple_list : fof_logic_formula (\",\" fof_logic_formula)*\n",
    "\n",
    "\n",
    "\tcnf_formula : disjunction | \"(\" disjunction \")\"\n",
    "\tdisjunction : literal (\"|\" literal)*\n",
    "\tliteral : fof_atomic_formula | \"~\" fof_atomic_formula | fof_infix_unary\n",
    "\n",
    "\tthf_quantifier : FOF_QUANTIFIER | TH0_QUANTIFIER | TH1_QUANTIFIER\n",
    "\n",
    "\tTH1_QUANTIFIER : \"!>\" | \"?*\"\n",
    "\tTH0_QUANTIFIER : \"^\" | \"@+\" | \"@-\"\n",
    "\tthf_pair_connective : INFIX_EQUALITY | INFIX_INEQUALITY | BINARY_CONNECTIVE | ASSIGNMENT\n",
    "\tthf_unary_connective : \"~\" | TH1_UNARY_CONNECTIVE\n",
    "\tTH1_UNARY_CONNECTIVE : \"!!\" | \"??\" | \"@@+\" | \"@@-\" | \"@=\"\n",
    "\n",
    "\tFOF_QUANTIFIER : \"!\" | \"?\"\n",
    "\tBINARY_CONNECTIVE : \"<=>\" | \"=>\" | \"<=\" | \"<~>\" | \"~|\" | \"~&\"\n",
    "\tASSOC_CONNECTIVE: \"&\" | \"|\"\n",
    "\n",
    "\tASSIGNMENT : \":=\"\n",
    "\n",
    "\tTYPE_CONSTANT : TYPE_FUNCTOR\n",
    "\tTYPE_FUNCTOR : ATOMIC_WORD\n",
    "\tDEFINED_TYPE : ATOMIC_DEFINED_WORD | \"$oType\" | \"$o\" | \"$iType\" | \"$i\" | \"$tType\" | \"$real\" | \"$rat\" | \"$int\"\n",
    "\n",
    "\tSYSTEM_TYPE : ATOMIC_SYSTEM_WORD\n",
    "\n",
    "\tATOM : UNTYPED_ATOM | DEFINED_CONSTANT\n",
    "\tUNTYPED_ATOM : CONSTANT | SYSTEM_CONSTANT\n",
    "\tDEFINED_PROPOSITION :  ATOMIC_DEFINED_WORD | \"$true\" | \"$false\"\n",
    "\tDEFINED_PREDICATE : ATOMIC_DEFINED_WORD | \"$distinct\" | \"$less\" | \"$lesseq\" | \"$greater\" | \"$greatereq\" | \"$is_int\"\n",
    "\t\t\t\t\t\t| \"$is_rat\" | \"$box_P\" | \"$box_i\" | \"$box_int\" | \"$box\" | \"$dia_P\" | \"$dia_i\" | \"$dia_int\" | \"$dia\"\n",
    "\n",
    "\tdefined_infix_pred : INFIX_EQUALITY | ASSIGNMENT\n",
    "\tINFIX_EQUALITY : \"=\"\n",
    "\tINFIX_INEQUALITY : \"!=\"\n",
    "\n",
    "\tCONSTANT : FUNCTOR\n",
    "\tFUNCTOR : ATOMIC_WORD\n",
    "\tSYSTEM_CONSTANT : SYSTEM_FUNCTOR\n",
    "\tSYSTEM_FUNCTOR : ATOMIC_SYSTEM_WORD\n",
    "\tDEFINED_CONSTANT : DEFINED_FUNCTOR\n",
    "\tDEFINED_FUNCTOR : ATOMIC_DEFINED_WORD |\"$uminus\" | \"$sum\" | \"$difference\" | \"$product\" | \"$quotient\" | \"$quotient_e\" | \"$quotient_t\" | \"$quotient_f\"\n",
    "\t\t\t\t\t| \"$remainder_e\" | \"$remainder_t\" | \"$remainder_f\" | \"$floor\" | \"$ceiling\" | \"$truncate\" | \"$round\" | \"$to_int\" | \"$to_rat\" | \"$to_real\"\n",
    "\tdefined_term : number | DISTINCT_OBJECT\n",
    "\tVARIABLE : UPPER_WORD\n",
    "\n",
    "\tsource : general_term | dag_source | internal_source | external_source | \"[\" sources \"]\"\n",
    "\tsources : source (\",\" source)*\n",
    "\tdag_source : NAME | inference_record\n",
    "\tinference_record : \"inference(\" INFERENCE_RULE \",\" useful_info \",\" inference_parents \")\"\n",
    "\tINFERENCE_RULE : ATOMIC_WORD\n",
    "\n",
    "\tinference_parents : \"[\" parent_list* \"]\"\n",
    "\tparent_list : parent_info (\",\" parent_info)*\n",
    "\tparent_info : source parent_details*\n",
    "\tparent_details : general_list\n",
    "\tinternal_source : \"introduced(\" intro_type optional_info* \")\"\n",
    "\tintro_type : \"definition\" | \"axiom_of_choice\" | \"tautology\" | \"assumption\"\n",
    "\n",
    "\texternal_source : file_source | theory | creator_source\n",
    "\tfile_source : \"file(\" FILE_NAME FILE_INFO* \")\"\n",
    "\tFILE_INFO : \",\" NAME\n",
    "\ttheory : \"theory(\" THEORY_NAME optional_info* \")\"\n",
    "\tTHEORY_NAME : \"equality\" | \"ac\"\n",
    "\n",
    "\tcreator_source : \"creator(\" CREATOR_NAME optional_info* \")\"\n",
    "\tCREATOR_NAME : ATOMIC_WORD\n",
    "\n",
    "\toptional_info : \",\" useful_info\n",
    "\tuseful_info : general_list | \"[\" info_items* \"]\"\n",
    "\tinfo_items : info_item (\",\" info_item)*\n",
    "\tinfo_item : formula_item | inference_item | general_function\n",
    "\n",
    "\tformula_item : DESCRIPTION_ITEM  | IQUOTE_ITEM\n",
    "\tDESCRIPTION_ITEM : \"description(\" ATOMIC_WORD \")\"\n",
    "\tIQUOTE_ITEM : \"iquote(\" ATOMIC_WORD \")\"\n",
    "\n",
    "\tinference_item : inference_status | assumptions_record | new_symbol_record | refutation\n",
    "\tinference_status : \"status(\" STATUS_VALUE \")\" | inference_info\n",
    "\n",
    "\tSTATUS_VALUE : \"suc\" | \"unp\" | \"sap\" | \"esa\" | \"sat\" | \"fsa\" | \"thm\" | \"eqv\" | \"tac\" | \"wec\" | \"eth\" | \"tau\" | \"wtc\" | \"wth\" | \"cax\" | \"sca\" | \"tca\"\n",
    "\t\t\t\t| \"wca\" | \"cup\" | \"csp\" | \"ecs\" | \"csa\" | \"cth\" | \"ceq\" | \"unc\" | \"wcc\" | \"ect\" | \"fun\" | \"uns\" | \"wuc\" | \"wct\" | \"scc\" | \"uca\" | \"noc\"\n",
    "\n",
    "\tinference_info : INFERENCE_RULE \"(\" ATOMIC_WORD \",\" general_list \")\"\n",
    "\n",
    "\tassumptions_record : \"assumptions([\" name_list \"])\"\n",
    "\n",
    "\trefutation : \"refutation(\" file_source \")\"\n",
    "\n",
    "\tnew_symbol_record : \"new_symbols(\" ATOMIC_WORD \", [\" new_symbol_list \"])\"\n",
    "\tnew_symbol_list : principal_symbol (\",\" principal_symbol)*\n",
    "\n",
    "\tprincipal_symbol : FUNCTOR | VARIABLE\n",
    "\n",
    "\n",
    "\tinclude : \"include(\" FILE_NAME formula_selection* \").\"\n",
    "\tformula_selection : \",[\" name_list \"]\"\n",
    "\tname_list : NAME (\",\" NAME)*\n",
    "\n",
    "\tgeneral_term : general_data | general_data \":\" general_term | general_list\n",
    "\tgeneral_data : ATOMIC_WORD | general_function | VARIABLE | number | DISTINCT_OBJECT | formula_data | \"bind(\" VARIABLE \",\" formula_data \")\"\n",
    "\tgeneral_function : ATOMIC_WORD \"(\" general_terms \")\"\n",
    "\n",
    "\tformula_data : \"$thf(\" thf_formula \")\" | \"$tff(\" tff_formula \")\" | \"$fof(\" fof_formula \")\" | \"$cnf(\" cnf_formula \")\" | \"$fot(\" fof_term \")\"\n",
    "\tgeneral_list : \"[\" general_terms? \"]\"\n",
    "\tgeneral_terms : general_term (\",\" general_term)*\n",
    "\n",
    "\tNAME : ATOMIC_WORD | INTEGER\n",
    "\n",
    "\tATOMIC_WORD : LOWER_WORD | SINGLE_QUOTED\n",
    "\n",
    "\tATOMIC_DEFINED_WORD : \"$\" LOWER_WORD\n",
    "\tATOMIC_SYSTEM_WORD : \"$$\" LOWER_WORD\n",
    "\tnumber : INTEGER | RATIONAL | REAL\n",
    "\n",
    "\tFILE_NAME : SINGLE_QUOTED\n",
    "\n",
    "\tcomment : COMMENT_LINE | COMMENT_BLOCK\n",
    "\tCOMMENT_LINE : \"%\" PRINTABLE_CHAR*\n",
    "\tCOMMENT_BLOCK : \"/*\" NOT_STAR_SLASH? \"*\"+ \"/\"\n",
    "\tNOT_STAR_SLASH : (\"^*\"* \"*\"+ \"^/*\") (\"^*\")*\n",
    "\n",
    "\tSINGLE_QUOTED : \"'\" SQ_CHAR+ \"'\"\n",
    "\n",
    "\tDISTINCT_OBJECT : \"\\\"\" DO_CHAR* \"\\\"\"\n",
    "\n",
    "\tUPPER_WORD : UPPER_ALPHA ALPHA_NUMERIC*\n",
    "\tLOWER_WORD : LOWER_ALPHA ALPHA_NUMERIC*\n",
    "\n",
    "\tREAL : SIGN? DECIMAL_FRACTION | SIGN? DECIMAL_EXPONENT\n",
    "\tRATIONAL : SIGN? DECIMAL \"/\" POSITIVE_DECIMAL\n",
    "\tINTEGER : SIGN? DECIMAL\n",
    "\tDECIMAL : ZERO_NUMERIC | POSITIVE_DECIMAL\n",
    "\tPOSITIVE_DECIMAL : NON_ZERO_NUMERIC NUMERIC*           \n",
    "\tDECIMAL_EXPONENT : DECIMAL \"Ee\" EXP_INTEGER | DECIMAL_FRACTION \"Ee\" EXP_INTEGER\n",
    "\tDECIMAL_FRACTION : DECIMAL DOT_DECIMAL\n",
    "\tDOT_DECIMAL : \".\" NUMERIC+\n",
    "\tEXP_INTEGER : SIGN? NUMERIC+\n",
    "\n",
    "\tDO_CHAR : (/[\"\\40\"-\"\\41\", \"\\43\"-\"\\133\", \"\\135\"-\"\\176\"]/ | \"\\\\\\\\ \\\" \\\\\\\\\")\n",
    "\n",
    "\n",
    "\tSQ_CHAR : (/[\"\\40\"-\"\\46\", \"\\50\"-\"\\133\", \"\\135\"-\"\\176\"]/ | \"\\\\\\\\ ' \\\\\\\\\")\n",
    "\n",
    "\tSIGN : \"+\" | \"-\"\n",
    "\tZERO_NUMERIC : \"0\"\n",
    "\tNON_ZERO_NUMERIC : \"1\" .. \"9\"\n",
    "\tNUMERIC : \"0\" .. \"9\"\n",
    "\tLOWER_ALPHA : \"a\" .. \"z\"\n",
    "\tUPPER_ALPHA : \"A\" .. \"Z\"\n",
    "\tALPHA_NUMERIC : LOWER_ALPHA | UPPER_ALPHA | NUMERIC | \"_\"\n",
    "\tPRINTABLE_CHAR : /[\"\\32\"-\"\\126\"]/\n",
    "\n",
    "\tVIEWABLE_CHAR : \"\\n\"\n",
    "\n",
    "    %import common.WS  \n",
    "    %ignore WS\n",
    "\n",
    "    \"\"\", start='tptp_file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fof_annotated\n",
      "  fact1\n",
      "  axiom\n",
      "  fof_formula\n",
      "    fof_logic_formula\n",
      "      fof_unitary_formula\n",
      "        fof_atomic_formula\n",
      "          fof_defined_atomic_formula\n",
      "            fof_defined_infix_formula\n",
      "              fof_term\n",
      "                fof_function_term\n",
      "                  fof_plain_term\n",
      "                    p\n",
      "                    fof_arguments\n",
      "                      fof_term\n",
      "                        fof_function_term\n",
      "                          fof_plain_term\ta\n",
      "              defined_infix_pred\t=\n",
      "              fof_term\n",
      "                fof_function_term\n",
      "                  fof_plain_term\n",
      "                    f\n",
      "                    fof_arguments\n",
      "                      fof_term\n",
      "                        fof_function_term\n",
      "                          fof_plain_term\tb\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from lark import Visitor\n",
    "\n",
    "class HornClauseCollector(Visitor):\n",
    "    def __init__(self):\n",
    "        self.horn_clauses = []\n",
    "\n",
    "    # Override visit methods for the nodes you are interested in\n",
    "    def cnf_annotated(self, tree):\n",
    "        # CNF formulas often represent Horn clauses in TPTP format\n",
    "        self.horn_clauses.append(tree)\n",
    "\n",
    "    def fof_annotated(self, tree):\n",
    "        # In some cases, Horn clauses can be represented as FOF formulas\n",
    "        if self.is_horn_clause(tree):\n",
    "            self.horn_clauses.append(tree)\n",
    "\n",
    "    def is_horn_clause(self, tree):\n",
    "        # Implement logic to determine if the formula is a Horn clause\n",
    "        # A Horn clause has at most one positive literal\n",
    "        literals = list(tree.find_data('fof_atomic_formula'))\n",
    "        positive_literals = [lit for lit in literals if not lit.children or lit.children[0] != '~']\n",
    "        return len(positive_literals) <= 1\n",
    "\n",
    "    def visit(self, tree):\n",
    "        # Start visiting the tree\n",
    "        super().visit(tree)\n",
    "\n",
    "    def get_horn_clauses(self):\n",
    "        # Return the collected Horn clauses\n",
    "        return self.horn_clauses\n",
    "\n",
    "\n",
    "parsed_tree = tptp_parser.parse(\"fof(fact1, axiom, p(a) = f(b)).\")\n",
    "\n",
    "# Create an instance of the collector\n",
    "collector = HornClauseCollector()\n",
    "\n",
    "# Traverse the parse tree to collect Horn clauses\n",
    "collector.visit(parsed_tree)\n",
    "\n",
    "# Get the list of Horn clauses\n",
    "horn_clauses = collector.get_horn_clauses()\n",
    "\n",
    "# Print or process the collected Horn clauses\n",
    "for clause in horn_clauses:\n",
    "    print(clause.pretty())\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Literal(negated=True, atom=Predicate(index=298, arguments=(Variable(index=1009), Variable(index=1007)))),\n",
       " Literal(negated=False, atom=Predicate(index=300, arguments=(Variable(index=1007), Variable(index=1009)))))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tptp_lark_parser import TPTPParser\n",
    "\n",
    "tptp_parser = TPTPParser()\n",
    "parsed_text = tptp_parser.parse(\"cnf(test, axiom, ~ p(Y, X) | q(X, Y)).\")\n",
    "clause_literals = parsed_text[0].literals\n",
    "parsed_text[0].literals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'children'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m     tptp_data \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Extract facts and Horn clauses\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m facts, horn_clauses \u001b[38;5;241m=\u001b[39m \u001b[43mextract_facts_and_horn_clauses\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtptp_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFacts:\u001b[39m\u001b[38;5;124m'\u001b[39m, facts)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHorn Clauses:\u001b[39m\u001b[38;5;124m'\u001b[39m, horn_clauses)\n",
      "Cell \u001b[0;32mIn[3], line 15\u001b[0m, in \u001b[0;36mextract_facts_and_horn_clauses\u001b[0;34m(tptp_data)\u001b[0m\n\u001b[1;32m     12\u001b[0m facts \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     13\u001b[0m horn_clauses \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m formula \u001b[38;5;129;01min\u001b[39;00m \u001b[43mparse_tree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchildren\u001b[49m:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m formula\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfof_annotated\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     17\u001b[0m         role \u001b[38;5;241m=\u001b[39m formula\u001b[38;5;241m.\u001b[39mchildren[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'children'"
     ]
    }
   ],
   "source": [
    "from tptp_lark_parser import TPTPParser\n",
    "\n",
    "def is_horn_clause(literals):\n",
    "    # Check if a clause is a Horn clause (at most one positive literal)\n",
    "    positive_literals = [lit for lit in literals if not lit.startswith('~')]\n",
    "    return len(positive_literals) <= 1\n",
    "\n",
    "def extract_facts_and_horn_clauses(tptp_data):\n",
    "    parser = TPTPParser()\n",
    "    parse_tree = parser.parse(tptp_data)\n",
    "\n",
    "    facts = []\n",
    "    horn_clauses = []\n",
    "\n",
    "    for formula in parse_tree.children:\n",
    "        if formula.data == 'fof_annotated':\n",
    "            role = formula.children[1].value\n",
    "            if role == 'axiom':\n",
    "                clause = formula.children[2]\n",
    "                literals = [str(literal) for literal in clause.children if literal.data == 'atomic_formula']\n",
    "                if len(literals) == 1:\n",
    "                    facts.append(literals[0])\n",
    "                elif is_horn_clause(literals):\n",
    "                    horn_clauses.append(literals)\n",
    "\n",
    "    return facts, horn_clauses\n",
    "\n",
    "# Read the TPTP file data\n",
    "tptp_file = '/tmp/example.p'\n",
    "with open(tptp_file, 'r') as file:\n",
    "    tptp_data = file.read()\n",
    "\n",
    "# Extract facts and Horn clauses\n",
    "facts, horn_clauses = extract_facts_and_horn_clauses(tptp_data)\n",
    "\n",
    "print('Facts:', facts)\n",
    "print('Horn Clauses:', horn_clauses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tptp_lark_parser\n",
      "  Downloading tptp_lark_parser-0.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: lark-parser in /home/philip/.local/lib/python3.10/site-packages (from tptp_lark_parser) (0.12.0)\n",
      "Downloading tptp_lark_parser-0.2.0-py3-none-any.whl (59 kB)\n",
      "Installing collected packages: tptp_lark_parser\n",
      "Successfully installed tptp_lark_parser-0.2.0\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install tptp_lark_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nelson Oppen\n",
    "Nelson oppen is some kind of actual theorem.\n",
    "https://web.stanford.edu/class/cs357/lecture11.pdf\n",
    "\n",
    "https://homepage.cs.uiowa.edu/~tinelli/papers/TinHar-FROCOS-96.pdf  A ew correctness proof of neslon oppen\n",
    "\n",
    "Convexn theories ~ constructive. If they prover a /\\ b /\\ c |- d \\/ e  reall they prove d or e.\n",
    "Actually, stably infinite.\n",
    "\n",
    "Cody says is a kind of interpolation theorem? https://en.wikipedia.org/wiki/Craig_interpolation\n",
    "phi => psi\n",
    "Then exists z, phi => z, z => psi\n",
    "I suppose maybe SMT itself could be viewed in this light. if we abstract `x >= 7 <-> p` and we want to prove some property p \\/ q \\/ r then    \n",
    "\n",
    "\n",
    "Can we do a contextual egraph using z3 SAT?\n",
    "Twee style context\n",
    "\n",
    "\"in\" the T set can be done as an smt call\n",
    "\n",
    "solver.push()\n",
    "s.add(Not(Or(pat == t for t in T)))\n",
    "res = s.check()\n",
    "s.pop()\n",
    "if unsat: # match\n",
    "    s.unsat_core() #? useful for proof?\n",
    "\n",
    "\n",
    "### forbid table\n",
    "Union find dict of things not equal to.\n",
    "\n",
    "## Confluence Competition\n",
    "\n",
    "Moca conlfuecne compeitition.\n",
    "logically constrained rewrite systems. THis is similar to egraphs modulo theories? https://project-coco.uibk.ac.at/2024/categories/lctrs.php \n",
    "The ARI format. I like this https://project-coco.uibk.ac.at/ARI/index.php\n",
    "https://ari-cops.uibk.ac.at/ARI/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## external z3 egraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EggMT ( egraphs modulo theories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EGraph(R=[(y, x), (x + x, z)], E=[], T=defaultdict(<class 'set'>, {Real: {y, z, x, 1, y + z, x + y + z}}))\n",
      "EGraph(R=[(y, x), (x + x, z)], E=[], T=defaultdict(<class 'set'>, {Real: {z, x, x + z, 1, x + x + z}}))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EGraph(R=[(y, x), (x + x, z)], E=[], T=defaultdict(<class 'set'>, {Real: {z, x, x + z, 1, x + x + z}}))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from z3 import *\n",
    "from dataclasses import dataclass\n",
    "from functools import cache\n",
    "from itertools import product\n",
    "from collections import defaultdict\n",
    "\n",
    "@cache\n",
    "def z3_size(e):\n",
    "    if is_app(e):\n",
    "        return 1 + sum(z3_size(a) for a in e.children())\n",
    "    return 1\n",
    "def order(t1,t2):\n",
    "    s1,s2 = z3_size(t1), z3_size(t2)\n",
    "    if s1 > s2:\n",
    "        return (t1, t2)\n",
    "    elif s2 > s1:\n",
    "        return (t2, t1)\n",
    "    else:\n",
    "        if t1.get_id() > t2.get_id(): # yeaaaa, I'm not sure this is ok.\n",
    "            return (t1, t2)\n",
    "        else:\n",
    "            return (t2, t1)\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EGraph():\n",
    "    R: list[tuple[ExprRef, ExprRef]]\n",
    "    E: list[tuple[ExprRef, ExprRef]]\n",
    "    T: dict[SortRef, set[ExprRef]]\n",
    "    def __init__(self):\n",
    "        self.solver = Solver()\n",
    "        self.R = []\n",
    "        self.E = []\n",
    "        self.T = defaultdict(set) \n",
    "    def add_term(self, term):\n",
    "        self.T[term.sort()].add(term)\n",
    "        for t in term.children():\n",
    "            self.add_term(t)\n",
    "    def reduce_term(self, e):\n",
    "        while True:\n",
    "            e1 = z3.substitute(e, *self.R)\n",
    "            if e.eq(e1): #fixpoint\n",
    "                return e # simplify(e) reuse built in z3 simplify?\n",
    "            e = e1\n",
    "    def union(self, t1, t2):\n",
    "        t1 = self.reduce_term(t1)\n",
    "        t2 = self.reduce_term(t2)\n",
    "        if not t1.eq(t2):\n",
    "            self.solver.add(t1 == t2)\n",
    "            #self.E.append((t1, t2))\n",
    "            t1,t2 = order(t1, t2)\n",
    "            self.R.append((t1, t2))\n",
    "    def canon(self):\n",
    "        self.T = defaultdict(set,{k : set(self.reduce_term(t) for t in Ts) for k,Ts in self.T.items()})\n",
    "        # rule self reduction.\n",
    "        \"\"\"while self.E:\n",
    "            t1,t2 = self.E.pop()\n",
    "            t1 = self.reduce_term(t1)\n",
    "            t2 = self.reduce_term(t2)\n",
    "            if not t1.eq(t2):\n",
    "            \"\"\"\n",
    "    def guard(self, C):\n",
    "        self.solver.push()\n",
    "        self.solver.add(Not(C))\n",
    "        res = self.solver.check()\n",
    "        self.solver.pop()\n",
    "        if res == unsat:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    def rw(self, sorts, f, add=True):\n",
    "        for t in product(*[self.T[sort] for sort in sorts]):\n",
    "            lhs, rhs = f(*t)\n",
    "            lhs = self.reduce_term(lhs) \n",
    "            if  lhs in self.T:\n",
    "                if add:\n",
    "                    # if add is false, we never increase T. It will terminate. This is yihong's thing?\n",
    "                    self.add_term(rhs)\n",
    "                self.union(lhs, rhs)\n",
    "        #self.T\n",
    "    def z3_simplify():\n",
    "        # asserting built in z3 rules to the egraph\n",
    "        # One of the perks of piggybacking on z3\n",
    "        for t in self.T:\n",
    "            t1 = z3.simplify(t)\n",
    "            if not t1.eq(t):\n",
    "                self.union(t, t1)\n",
    "    def iter(self, *sorts):\n",
    "        return product(*[self.T[sort] for sort in sorts])\n",
    "    def rule(self, n, f):\n",
    "        \"\"\"\n",
    "         f produces a lhs => rhs thing that \n",
    "        \"\"\"\n",
    "        for t in product(self.T, repeat=n):\n",
    "            lhs, rhs = f(*t)\n",
    "            self.solver.push()\n",
    "            self.solver.add(Not(And(lhs)))\n",
    "            res = self.solver.check()\n",
    "            self.solver.pop()\n",
    "            if res == unsat:\n",
    "                self.solver.add(And(rhs))\n",
    "\n",
    "\n",
    "x,y,z = Reals(\"x y z\")\n",
    "E = EGraph()\n",
    "E.add_term(RealVal(1))\n",
    "E.add_term(x + (y + z))\n",
    "E.union(x, y)\n",
    "E.union(x + x, z)\n",
    "print(E)\n",
    "E.canon()\n",
    "E.rw([RealSort(), RealSort()], lambda x,y: (x + y, y + x))\n",
    "print(E)\n",
    "E.canon()\n",
    "E\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "z + 2&middot;x + y"
      ],
      "text/plain": [
       "z + 2*x + y"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simplify(z + x + y + x) # ok so it doesn't order them. Huh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if we weren't doing egraph stuff, this is kind of an interesting paradigm for quantifier instantion search.\n",
    "Maybe I want to retain \n",
    "\n",
    "\n",
    "T : dict[SortRef, ExprRef]\n",
    "def rw(self, f, *sorts):\n",
    "    product(*[self.T[sort] for sort in sorts])\n",
    "\n",
    "proofs\n",
    "\n",
    "self.R\n",
    "\n",
    "\n",
    "Miller Lambda\n",
    "z3 <-> sympy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# aegraph\n",
    "I have a draft comparing to smart consturcotrs.\n",
    "\n",
    "I'ts interesting to cinsider the union find on its own\n",
    "\n",
    "\n",
    "\n",
    "There is an \n",
    "\n",
    "\n",
    "What abotu view(n) which is a depth limited form of extracting all terms.\n",
    "\n",
    "This is nice from a compiler persecptive.\n",
    "\n",
    "But also eclasses[i] = []\n",
    "seems find too. If you're going to go top down.\n",
    "It is kind of cute\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UF(uf=[3, 3, 2, 3], unodes=[None, None, None, (0, 1)])\n",
      "[0, 1]\n",
      "[0]\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "@dataclass\n",
    "class UF():\n",
    "    uf : list[int]\n",
    "    unodes : list[Optional[tuple[int,int]]]\n",
    "    def __init__(self):\n",
    "        self.uf = []\n",
    "        self.unodes = [] # union_nodes\n",
    "    def makeset(self):\n",
    "        z = len(self.uf)\n",
    "        self.uf.append(z)\n",
    "        self.unodes.append(None) # or really it might hold an actual thing.\n",
    "        return z\n",
    "    def find(self,x):\n",
    "        while self.uf[x] != x:\n",
    "            x = self.uf[x]\n",
    "        return x\n",
    "    def union(self, x, y):\n",
    "        x = self.find(x)\n",
    "        y = self.find(y)\n",
    "        if x != y:\n",
    "            z = self.makeset()\n",
    "            self.uf[x] = z\n",
    "            self.uf[y] = z\n",
    "            self.unodes[z] = (x,y)\n",
    "            return z\n",
    "        else:\n",
    "            return x\n",
    "    def enum_then(self, x):\n",
    "        t = self.unodes[x]\n",
    "        match t:\n",
    "            case None:\n",
    "                yield x\n",
    "            case (l,r):\n",
    "                yield from self.enum_then(l)\n",
    "                yield from self.enum_then(r)\n",
    "            case _:\n",
    "                raise ValueError(\"Invalid value\", x)\n",
    "    def enum_now(self, x):\n",
    "        x = self.find(x)\n",
    "        yield from self.enum_then(x)\n",
    "\n",
    "\n",
    "uf = UF()\n",
    "x,y,z = uf.makeset(), uf.makeset(), uf.makeset()\n",
    "q = uf.union(x,y)\n",
    "print(uf)\n",
    "print(list(uf.enum_now(x)))\n",
    "print(list(uf.enum_then(x)))\n",
    "print(uf.find(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use uniuon find to store other stuff. The Ids.\n",
    "\n",
    "Is the following an egraph?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UF(uf=[3, 3, 2, 3], unodes=['a', 'b', 'c', UNode(l=0, r=1)], hashcons={'a': 0, 'b': 1, 'c': 2, UNode(l=0, r=1): 3})\n",
      "['a', 'b']\n",
      "['a']\n",
      "3\n",
      "[ENode(f='a', args=())]\n",
      "UF(uf=[0, 1, 2], unodes=[ENode(f='a', args=()), ENode(f='b', args=()), ENode(f='foo', args=(0,))], hashcons={ENode(f='a', args=()): 0, ENode(f='b', args=()): 1, ENode(f='foo', args=(0,)): 2})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('foo', (2,)), ('foo', (('foo', (('a', ()),)),))]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "import itertools\n",
    "@dataclass(frozen=True)\n",
    "class UNode():\n",
    "    l: int\n",
    "    r: int\n",
    "\n",
    "@dataclass\n",
    "class UF():\n",
    "    uf : list[int]\n",
    "    unodes : list[Optional[tuple[int,int]]]\n",
    "    hashcons : dict[object,int]\n",
    "    def __init__(self):\n",
    "        self.uf = []\n",
    "        self.unodes = [] # union_nodes\n",
    "        self.hashcons = {}\n",
    "    def add(self, x) -> int:\n",
    "        if x not in self.hashcons:\n",
    "            z = len(self.uf)\n",
    "            self.hashcons[x] = z\n",
    "            self.uf.append(z)\n",
    "            self.unodes.append(x)\n",
    "            return z\n",
    "        else:\n",
    "            return self.hashcons[x]\n",
    "    def find(self,x : int) -> int:\n",
    "        while self.uf[x] != x:\n",
    "            x = self.uf[x]\n",
    "        return x\n",
    "    def union(self, x, y):\n",
    "        x = self.find(x)\n",
    "        y = self.find(y)\n",
    "        if x != y:\n",
    "            z = self.add(UNode(x,y))\n",
    "            self.uf[x] = z\n",
    "            self.uf[y] = z\n",
    "            return z\n",
    "        else:\n",
    "            return x\n",
    "    def enum_then(self, x):\n",
    "        t = self.unodes[x]\n",
    "        match t:\n",
    "            case UNode(l,r):\n",
    "                yield from self.enum_then(l)\n",
    "                yield from self.enum_then(r)\n",
    "            case _:\n",
    "                yield self.unodes[x]\n",
    "    def enum_now(self, x):\n",
    "        x = self.find(x)\n",
    "        yield from self.enum_then(x)\n",
    "    def view(self, x, n):\n",
    "        if n == 0:\n",
    "            yield x\n",
    "        for enode in self.enum_now(x):\n",
    "            for args in itertools.product(*[self.view(a, n-1) for a in enode.args]):\n",
    "                yield (enode.f, args)\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ENode():\n",
    "    f: str\n",
    "    args : tuple[int]\n",
    "\n",
    "uf = UF()\n",
    "x,y,z = uf.add(\"a\"), uf.add(\"b\"), uf.add(\"c\")\n",
    "q = uf.union(x,y)\n",
    "print(uf)\n",
    "print(list(uf.enum_now(x)))\n",
    "print(list(uf.enum_then(x)))\n",
    "print(uf.find(x))\n",
    "\n",
    "uf = UF()\n",
    "a = uf.add(ENode(\"a\", ()))\n",
    "b = uf.add(ENode(\"b\", ()))\n",
    "print(list(uf.enum_now(a)))\n",
    "list(uf.view(a, 1))\n",
    "foo_a = uf.add(ENode(\"foo\", (a,)))\n",
    "print(uf)\n",
    "foo_a_a = uf.add(ENode(\"foo\", (foo_a,)))\n",
    "list(uf.view(foo_a_a, 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then smart constructors take a view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(x,y):\n",
    "    e = uf.add(ENode(\"add\", (x,y)))\n",
    "    for v in egraph.view(e, n):\n",
    "        match v:\n",
    "            case (\"add\", (\"zero\",y)):\n",
    "                egraph.union(e, y)\n",
    "            case _:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rust\n",
    "pip install maturin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[32m     Created\u001b[0m library `/tmp/temp_rust_project` package\n",
      "\u001b[1m\u001b[32m    Updating\u001b[0m crates.io index\n",
      "\u001b[1m\u001b[32m      Adding\u001b[0m pyo3 v0.22.2 to dependencies.\n",
      "             Features:\n",
      "             \u001b[1m\u001b[32m+\u001b[0m extension-module\n",
      "             \u001b[1m\u001b[32m+\u001b[0m indoc\n",
      "             \u001b[1m\u001b[32m+\u001b[0m macros\n",
      "             \u001b[1m\u001b[32m+\u001b[0m pyo3-macros\n",
      "             \u001b[1m\u001b[32m+\u001b[0m unindent\n",
      "             30 deactivated features\n",
      "\u001b[1m\u001b[32m    Updating\u001b[0m crates.io index\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cargo new --lib /tmp/temp_rust_project\n",
    "cargo add pyo3 --features extension-module --manifest-path /tmp/temp_rust_project/Cargo.toml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /tmp/temp_rust_project/src/lib.rs\n"
     ]
    }
   ],
   "source": [
    "%%file /tmp/temp_rust_project/src/lib.rs\n",
    "use pyo3::prelude::*;\n",
    "\n",
    "#[pyfunction]\n",
    "fn sum_array(arr: Vec<f64>) -> f64 {\n",
    "    arr.iter().sum()\n",
    "}\n",
    "\n",
    "// fn guessing_game(m: &Bound<'_, PyModule>) -> PyResult<()>\n",
    "#[pymodule]\n",
    "fn temp_rust_project(m: &Bound<'_, PyModule>) -> PyResult<()>{\n",
    "    m.add_function(wrap_pyfunction!(sum_array, m)?)?;\n",
    "    Ok(())\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔗 Found pyo3 bindings\n",
      "🐍 Found CPython 3.12 at /home/philip/miniconda3/envs/sage/bin/python\n",
      "\u001b[1m\u001b[32m   Compiling\u001b[0m temp_rust_project v0.1.0 (/tmp/temp_rust_project)\n",
      "\u001b[K\u001b[1m\u001b[32m    Finished\u001b[0m dev [unoptimized + debuginfo] target(s) in 0.51sroject           \n",
      "📦 Built wheel for CPython 3.12 to /tmp/.tmpcdQDLq/temp_rust_project-0.1.0-cp312-cp312-linux_x86_64.whl\n",
      "✏️  Setting installed package as editable\n",
      "🛠 Installed temp_rust_project-0.1.0\n"
     ]
    }
   ],
   "source": [
    "!maturin develop -m /tmp/temp_rust_project/Cargo.toml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import temp_rust_project\n",
    "temp_rust_project.sum_array([1.0, 2.0, 3.0, 4.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gauss / Grobner\n",
    "https://worldscientific.com/doi/abs/10.1142/S0129054192000085?srsltid=AfmBOooNJKz_vsh46RnDAxfdj1hdxoTX0I8Q20qSnLFW1uYIr8LXeqhE  THE WORD PROBLEM OF ACD-GROUND THEORIES IS UNDECIDABLE\n",
    "CLAUDE MARCHÉ \n",
    "https://toccata.gitlabpages.inria.fr/toccata/publications/1992-journal_bib.html\n",
    "\n",
    "  author = {Jean-Pierre Jouannaud and Claude March{\\'e}},\n",
    "  topics = {rewriting, old-team, cclserver},\n",
    "  location = {CM, HC 637},\n",
    "  title = {Termination and completion modulo associativity,\n",
    "\t\t commutativity and identity},\n",
    "\n",
    "https://core.ac.uk/download/pdf/82774966.pdf Normalized Rewriting: an Alternative to Rewriting\n",
    "modulo a Set of Equations. Is this related to strategy rewriting? Our strategy is to be very eager on certain normalizations.\n",
    "\n",
    "https://egraphs.zulipchat.com/#narrow/stream/328972-general/topic/Is.20WCOJ.20the.20same.20as.20Grobner.20Bases.3F/near/468226525 Pavel discuission\n",
    "\n",
    "What about when I have multiple interacting things. If separate by sort?\n",
    "otimes and comp are both associative. Not clear if the associative operation is tagged by sort here. If sort is Hom, no.\n",
    "Try to revisit category problem.\n",
    "\n",
    "Mini KB glued together via maps.\n",
    "Each individual theory has a method of building canonizizer for any ground equation involving symbols in the theory.\n",
    "+ \"enode\" mapping to new domain\n",
    "\n",
    "\n",
    "The sympy entangled form.\n",
    "Mixing sympy and Z3? Once I have miller egraphs, we're really cooking with gas.\n",
    "If graver bases work for integer programming, why can't I do LP?\n",
    "Hrep Vrep. Incremental. \n",
    "We could say linear expression becomes invalid? Or predicates over linear expressions. pred(x + y)\n",
    "guaraded rule : e >= 0 -> pos(e)\n",
    "\n",
    "What do we need to do gappa/polya\n",
    "\n",
    "If we just had an AC symbol, we could intergate that in as two term porlynoials. This is related to hermite normal form, special algorithms for finindg hte normalization\n",
    "x*x = x would also be allowed, giving us idempotence\n",
    "x U y = z\n",
    "I mean, I guess we could go for general knuth bendix considerations. Pick a term ordering. Then x*y*z -> x is ordered. Overlap condition is S-polynmoial. Stays S polynomials\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Bergman diamond lemma https://en.wikipedia.org/wiki/Bergman%27s_diamond_lemma interesting pointy for non commutative\n",
    "\"diamond lemma\" is analog of \"critical pair lemma\"\n",
    "papewr\n",
    "\n",
    "w = u x v is analog of monomial being \"submonomial\". More like finding a string rewriting pattern. associative.\n",
    "power series algebras\n",
    "Matrix sums I suppose. ground matrix/operator equations. Hmm.\n",
    "Categorical constructions... (f . g . h)  but also with a plus? \n",
    "AB = C\n",
    "String knuth bendix - relation to sequence egraphs. Special string operator + egraph behavior. Sometimes might work.\n",
    "\n",
    "https://www.sciencedirect.com/science/article/pii/0304397594902836 An introduction to commutative and noncommutative Gröbner bases teo mora\n",
    "Ore algerba\n",
    "https://www.gap-system.org/Packages/gbnp.html\n",
    "https://mathweb.ucsd.edu/~ncalg/ mathematica\n",
    "https://macaulay2.com/doc/Macaulay2/share/doc/Macaulay2/NCAlgebra/html/_nc__Groebner__Basis.html\n",
    "\n",
    "Nate points out that (t1,t2) is painful andf that from his perswepctive t1(t2)^-1 is better. abeliean gives us symmettry. Huh. \n",
    "I used addition as an abelianm gruop\n",
    "abeleian group is how he put it\n",
    "\n",
    "x^n z^m =   grobner\n",
    "integer linear equalities. Yeah, same sort of thing.\n",
    "\n",
    "I guess this is different from the group union find / grouping union fiod\n",
    "\n",
    "Can I take any convergent rewrite system and put it in the \"eclass\" part? Structured enodes that are fully normalized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The verything is KB insanity\n",
    "https://cstheory.stackexchange.com/questions/12326/unification-and-gaussian-elimination\n",
    "https://www3.risc.jku.at/education/courses/ss2011/rw/1-intro.pdf canonical reduction systerms in symbolic mathemtics\n",
    "\n",
    "Lattice Basis Reduction: An Introduction to the LLL Algorithm and Its Applications\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (913052065.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[2], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    sage.\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# graver Multiset\n",
    "Liofted out into its own notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated triple-quoted string literal (detected at line 43) (3295482584.py, line 21)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 21\u001b[0;36m\u001b[0m\n\u001b[0;31m    \"\"\"\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated triple-quoted string literal (detected at line 43)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cmake -G Ninja .. \\\n",
    "  -DMLIR_DIR=$PWD/../../circt/llvm/build/lib/cmake/mlir \\\n",
    "  -DCLANG_DIR=$PWD/../../circt/llvm/build/lib/cmake/clang \\\n",
    "  -DLLVM_TARGETS_TO_BUILD=\"host\" \\\n",
    "  -DLLVM_ENABLE_ASSERTIONS=ON \\\n",
    "  -DCMAKE_BUILD_TYPE=DEBUG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set rewrite\n",
    "ACI\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace(s, lhs, rhs):\n",
    "    res = s.copy()\n",
    "    for k in lhs:\n",
    "        if k in res:\n",
    "            res.remove(k)\n",
    "    res.update(rhs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## Soft Egraphs\n",
    "a.x ~ b.x\n",
    "Or take all the linear expressions. SVD them. We could note that 3.1x + 4y ~ 3x + 4y if we have both. We could choose some slider cutoff to guess at exact equalities. I don't know that we'd learn an equality so much as round the two as to be equal. We don't want to say x = 0 as we want to say.\n",
    "It's an extra kind of rebuilding. Not an equality discoverer. If we have an oracle, we could find out if our guesses make sense.\n",
    "\n",
    "Or maybe somehow use lasso like something |x| regularization. It's a really good trick. Whether things should be sparsified?\n",
    "\n",
    "\n",
    "\n",
    "But we kind of expect to have more variables than equations? \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Egraphs / String Rewriting\n",
    "Well typed critical pairs / overlaps. Is it possible for two terms to be syntactically unificable but have distinct incompatible types? Maybe. id . f and id . g. Seems like some kind of syntax suppressio nthough. Polymorphic system T stuff? hindley milner. Order Sorted KB too maybe. GATs?\n",
    "\n",
    "groupoids / categories.\n",
    "\n",
    "There is a temptation that specializing egraphs for string like theories would be good.\n",
    "Maybe it'd be easier to integerate associ or somm somehow.\n",
    "\n",
    "Opens up string stroage like ideas.\n",
    "Tries with eclasses\n",
    "Regular string automata rather than tree automata\n",
    "\n",
    "\n",
    "Paths are sequences.\n",
    "Denali was sequences of instructions.\n",
    "\n",
    "\n",
    "String rewriting is not 'ground\". implicit variable is there. String rewriting aka egraphs + assoc is not decidable.\n",
    "String rewriting at the tail only is ground.\n",
    "\n",
    "String Knuth bendix could be one answer to a sequence egraph.\n",
    "Or dual representation of cons, snoc. Then wqe can share both directions (?)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eggmt.string_kb as kb\n",
    "\n",
    "\n",
    "class ENode(NamedTuple):\n",
    "    f: str\n",
    "    args: tuple[EId]\n",
    "\n",
    "\n",
    "class EGraph():\n",
    "    def __init__(self):\n",
    "        self.R = []\n",
    "        self.E = []\n",
    "        self.enodes = {}\n",
    "        self.T = set()\n",
    "    def find(self, e):\n",
    "        return kb.rewrite(e, self.R)\n",
    "    def union(self, e1, e2):\n",
    "        e1 = self.find(e1)\n",
    "        e2 = self.find(e2)\n",
    "        if e1 != e2:\n",
    "            self.E.add((e1, e2))\n",
    "    def rebuild(self):\n",
    "        R = kb.KB(E,R)\n",
    "        self.E, self.R = kb.simplify(R)\n",
    "        self.T = set(self.find(e) for e in self.T)\n",
    "        for ENode(f,args) in self.enodes:\n",
    "            args = tuple(self.find(a) for a in args)\n",
    "            enode1 = ENode(f,args)\n",
    "            eid1 = self.enodes.get(enode1)\n",
    "            if eid1 != None:\n",
    "                self.union(eid1, eid)\n",
    "            else:\n",
    "                self.enodes[enode1] = eid\n",
    "\n",
    "    def rw(self, f):\n",
    "        for e in self.egraph:\n",
    "            lhs, rhs = f(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KBMAG computationa group\n",
    "https://github.com/gap-packages/kbmag\n",
    "\n",
    "https://www.sciencedirect.com/science/article/pii/S1567832610001025 efficiency issues of kbmag - swan https://www.cs.stir.ac.uk/~kjt/techreps/pdf/TR197.pdf Augmenting Metaheuristics with Rewriting Systems\n",
    "https://maffsa.sourceforge.net/old_manpages/maf.html MAF\n",
    "rkbp\n",
    "\n",
    "Word processing in groups\n",
    "automatic groups https://en.wikipedia.org/wiki/Automatic_group\n",
    "word acceptor. Is this for partiality? the valid stings = {w | accept(w)} ?\n",
    "Could have automata that identifies strings that correspond to particular elements\n",
    "wa=v is the same asa recognizer a=i(w)v\n",
    "\n",
    "\n",
    "What's the deal with the automata stuff\n",
    "\n",
    "Twee sorting.\n",
    "If we specialize commutiativyt, a very bad rule, to the individual guys, it becopmes sorting rules\n",
    "\n",
    "\n",
    "This rewrite system will sort.\n",
    "\n",
    "Computational group theory. Orbits. Caleb's graph hashing. Cayley Graph. Nauty\n",
    "\n",
    "Free monoid\n",
    "I can totalize the groupoid of paths into path fragment sequences?\n",
    "Use string KB a la grobner egraphs. String\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generic morphism:\n",
       "  From: Finitely presented group < x0, x1, x2 | x0^2, x1*x0 >\n",
       "  To:   Finitely presented group < x0, x2 | x0^2 >\n",
       "  Defn: x0 |--> x0\n",
       "        x1 |--> x0\n",
       "        x2 |--> x2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://doc.sagemath.org/html/en/reference/groups/sage/groups/finitely_presented.html\n",
    "from sage.all import *\n",
    "F = FreeGroup(3)\n",
    "x0,x1,x2 = F.gens()\n",
    "G = F / [x0**2, x1 * x0]\n",
    "G\n",
    "F([1])\n",
    "F.gap()\n",
    "G.relations()\n",
    "k = G.rewriting_system()\n",
    "k.make_confluent()\n",
    "k\n",
    "G.simplification_isomorphism()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /tmp/tweesort.p\n"
     ]
    }
   ],
   "source": [
    "%%file /tmp/tweesort.p\n",
    "\n",
    "fof(ax, axiom, ![X] : a(b(X)) = b(a(X))).\n",
    "fof(ax, axiom, ![X] : a(c(X)) = c(a(X))).\n",
    "fof(ax, axiom, ![X] : c(b(X)) = b(c(X))).\n",
    "\n",
    "fof(goal,conjecture, true=false).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the input problem:\n",
      "  Axiom 1 (ax): a(b(X)) = b(a(X)).\n",
      "  Axiom 2 (ax): a(c(X)) = c(a(X)).\n",
      "  Axiom 3 (ax): c(b(X)) = b(c(X)).\n",
      "  Goal 1 (goal): true = false.\n",
      "\n",
      "1. b(a(X)) -> a(b(X))\n",
      "2. c(a(X)) -> a(c(X))\n",
      "3. c(b(X)) -> b(c(X))\n",
      "\n",
      "Ran out of critical pairs. This means the conjecture is not true.\n",
      "Here is the final rewrite system:\n",
      "  b(a(X)) -> a(b(X))\n",
      "  c(b(X)) -> b(c(X))\n",
      "  c(a(X)) -> a(c(X))\n",
      "\n",
      "RESULT: CounterSatisfiable (the conjecture is false).\n"
     ]
    }
   ],
   "source": [
    "!twee /tmp/tweesort.p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String Knuth Bendix\n",
    "Good for instruction sequences? Denali.\n",
    "\n",
    "Structured \"characters\". add(r1,r2,r3) .\n",
    "peephole.\n",
    "\n",
    "even add(X,Y,Z) . sub(Z,Y,Z)  is easier to unify against than typical... Hmm. String overlap matching + atomic unification / narrowing. Kind of cool.\n",
    "\n",
    "Kind of reminds me of dcg. dcg is string like parsing but allows unificvation vars or structure inside charcters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so we could make a stirng ergapoh in the same style as the grobner egraph.\n",
    "We can sequence enodes and make equations between enode sequences.\n",
    "\n",
    "(1,) -> (2,) is a raw eclass equation like before. Sequences of size 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(1,4,2,5) # use tuples instead of strings \n",
    "(-1,2,4) # negative as inverse group members?\n",
    "from typing import Any\n",
    "def critical_pairs(x : tuple[Any,...], y : tuple[Any,...]):\n",
    "\n",
    "def reduce(x : tuple[Any,...], R):\n",
    "    \n",
    "\n",
    "class EGraph():\n",
    "    def __init__(self):\n",
    "        self.E = [] # string/sequence equations\n",
    "        self.R = [] # string rewrities\n",
    "        self.uf = []\n",
    "        self.enodes = {}\n",
    "    def find(self, x):\n",
    "        while self.uf[x] != x:\n",
    "            x = self.uf[x]\n",
    "        return x\n",
    "    def union(self, x, y):\n",
    "        pass\n",
    "    def canon(self):\n",
    "        # string knuth bendix complete + congruence closure.\n",
    "        \n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was nice that strings had a built in contains function and regex subst. oh well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1,2) in (4,1,2,4)\n",
    "\"ab\" in \"xaby\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    overlaps = []\n",
    "\n",
    "    # Check if one string is a substring of the other\n",
    "    if s1 in s2:\n",
    "        overlaps.append((\"s1 in s2\", s1))\n",
    "    if s2 in s1:\n",
    "        overlaps.append((\"s2 in s1\", s2))\n",
    "\n",
    "    # Check for tail of s1 overlapping with beginning of s2\n",
    "    for i in range(1, min(len(s1), len(s2)) + 1):\n",
    "        if s1[-i:] == s2[:i]:\n",
    "            overlaps.append((\"tail of s1 with beginning of s2\", s1[-i:]))\n",
    "        if s2[-i:] == s1[:i]:\n",
    "            overlaps.append((\"tail of s2 with beginning of s1\", s2[-i:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refinement\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "E derived Rules R\n",
    "R rules. R rules express refinement. \"More defined than\" rewriting.\n",
    "Undef on int overflow can be rewritten to wraparound addition, but not vice versa\n",
    "\n",
    "E, R, R<\n",
    "\n",
    "R< can interreduce to a frontier. That lattice width thing https://en.wikipedia.org/wiki/Antichain\n",
    "non confluent. Irreparably.\n",
    "\n",
    "WHen can I remove a rule from R< as redundant? Is R-simplify still what we want? We want the minimal frontier only?\n",
    "We can't infer two things are comparable just because they have a critical pair. But maybe we should ask the question? Take critical pairs, ask oracle if they are ocmparable. If so, we can reduce, if not ok well then nothing.\n",
    "Oracle might be in the form of domain specific rewrite / ordering rules.\n",
    "Or for constantsc (semantic things) we can just run some function.\n",
    "\n",
    "But if we use R-simplify for an element that has a non trivial frontier, we need to expand them all. This feels like it needs a closed world assumption.\n",
    "\n",
    "R<, Rmin as two systems.\n",
    "critical pairs from Rmin become questions into R<.\n",
    "\n",
    "Two orderings. Definedness and cost.\n",
    "[E + R<] , Rmin\n",
    "\n",
    "Orient. E we can irent into an Rmin rule (some are unorientable. unfailing completion). R< we can't orient, we can only push into Rmin if it's definedness ordering matches term ordering.\n",
    "\n",
    "{a -> b, b -> a} in R<, we can move it into E {a = b}. scc compression.\n",
    "\n",
    "Another intuition says we want the closest frontier (edges, not paths), since the further stuff is implied by transitivity.\n",
    "{a -> c, a -> b, b -> c} ==> {a -> b, b -> c}\n",
    "\n",
    "Kind of reminds me of hilbert basis being frontier edge. Is that meaningful?\n",
    "\n",
    "Semi confluence. That one step confluent thing. We're kind of trying to repair semi confluence. \n",
    "Strong conlfuence is what I meant.\n",
    "\n",
    "\n",
    "Atomic ground\n",
    "Ground\n",
    "\n",
    "Both should be decidable and fine under a good term ordering.\n",
    "\n",
    "\n",
    "\n",
    "Egraph modulo theory style. replace / enhance union find with bellamn solver. Even building terms in nondet now. we can now choose to enumerate all terms above and all terms known below (comparable). Strucutred or layered eid. eq(lt)\n",
    "\n",
    "\n",
    "GRS simplify edghe as refinement.\n",
    "Maybe if refinement is a total ordering?\n",
    "\n",
    "If bottom can be replaced with anything (whatecver is most convenient), which seems to be the case in compilers, it's non confluent by design.\n",
    "The point of the undefineness to to make compilation more optimized.\n",
    "\n",
    "bottom -> 3\n",
    "bottom -> 4\n",
    "Different bottoms labelled by context maybe? bottom(ctx1) bottom(ctx2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Can't lose sight of this one.\n",
    "The Alive2 toy replication project was aimed at seeing\n",
    "\n",
    "cvxlean. Relaxations of convex programming problems.\n",
    "\n",
    "https://x.com/abhi9u/status/1826527029858103501 posets in lean. Using union find + DAG? Is this not hard actually?\n",
    "\n",
    "https://mastodon.social/@cfbolz/113017389946004834\n",
    "\n",
    "https://www.microsoft.com/en-us/research/wp-content/uploads/2009/01/pentagons.pdf is it a pentagon domain?\n",
    "Kind of jives\n",
    "https://microsoft.github.io/z3guide/docs/theories/Special%20Relations/\n",
    "\n",
    "https://www.amazon.com/Introduction-Lattice-Computer-Science-Applications/dp/1118914376\n",
    "\n",
    "a. the brute force ism yabe fine, question is how to make it ergonomic to use\n",
    "b. bellamn ford is probably fine / the canonical answer.\n",
    "\n",
    "mark function symbols as monotone or ant monotone\n",
    "\n",
    "patters. does t match p  if t <= p or is t >= p.\n",
    "rewrites lhs <= rhs or lhs >= rhs\n",
    "is a variable binding saying t <= x or t >= x or t == x\n",
    "\n",
    "pvael says tsort of scc.\n",
    "https://github.com/ruby/tsort\n",
    "TSort implements topological sorting using Tarjan's algorithm for strongly connected components.\n",
    "TSort is designed to be able to be used with any object which can be interpreted as a directed graph.\n",
    "\n",
    "If patterns have ordered semantics, even matching a ground pattern can be interesting... hmm.\n",
    "subpattern lhs -> rhs  seeks substitition s\n",
    "x <= s@lhs implies x <= s@rhs \n",
    "\n",
    "All function symbols are monotone?\n",
    "\n",
    "Given some term ordering, return all terms less than that in databank.\n",
    "\n",
    "All of this is writeable as regular rules though?\n",
    "And we're not gonna do anything _that_ special for the order.\n",
    "\n",
    "Making monotonicity a rebuild rather than a rule is nice. We could do that same for songruence (souffle-egglog) but that stinks.\n",
    "\n",
    "\n",
    "\n",
    "directed minimum spanning tree\n",
    "union find shows up in kruskal's algorithm. It _is_ a spanning tree\n",
    "https://www.cs.princeton.edu/courses/archive/spring13/cos528/\n",
    "https://www.cs.princeton.edu/courses/archive/spring13/cos528/directed-mst-1.pdf\n",
    "https://en.wikipedia.org/wiki/Edmonds%27_algorithm\n",
    "spanning arborescence of minimum weight.\n",
    "\n",
    "Maybe this is cheaper to maintain than a full shortest path.\n",
    "\n",
    "You maintain a union find for things that are possibly comparable (connected through some chain of <= >=). That could prune a lot.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Design choices. Does R< or R<=. R< asserts a sort of inequality.\n",
    "Does R< edges get annotated by the conditions under which they are a struct refinement?\n",
    "\n",
    "\n",
    "Choices of cnaonizaing R<.\n",
    "1. subsumed redunant edges to minimal hasse lattice\n",
    "2. insert all transitive edges\n",
    "3. have another simplification ordering < and insert only those edges in order. guass elim kind of.\n",
    "\n",
    "`path(x,z) :- edge(x,y), path(y,z), y > z.` path is a kind of refinement ordering, > is a different simplification ordering.\n",
    "But insert path(x,z) only in \n",
    "Trying to bake in ordered resolution style thinking.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EGraph():\n",
    "    uf :  # R=\n",
    "    refine : # R<\n",
    "    def rebuild():\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we maintain a strict less than graph and an eq graph.\n",
    "We can then prune less than...\n",
    "Basically seems like rules.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'graph_tool'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#import scipy.sparse\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgraph_tool\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'graph_tool'"
     ]
    }
   ],
   "source": [
    "#import scipy.sparse\n",
    "#import graph_tool\n",
    "import networkx as nx\n",
    "\n",
    "class Egraph():\n",
    "    #ineq : nx.DiGraph\n",
    "    #enodes : dict[ , int]\n",
    "    def init(self):\n",
    "        self.ineq = nx.DiGraph()\n",
    "        self.enodes = {}\n",
    "        self.mono_nodes = {} # monotonic enodes?\n",
    "    def makeset(self):\n",
    "        x = len(self.ineq)\n",
    "        self.ineq.add_node(x)\n",
    "        return x\n",
    "    def union(self, x, y):\n",
    "        self.ineq.add_edge(x,y)\n",
    "        self.ineq.add_edge(y,x)\n",
    "    def assert_le(self, x, y): #hmm. le. vs lt.\n",
    "        self.ineq.add_edge(x,y)\n",
    "    \n",
    "    def rebuild(self):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Color / Contextual\n",
    "\n",
    "Colored egraphs\n",
    "\n",
    "We want to retain multiple copies of the union find. They might communicate.\n",
    "\n",
    "context itself should retain an up pointing tree\n",
    "\n",
    "\n",
    "contexts could be labelled by sometrhing other than an oqaque integer.\n",
    "Maybe some unique identifier.\n",
    "\n",
    "Can merge contexts on same path.\n",
    "Ordered contexts. Name the by an ordered sequence of assumptions. \n",
    "\n",
    "\n",
    "Slotted union find. For kruskal on graphs that look like tree decompoistion graphs? Bundles of flow.\n",
    "\n",
    "quasi Kruskal on bag graph could be some kind of greedy algoirhtm for a tree decomp?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UF():\n",
    "    ufs : list[tuple[int , dict[int,int]]] # sparse uf inside a dense ctx tree\n",
    "    counter : int\n",
    "    def __init__(self):\n",
    "        self.ufs = [(0, {})]\n",
    "        self.counter = 0\n",
    "    def find(self, ctx, x):\n",
    "        # no exactly wrong (?). I need to get the context chain first\n",
    "        # No maybe this is fine.\n",
    "        # well maybe nice because we can detect early that global context is happy\n",
    "        while True:\n",
    "            parent, uf = self.ufs[ctx]\n",
    "            while x in uf:\n",
    "                x = uf[x]\n",
    "            if parent == ctx:\n",
    "                return x \n",
    "            else:\n",
    "                ctx = parent\n",
    "    def find2(self,x,y):\n",
    "        pass\n",
    "        # can have early stopping at each context level\n",
    "    def make_ctx(self, parent=None):\n",
    "        if parent == None:\n",
    "            parent = len(self.ufs)\n",
    "        self.ufs.append((parent, {}))\n",
    "    def make_set(self):\n",
    "        self.counter += 1\n",
    "        return self.counter\n",
    "    def union(self, x, y, ctx):\n",
    "        x = self.find(ctx, x)\n",
    "        y = self.find(ctx, y)\n",
    "        if x != y:\n",
    "            self.ufs[ctx][x] = y\n",
    "    # def merge_ctx(self, ctx1, ctx2):\n",
    "    # maybe this one becomes easier if we can make one context the parent of another? hmm\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proofs\n",
    "https://ceur-ws.org/Vol-3717/short1.pdf Towards Producing Shorter Congruence Closure Proofs in\n",
    "a State-of-the-art SMT Solver (Extended Abstract)\n",
    "\n",
    "https://microsoft.github.io/z3guide/programming/Proof%20Logs/\n",
    "\n",
    "\n",
    "Proofs are really important. I don't emphaszie them enough\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pointery Egraphs\n",
    "Traat\n",
    "z3 - where did I see the description of this? programming z3?\n",
    "\n",
    "struct Node = {head : Stiring, args :  ,  parents: ,  ufpointer :  , left, right } # left and right as doubly linked list to enumerate eclass\n",
    "\n",
    "What does cvc5 look like?\n",
    "\n",
    "Write SAT solver."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Well founded egraph / cyclic proofs\n",
    "I wanted well founded extraction.\n",
    "Cyclic proof would like some kind of protection\n",
    "occurs check in smt solvers.\n",
    "\n",
    "# co-egraphs\n",
    "https://www.philipzucker.com/coegraph/\n",
    "https://www.philipzucker.com/naive_automata/\n",
    "\n",
    "### unifiction\n",
    "### clp(set)\n",
    "\n",
    "\n",
    "observations are refinable?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we hold on on compaction until everything we've built is \"closed\"\n",
    "\n",
    "\n",
    "class CoEGraph():\n",
    "    enodes: Dict[ENode, EId]\n",
    "    coenodes: Dict[PId, CoENode] #observations : Dict[OId, Observation]\n",
    "    uf:\n",
    "    pid_counter\n",
    "\n",
    "    def rebuild(self):\n",
    "        # do congurence closure\n",
    "\n",
    "        # do automata reduction\n",
    "\n",
    "        # repeat\n",
    "\n",
    "p = E.make_pid()\n",
    "o = E.add_term((\"cons\", (\"zero\",), p))\n",
    "E.rebuild()\n",
    "E.observe(p, o)\n",
    "E.rebuild()\n",
    "\n",
    "# Once we discover 1*0 = 0, this compacts with the above.\n",
    "p = E.make_pid()\n",
    "o = E.add_term((\"cons\", (\"mul\", (\"one\",), (\"zero\",)), p))\n",
    "E.observe(p, o)\n",
    "\n",
    "\n",
    "zero = E.add_term((\"zero\", ))\n",
    "p = E.make_pid()\n",
    "\n",
    "E.rebuild()\n",
    "E.observe(p, (\"cons\", zero, p))\n",
    "E.rebuild()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is very much like the discussions in CLP set. viscious circles says equation system.\n",
    "\n",
    "Set constraints. kozen aiken\n",
    "setlog\n",
    "\n",
    "\n",
    "So I am using containers ion the cofunctions.\n",
    "Equations and co-equations?\n",
    "\n",
    "pid1 = pid2 is an epsilon transition? But their observations better be consistent anyway. Maybe unobserved states can be equated?\n",
    "pid1 -> pid2\n",
    "\n",
    "pid -> set pid\n",
    "pid -> multiset pid\n",
    "pid -> tuple pid \n",
    "pid -> polynomial pid\n",
    "x = 1+a*x as geometric series. As infinite series. solution x = 1/(1-a). Uses rationals. As operator equation?\n",
    "f = f0 + \\int d(f)  taylor series.\n",
    "\n",
    "presburger automata. solutions of 3x + y = 1... hmm.\n",
    "rational automata ax=b. Automata for the rational expansion of a ratyional number?\n",
    "\n",
    "UW stream calculus\n",
    "blanchette coadatatypes smt\n",
    "\n",
    "grobner pid? Could this be a way to finitize polynomial solutions? infinite but rational series.\n",
    "\n",
    "grahams non well founded haskell sets.\n",
    "powser.\n",
    "But now its sort of packaged up. We can recover global equalities.\n",
    "\n",
    "this is the coinductive  analog of my hash set universe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[...]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = []\n",
    "f.append(f)\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: [1, 2, 3, 4, 5], 2: [1, 2, 3, 4, 5], 3: [1, 2, 3, 4, 5], 4: [1, 2, 3, 4, 5], 5: [1, 2, 3, 4, 5]}\n",
      "{1: [1, 2, 3], 2: [1, 2, 3], 3: [1, 2, 3], 4: [4, 5], 5: [4, 5]}\n",
      "{1: [1], 2: [2, 3], 3: [2, 3], 4: [4, 5], 5: [4, 5]}\n",
      "{1: [1], 2: [2, 3], 3: [2, 3], 4: [4, 5], 5: [4, 5]}\n",
      "{1: [1], 2: [2, 3], 3: [2, 3], 4: [4, 5], 5: [4, 5]}\n",
      "{1: [1], 2: [2, 3], 3: [2, 3], 4: [4, 5], 5: [4, 5]}\n"
     ]
    }
   ],
   "source": [
    "ex1 = {\n",
    "  1 : (False, 2, 3),\n",
    "  2 : (False, 4, 3),\n",
    "  3 : (False, 5, 3),\n",
    "  4 : (True, 5, 4),\n",
    "  5 : (True, 4, 4)\n",
    "}\n",
    "def dfa_map(f,x):\n",
    "  return {n : (term, f[a], f[b])  for n, (term, a, b) in x.items()}\n",
    "\n",
    "\n",
    "from itertools import groupby\n",
    "states = list(ex1.keys())\n",
    "partmap = {i : states for i in range(1,6)}\n",
    "for i in range(6):\n",
    "    print(partmap)\n",
    "    obs = dfa_map(partmap, ex1) # the observation map _is_ almost the partmap\n",
    "    \n",
    "    \n",
    "    partmap = {}\n",
    "    for obs1, equivs in groupby(states, lambda state: obs[state]):\n",
    "        equivs = list(equivs)\n",
    "        for id_ in equivs:\n",
    "            partmap[id_] = equivs\n",
    "    # not working\n",
    "    #partmap = {state: list(partition) for (_, partition) in groupby(states, lambda state1: obs[state1]) for state in list(partition)}\n",
    "\n",
    "# I don't _need_ partmap. Coukld have set of disjoint sets and you just scan it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: (), 2: (), 3: (), 4: (), 5: ()}\n",
      "{1: -2, 2: -2, 3: -2, 4: -1, 5: -1}\n",
      "{1: -1, 2: -2, 3: -2, 4: -3, 5: -3}\n",
      "{1: -2, 2: -1, 3: -1, 4: -3, 5: -3}\n",
      "{1: -2, 2: -1, 3: -1, 4: -3, 5: -3}\n",
      "{1: -2, 2: -1, 3: -1, 4: -3, 5: -3}\n",
      "{1: -2, 2: -1, 3: -1, 4: -3, 5: -3}\n",
      "{1: -2, 2: -1, 3: -1, 4: -3, 5: -3}\n",
      "{1: -2, 2: -1, 3: -1, 4: -3, 5: -3}\n",
      "{1: -2, 2: -1, 3: -1, 4: -3, 5: -3}\n",
      "{1: -2, 2: -1, 3: -1, 4: -3, 5: -3}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nHmm.\\nWhat if I Z3-ified this process? A symbolic transition map. eqclass(c) = a.\\nCould use same justification tricks.\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "ex1 = {\n",
    "  1 : (False, 2, 3),\n",
    "  2 : (False, 4, 3),\n",
    "  3 : (False, 5, 3),\n",
    "  4 : (True, 5, 4),\n",
    "  5 : (True, 4, 4)\n",
    "}\n",
    "\n",
    "def dfa_map(f,x):\n",
    "  return {n : (term, f[a], f[b])  for n, (term, a, b) in x.items()}\n",
    "\n",
    "eqclass = {i : () for i in range(1,6)}\n",
    "for _ in range(10):\n",
    "  print(eqclass)\n",
    "  eqclass = dfa_map(eqclass, ex1)\n",
    "  #eqclass = {k : hash(v) for k,v in eqclass.items()} # not quite right. hash could collide. But you get the idea\n",
    "  statelabel = { k : -i-1 for i,k in enumerate(set(eqclass.values()))} # negative just so I can see different between state and eqclass easier\n",
    "  eqclass = {k : statelabel[v] for k,v in eqclass.items()} \n",
    "\n",
    "print(eqclass)\n",
    "\n",
    "'''\n",
    "\n",
    "Hmm.\n",
    "What if I Z3-ified this process? A symbolic transition map. eqclass(c) = a.\n",
    "Could use same justification tricks.\n",
    "\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a conception that the states should be identified with their observation trees.\n",
    "We can build infinite trees in python using knot tying. This is a different way of doing it than generators, although less general\n",
    "We could write the compaction loop in this form without converting back to dict form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simplest self knot\n",
    "l = []\n",
    "l.append(l)\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# a cons stream of infinite 0\n",
    "zeros = [0]\n",
    "zeros.append(zeros)\n",
    "zeros\n",
    "\n",
    "# alternating 0,1,0,1,0,1...\n",
    "z0 = [0]\n",
    "z1 = [1]\n",
    "z0.append(z1)\n",
    "z1.append(z0)\n",
    "\n",
    "#vs generator form\n",
    "def zeros1():\n",
    "    yield 0\n",
    "    yield from zeros1()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{135909010080512: (False, 135909010080448, 135909010080256),\n",
       " 135909010080448: (False, 135909010080192, 135909010080256),\n",
       " 135909010080256: (False, 135909010081600, 135909010080256),\n",
       " 135909010080192: (True, 135909010081600, 135909010080192),\n",
       " 135909010081600: (True, 135909010080192, 135909010080192)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = [\n",
    "    {\"accept\" : False},\n",
    "    {\"accept\" : False},\n",
    "    {\"accept\" : False},\n",
    "    {\"accept\" : True},\n",
    "    {\"accept\" : True}\n",
    "]\n",
    "\n",
    "s[0][\"a\"] = s[1]\n",
    "s[0][\"b\"] = s[2]\n",
    "s[1][\"a\"] = s[3]\n",
    "s[1][\"b\"] = s[2]\n",
    "s[2][\"a\"] = s[4]\n",
    "s[2][\"b\"] = s[2]\n",
    "s[3][\"a\"] = s[4]\n",
    "s[3][\"b\"] = s[3]\n",
    "s[4][\"a\"] = s[3]\n",
    "s[4][\"b\"] = s[3]\n",
    "\n",
    "# we can convert it back to the dict form using `id`\n",
    "{ id(x) : (x[\"accept\"], id(x[\"a\"]), id(x[\"b\"]))  for x in s}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_string(abba):\n",
    "    state = s[0]\n",
    "    for c in abba:\n",
    "        state = state[c]\n",
    "    return state[\"accept\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Occurs Check\n",
    "How does the occurs check work for regular terms in egraph?\n",
    "1. no constructors in same eclass\n",
    "2. no constructor loops (?). If you project to eclasses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Program, SSA, PEGs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fancy Extraction\n",
    "I should rip together cvxpy and basic egraph.\n",
    "\n",
    "Fancy ILP extracxtion from knuth bendix perspective - symbolic execution of rewrite rules? We want to search the cone of terms. Sure we could do brute force search.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Roughly, extraction is taking compact pile of stuff in an egraph an gettng a term out. Ultimately, ther systems are ingesting terms, so for applications you need this.\n",
    "\n",
    "Part of the appeal of the egg approach is that you can extract _good_ terms.\n",
    "\n",
    "In this post, I don't want to talk about how to best solve for good terms, but instead some interesting varietions on the expressivity or semantics of extraction.\n",
    "\n",
    "For an extraction elgorithm, there is a question\n",
    "`(Egraph, Term) -> (Cost, Term)`\n",
    "\n",
    "A different fascinating possibility raised by _ is considerng the extraction problem that wants to produce a set of equatons equivalent to an egraph (a set of equations that should you add those terms and union them in a frsh egraph, you get back the same egraph). This is a methodology for finding a \"good\" set of equations rather than a single good term.\n",
    "`Egraph -> [(Term, Term)]`\n",
    "They phrased this as a form of quantifier elimination.\n",
    "\n",
    "It is a reasonable problem that you may insert a pile of equations and receive a \"solution\".\n",
    "Solutions are often equations with an isolated left hand side, and a right hand side that only contains certain entities.\n",
    "These are definitions.\n",
    "\n",
    "Scoping. Egraphs and egglog have a pretty bad story here.\n",
    "You can use egglog as a theorem prover, but you have to skolemize and herbrandize yourself. This situation is reminiscent of a resolution solver, where the basic operational mechanims does not really support quantifiers either, so clausal form is produced in a preprocessing pass.\n",
    "\n",
    "Termination\n",
    "\n",
    "The most naive mode of extraction is a term model. In this model, we pay for a term every time we use it.\n",
    "\n",
    "It is often closer to the truth to consider a DAG model, in which you only need to pay for a term once and may reuse it again and again.\n",
    "\n",
    "There is also a spectrum between them.\n",
    "\n",
    "Cycle breaking.\n",
    "In the PEG paper using egraphs as an IR, they produced egraphs that are not produceable from a process of adding terms and term rewriting rules. These egraphs were not \"well founded\" in some sense that it would be interesting to have a nice definition of. There is intrinsic knot tying.\n",
    "\n",
    "The simplest example is an egraph with a single looping enode and eclass. It perhaps represents the truly infinite term `f(f(f(f(f(f(...))))))`\n",
    "\n",
    "```mermaid\n",
    "f self loop.\n",
    "```\n",
    "\n",
    "If you run an iterative greedy dynamic programming approach, it does not terminate or terminates with infnite cost. If you start trying to get a term out, that will also not terminate because there is no term to be had in there.\n",
    "\n",
    "You can kind of produce egraphs like this if you allow yourself access to a raw fresh-eid, or if you make a temporary nonsense placeholder to tie the knot with, whch you subsequently set the cost to infinity.\n",
    "\n",
    "While PEGs are couched in the language and greek letters remiscent of SSA-like compiler IRs, to my mind basically it is an equational theory of coinductive streams, akin to what you find in Haskell. The contents of the streams are the symbolic state of the system.\n",
    "\n",
    "### Examples Where Methods Help\n",
    "\n",
    "Greedy did so well that  started to be convinced that it was actual optimal or guaranteed some fraction of optimal. Taking big benchmarks where a priori no one knows the optimal answer made this unclear.\n",
    "It is not the case. You can design some simple examples where the greedy heuristic is arbitrarily bad.\n",
    "\n",
    "### Methods\n",
    "\n",
    "The extraction gym is a good place to look to see some ideas people have had.\n",
    "Brute Force.\n",
    "\n",
    "Greedy.\n",
    "\n",
    "ZDD\n",
    "ILP\n",
    "MaxSAT\n",
    "Answer Set Programming\n",
    "\n",
    "One I really like is from the Kestrel talk, monte carlo optimization. It is a somewhat sad fact of life that exact optimizatin methods are actually not _that_ useful. This is because 1. they are often computationally expensive 2. heuristics are often pretty good and very fast 2. they can require mangled modelling to fit into thir language 3. Optimizing beyond the fidelity of the model to reality is pointless and in fact often counterproductive. This is a form of overfitting, the bane of machine learning.\n",
    "Figuring out an accurate model of a cpu is very very hard.\n",
    "Profile guided optimization.\n",
    "So just run the program! Maybe weight your sampling a bit towards things you think are good.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### well scoped extraction\n",
    "Extraction is the analog of performing a query.\n",
    "(?t = q)\n",
    "aka. \n",
    "min t, t = q\n",
    "\n",
    "forall a, exists b, \n",
    "exists b, forall a, \n",
    "harrop form backchaining\n",
    "\n",
    "tie breaking costs.\n",
    "(1,0,0)\n",
    "\n",
    "\n",
    "minimax is analog of nested quantifiers?\n",
    "min t max r, = \n",
    "\n",
    "\n",
    "refutation style theorem proving mixes all kinds of concepts. I kind of hate it.\n",
    "\n",
    "herbvanmdization vs skolemization.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extraction gym format\n",
    "egraph serialize\n",
    "https://github.com/egraphs-good/egraph-serialize\n",
    "https://github.com/egraphs-good/extraction-gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional, NewType\n",
    "from pydantic import BaseModel, Field\n",
    "from ordered_set import OrderedSet\n",
    "\n",
    "# Use NewType to define distinct types for NodeId and ClassId\n",
    "NodeId = NewType('NodeId', str)\n",
    "ClassId = NewType('ClassId', str)\n",
    "\n",
    "class Node(BaseModel):\n",
    "    op: str\n",
    "    children: List[NodeId] = Field(default_factory=list)\n",
    "    eclass: ClassId\n",
    "    cost: float = Field(default=1.0)\n",
    "    subsumed: bool = Field(default=False)\n",
    "\n",
    "    def is_leaf(self) -> bool:\n",
    "        return len(self.children) == 0\n",
    "\n",
    "class Class(BaseModel):\n",
    "    id: ClassId\n",
    "    nodes: List[NodeId] = Field(default_factory=list)\n",
    "\n",
    "class ClassData(BaseModel):\n",
    "    typ: Optional[str] = None\n",
    "\n",
    "\n",
    "# Define the EGraph structure\n",
    "class EGraph(BaseModel):\n",
    "    nodes: Dict[NodeId, Node] = Field(default_factory=dict)\n",
    "    root_eclasses: List[ClassId] = Field(default_factory=list)\n",
    "    class_data: Dict[ClassId, ClassData] = Field(default_factory=dict)\n",
    "\n",
    "    def add_node(self, node_id: NodeId, node: Node):\n",
    "        if node_id in self.nodes:\n",
    "            raise ValueError(f\"Duplicate node with id {node_id}\")\n",
    "        self.nodes[node_id] = node\n",
    "\n",
    "    def nid_to_cid(self, node_id: NodeId) -> ClassId:\n",
    "        return self.nodes[node_id].eclass\n",
    "\n",
    "    def nid_to_class(self, node_id: NodeId) -> Class:\n",
    "        eclass_id = self.nid_to_cid(node_id)\n",
    "        return self.classes().get(eclass_id)\n",
    "\n",
    "    def classes(self) -> Dict[ClassId, Class]:\n",
    "        # This method will group nodes into classes\n",
    "        classes = {}\n",
    "        for node_id, node in self.nodes.items():\n",
    "            class_id = node.eclass\n",
    "            if class_id not in classes:\n",
    "                classes[class_id] = Class(id=class_id, nodes=[])\n",
    "            classes[class_id].nodes.append(node_id)\n",
    "        return classes\n",
    "\n",
    "    @classmethod\n",
    "    def from_json_file(cls, path: str) -> \"EGraph\":\n",
    "        with open(path, 'r') as file:\n",
    "            data = file.read()\n",
    "        return cls.model_validate_json(data)\n",
    "\n",
    "    def to_json_file(self, path: str):\n",
    "        with open(path, 'w') as file:\n",
    "            file.write(self.model_dump_json(indent=4))\n",
    "\n",
    "    @classmethod\n",
    "    def from_json(cls, json_data: str) -> \"EGraph\":\n",
    "        return cls.model_validate_json(json_data)\n",
    "\n",
    "    def to_json(self) -> str:\n",
    "        return self.model_dump_json(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "def fetch_and_parse_egraph(url: str) -> EGraph:\n",
    "    # Fetch the JSON content from the URL\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Raise an exception if the request failed\n",
    "\n",
    "    # Parse the JSON content into EGraph model\n",
    "    egraph = EGraph.from_json(response.text)\n",
    "    return egraph\n",
    "\n",
    "json_url = \"https://raw.githubusercontent.com/egraphs-good/extraction-gym/refs/heads/main/data/dummy_examples/ab_add.json\"\n",
    "egraph = fetch_and_parse_egraph(json_url)\n",
    "\n",
    "# Print out the loaded EGraph structure\n",
    "print(egraph)\n",
    "# Optionally, you can serialize it back to JSON and print it\n",
    "print(\"Serialized EGraph to JSON:\")\n",
    "print(egraph.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def test_round_trip(self):\n",
    "        json_str = self.model_dump_json(indent=4)\n",
    "        egraph_copy = EGraph.model_validate_json(json_str)\n",
    "        assert self == egraph_copy, \"Round trip serialization failed\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    node = Node(op=\"Add\", children=[], eclass=ClassId(\"c1\"))\n",
    "    egraph = EGraph()\n",
    "    egraph.add_node(NodeId(\"n1\"), node)\n",
    "    egraph.root_eclasses.append(ClassId(\"c1\"))\n",
    "\n",
    "    # Serialize to JSON and save to file\n",
    "    egraph.to_json_file(\"egraph.json\")\n",
    "\n",
    "    # Deserialize from JSON file\n",
    "    egraph_loaded = EGraph.from_json_file(\"egraph.json\")\n",
    "\n",
    "    # Perform a round-trip test\n",
    "    egraph.test_round_trip()\n",
    "    print(\"Round trip test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profile Guided\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Destrcutive Rewriting\n",
    "\n",
    "Combination problems\n",
    "If I had \"good\" rewrite rules in KB's R, that is pretty similar to equality saturation during the critical pair finding. Hyper-critical pairs in analogy to hyper resolution. We totally ground out the unification vars.\n",
    "I know string rewriting and group rewriting are not decidable. These are good null results\n",
    "But I could use the rules as R-simplfy L-simplify rules for compression. I don't think that is being done?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Egraphs\n",
    "prototype in groove\n",
    "Could I get that mathjemtcia hypergraph rewriting package\n",
    "\n",
    "https://link.springer.com/content/pdf/10.1007/3-540-53982-4_16.pdf  Logic Programming as Hypergraph Rewriting*\n",
    "\n",
    "A. Corradini and F. Rossi. Hyperedge replacement jungle rewriting for term rewriting systems and logic programming.\n",
    "\n",
    "https://eprints.whiterose.ac.uk/148076/1/Plump.MFCS.94.pdf Critical Pairs in Term Graph Rewriting\n",
    "\n",
    "\"jungle rewritingt\" \"jungle evaluation\"\n",
    "\n",
    "D. Plump. Hypergraph rewriting: Critical pairs and undecidability of confluence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nominal / Miller Ground Completion / Slotted\n",
    "See graph algerba note for tesnro canonical;ization\n",
    "\n",
    "\n",
    "Slotting. My point in my talk was that the flatterning trasnformation doesn't work. This is beside the point of KB or other complication\n",
    "f(f(x)) flat ->    {e3 = f(e1),  }\n",
    "\n",
    "Howveer, \n",
    "lam i. f(f(i)) -->  {e3 = lam i. e2}\n",
    "\n",
    "This is nonsensical.\n",
    "So what do we do, we can lambda lift and explicitly carry the functional values\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "What about considering de bruijn shift as a group. +-1 additive group.\n",
    "Then it goes in the union find instead of into junk nodes.\n",
    "\n",
    "https://pavpanchekha.com/blog/egg-bindings.html\n",
    "It kind of jives because a loop in the egraph could make some kind of abitrarily deep lambda deeper requiring a lot of shifting.\n",
    "\n",
    "\n",
    "Slotted egraphs as a hash cons mod alpha\n",
    "\n",
    "Flat slotted egraphs as tensor expressions? \"Everything\" is in top scope?\n",
    "T_ijk\n",
    "\n",
    "From my persepctive of group uinion find, I have full permutations at all times. But maybe I only have a subset of permutations as being relevant. Permutations are generators but their action on my objects or egraph is not free.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_slotted(f, vs):\n",
    "    if f in vs:\n",
    "        return SlotId, [f] \n",
    "    for c in f.children():\n",
    "        to_slotted(c)\n",
    "\n",
    "# slotted terms\n",
    "(\"id\",)\n",
    "(\"f\", 2, ((\"id\"), []) )\n",
    "\n",
    "@dataclass\n",
    "class SlotTerm():\n",
    "    f : str\n",
    "    bvar: int\n",
    "    children: list[tuple[SlotTerm, list[int]])\n",
    "\n",
    "@dataclass\n",
    "class SlotTerm():\n",
    "    f : str\n",
    "    bvar: int # kind of impleid by it's app context? Maybe... What about top?\n",
    "    children: list[MultiAppTerm]\n",
    "\n",
    "class MultiAppTerm():\n",
    "    f : SlotTerm\n",
    "    xs : list[int]\n",
    "\n",
    "@dataclass\n",
    "class IdTerm():\n",
    "    pass\n",
    "\n",
    "SlotTerm(\"f\", 1, (Id(), [0], Id(), [0])) # \\x -> f(x, x)\n",
    "SlotTerm(\"f\", 2, (Id(), [0], Id(), [1])) # \\x y -> f(x, y)\n",
    "SlotTerm(\"f\", 2, [SlotTerm(\"g\", 1, [Id(), [0]]), SlotTerm(\"h\", 1, [Id(), [0]])]) # \\x -> g(x) h(x)\n",
    "\n",
    "\n",
    "def freevars(t):\n",
    "    match t:\n",
    "        case (\"var\", x):\n",
    "            return {x}\n",
    "        case (\"lam\", x, t):\n",
    "            return freevars(t) - {x}\n",
    "        case (f, *args):\n",
    "            return set().union(*(freevars(a) for a in args))\n",
    "\n",
    "# yes we can do freevars at the same time as to_slotterm, passing both back\n",
    "\n",
    "class to_slotterm(f, v_slot):\n",
    "    match t:\n",
    "        case (\"var\", x):\n",
    "            return App(IdTerm(), [v_slot(x)])\n",
    "        case (\"lam\", x, t):\n",
    "            vslot = v_slot + {x : len(v_slot)}\n",
    "            return to_slotterm(t, vslot)\n",
    "        case (f, *args):\n",
    "            # this inlcudes (\"app\", f, x) where \"app\" is a reegular uninterpeted symbol. We are intriniscally treating alpha, not beta.\n",
    "            return SlotTerm(f, len(vslot), [App(to_slotterm(a, v_slot), range(len(vslot))) for a in args]) # maximal passing down.\n",
    "        #case (\"app\", t1, t2):\n",
    "\n",
    "# multi app is playing a role bvery similar to that in Miller matching. \n",
    "# Application to variables are special and rigid in a way general application is not.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlotNode2(Container):\n",
    "    name:str\n",
    "    perm: list[int]\n",
    "    bvar: int\n",
    "    args:list[tuple[perm, Eid, list[int]]] # slotted application\n",
    "# we can canonicalize to make vars in order of appearance.\n",
    "# eid just is perm combo\n",
    "\n",
    "\n",
    "#special id node?\n",
    "# #bvar is really a property of the sort. so maybe we don't have to carry it in here.\n",
    "\n",
    "class SlotNode1(Container):\n",
    "    name:str\n",
    "    bvar: int\n",
    "    args:list[tuple[Eid, list[int]]] # slotted application\n",
    "\n",
    "# it isn't structured eid, it's a special container.\n",
    "\n",
    "# represents\n",
    "# lam x0 x1 .. xbvar, f(e(), e(), e())    \n",
    "\n",
    "# permutation symmettry is a separate thing?\n",
    "# also we can do subsumption of dropping unneeded prams separateky.\n",
    "\n",
    "\n",
    "\n",
    "# it's kind of eta long.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Z3 + slotted egraph.\n",
    "Use theory of arrays for e?\n",
    "Maybe I can get away with just lambda expanding first level?\n",
    "Lambda([x0,x1,x2], f(Lambda([x0,x1], e1(x0,x1), e2() ) ) = Lambda([x0,x1,x2], e3(x0, x1))\n",
    "If we don't use x2.\n",
    "Kind of an extensionality move\n",
    "Lambda([x2], (Lambda([x0,x1], ) == Lambda([]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Enode(\"var\", ())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Eclass():\n",
    "    eid:int\n",
    "    slots:tuple[int,...]\n",
    "\n",
    "\n",
    "class EGraph():\n",
    "    enodes: dict[tuple[str, tuple[Eclass]], Eclass]\n",
    "    uf : list[int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perm():\n",
    "    # permutations of finite support\n",
    "    def __init__(self):\n",
    "        self.to = {}\n",
    "        self.fro = {}\n",
    "    def add(self, x, y):\n",
    "        if x in self.to:\n",
    "            assert self.to[x] == y\n",
    "        assert y not in self.fro:        \n",
    "        self.to[x] = y\n",
    "        self.fro[y] = x\n",
    "    def __mul__(self, other):\n",
    "        p = Perm()\n",
    "        for x, y in self.to.items():\n",
    "            if y in other.to:\n",
    "                p.add(x, other.to[y])\n",
    "            else:\n",
    "                p.add(x, y)\n",
    "        for y, z in other.to.items():\n",
    "        return p\n",
    "    def invert(self):\n",
    "        p = Perm()\n",
    "        p.to = self.fro\n",
    "        p.fro = self.to\n",
    "    def apply(self, x):\n",
    "        if x in self.to:\n",
    "            return self.to[x]\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "\n",
    "class GroupUF():\n",
    "    def __init__(self):\n",
    "        self.uf = []\n",
    "    def find(self, x):\n",
    "        while True: \n",
    "         (P, y) = self.uf[x]\n",
    "         if x == P.apply(y):\n",
    "            return (P, y)\n",
    "         x = P.apply(y)\n",
    "    def union(self, x, g, y):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nocapture(t : ExprRef, vs=[]):\n",
    "    \"\"\"expand t to never capture variables. AND canonicalize names\n",
    "    fully curry also?\n",
    "    x5 lam x4, lam x3, lam x2 \n",
    "    Is this lambda lift?\n",
    "    \"\"\"\n",
    "    if is_quantifier(t):\n",
    "        return ForAll([Const(f\"{v.name()}_{i}\", v.sort()) for i,v in enumerate(t.var_sorts())], nocapture(t.body()))\n",
    "    elif is_app(t):\n",
    "        return t.decl()(*[nocapture(a) for a in t.children()])\n",
    "    else:\n",
    "        return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ackermmanized ASP + Lambda datalog\n",
    "= Lambda egraph / SMT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = [l for l,_ in G] + [r for _,r in G]\n",
    "# fixpoint\n",
    "T = T | [c for t in T for c in t.children()]\n",
    "\n",
    "sig = {}\n",
    "def R(T):\n",
    "    return [t,t for t in T]\n",
    "def T(E):\n",
    "    return [(l,r) for l,u in E for u1,r in E if u1 == u ]\n",
    "def C(E):\n",
    "    return [ for f,n in sig for  zip(*itertools.product(E, repeat=n))]\n",
    "def S(E):\n",
    "    return [(r,l) for l,r in E]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "https://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-218.pdf higher order critical pair nipkow\n",
    "PRS (pattern rewrite systems)\n",
    "\n",
    "Klop CRS combinatory reduction system\n",
    "\n",
    "Maybe I could use dedukti (?)\n",
    "\n",
    "Critical Pair Analysis in Nominal Rewriting https://easychair.org/publications/download/8LkF\n",
    "\n",
    "\"Critical pair lemma\" is a magic phrase. THere are many variations of this.\n",
    "\n",
    "When is one term a subterm of another? For the full lambda calculus it isn't clear.\n",
    "\n",
    "For completion egraphs I should maintain the rules _and_ the current term set (which is normalized / shrunk wrt the rules). We look into the set to match, but match  modulo the rules. Distinct purposes. In this way, I can drop a -> a rules.\n",
    "\n",
    "The single loop egraoh \n",
    "\n",
    "f(e1) -> e1\n",
    "\n",
    "So you can't get a general egraph out of term equations. We know this from PEG. PEG is trying to be coinductive and needs to tie a knot.\n",
    "\n",
    "\n",
    "\n",
    "egraphs are like grammars? Multiple people said this. Tree automata are grammars ish.\n",
    "\n",
    "\n",
    "refinement and egraphs. We have simplicity ordering and refinement ordering? They have to mesh? Term ordering becomes incomparable? Use unfailing ground completion?\n",
    "\n",
    "\n",
    "assoc flattening. Regular knuth bendix ought to do this.\n",
    "fof(axiom, add(add(X,Y), Z) = add(X,Y,Z)).\n",
    "fof(axiom, add(Z, add(X,Y)) = add(Z, X,Y)).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Miller pattern lambda egraphs. Keep everything in eta expanded form\n",
    "\n",
    "Miller pattern = we allow inverse beta genralization of lambdas, but only when they are applied to in scope variables.\n",
    "Normal flat form\n",
    "\\x y z ... f(\\x y. e1(x,y), \\x z. e2(x, z)) -> \\x y z. e3(x,y,z)\n",
    "\n",
    "Pretty similar ish to nominal smushing\n",
    "Allows for scope cpature\n",
    "\\x. foo(\\y. bar(x)) == \\x foo(\\y (\\z. bar(z))(x)) \n",
    "and in this sense, \\z. bar(z) is a subterm of the original term.\n",
    "\n",
    "If we keep everything expanded, we can use normal subterm algorithm?\n",
    "So Maybe I could do z3 ground egraph using Z3 lambda.\n",
    "Maybe we also want to allows commutativity in the binders.\\x y f(x,y) ~~ \\x y (\\z w. f w z)(y,x)\n",
    "Maybe also dropping binders. (\\x y z. foo(x,y))(k,j,m) = (\\x y foo(x,y))(k,j) \n",
    "Maybe dropping binders is a rewrite rule.\n",
    "And commutativyt.\n",
    "Then all we need is an ordering for closed lambda terms (?).\n",
    "\n",
    "This is related to an old concept that we can kind of support multi arg lambdas\n",
    "\n",
    "[a,b,c] foo() -> \n",
    "\n",
    "\n",
    "\n",
    "Ok, what about We only allow top level lambdas and any interesting inner lambda needs to be an indirection e1\n",
    "Then we can use Var(0), Var(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "&exist;x : x"
      ],
      "text/plain": [
       "Lambda(x, x)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from z3 import *\n",
    "x,y,z = Reals(\"x y z\")\n",
    "assert not Lambda([x], x).eq(Lambda([y], y)) # fuck\n",
    "assert x.eq(x)\n",
    "assert Lambda([x], x).eq(Lambda([x], x))\n",
    "\n",
    "substitute(Lambda([x], x), (Lambda([y], y), Lambda([z], RealVal(3))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nominal ground completion and slotted egraphs\n",
    "\n",
    "https://link.springer.com/chapter/10.1007/978-3-642-31585-5_21\n",
    "ominal Completion for Rewrite Systems with Binders\n",
    "\n",
    "https://arxiv.org/pdf/0806.2517 cmputability pat hordering end of a auest\n",
    "\n",
    "We can flatten to 3 layers\n",
    "\n",
    "\n",
    "foo(bar(i,j), biz) becomes\n",
    "\n",
    "biz = e1\n",
    "bar(i,j) = e2(i,j)\n",
    "foo(e2(i,j), e1) = e3(i,j)\n",
    "\n",
    "\n",
    "nominal egraph\n",
    "\n",
    "coalgerbaic egraphs for \n",
    "\n",
    "completion egraph is caonically serializable.\n",
    "Takw ground rewrite system. Write rules down in order. Boom.\n",
    "But if you flatten, non canonical. Huh.\n",
    "Rules are just ordered tuples. Even easier than equations which are symmetrical. I suppose you could orientequations via a term order, but that's the point\n",
    "\n",
    "\n",
    "\n",
    "Comparison of nominal terms. Nominal term orders.\n",
    "\n",
    "1. stably under substituoion.\n",
    "2. subterms are smaller than big term\n",
    "\n",
    "Size first gets you 2\n",
    "\n",
    "1. u > v -> t[u/x] > t[v/x] \n",
    "2. t[u] > u\n",
    "\n",
    "stable under permutation\n",
    "t > u -> Pt > Pu\n",
    "\n",
    "so\n",
    "\n",
    "i -> j two different atoms... Hmm. That's not a valid rule. Or rather it identifies all atoms. We can't orient it in a premutation invaraint way. Or is this saying that they are equal... ?\n",
    "Consider like rotationally symmettric functions. Like radius or something. Also cross product like stuff.\n",
    "\n",
    "f(i) -> j  Fine, it's smaller.\n",
    "f(i,i) -> f(j,j) These are already \"equal\" ? Hmmm. Nooooo... It's confusing though \n",
    "f(i,j) -> f(i,i) I think I prefer. Reducing the number of different i is usually good. Or is having many independent i,j better?\n",
    "f(i) > f(j) but this is permutation symmettric. I can't.. ok they're \"equal\" ish. I don't all a > a either.\n",
    "f(i) -P> f(j) is analagous to a -> a.\n",
    "\n",
    "\"creating\" new symbols is bad? Deleting them is better?\n",
    "\n",
    "a > j  Hmmm. Could be. I suspect i want to eliminate vars though\n",
    "j > a  \n",
    "f(a) -> j ... hmm.\n",
    "f(i,i) -> f(i,j)\n",
    "f(i,i) -> f(j,k)\n",
    "\n",
    "What about leftmost appearance as highest? A way of tiebreaking\n",
    "\n",
    "can we have j -> f(j)  Well, not a rewriting order\n",
    "\n",
    "\n",
    "The kmett union find. Hmm.\n",
    "\n",
    "The multiset count of variables\n",
    "size, multisetcount, symbol, recurse\n",
    "\n",
    "multoisetcount does order f(i,i) vs f(i,j). a const function is like this. forall x y, f(i, x) = f(i, y) plausble axiom\n",
    "We can never order f(i,j) f(k,l) (?) They are the \"same\" in some sense already.\n",
    "f(i,j), f(i,k) also unorderable\n",
    "\n",
    "lam(i, i) = lam(j,j) but foo(i,i) != foo(j,j) unless in scope of lam(i, foo(i,i)) = lam(j, foo(j,j))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "@dataclass \n",
    "class Term():\n",
    "    head: Any\n",
    "    args: list[Any]\n",
    "\n",
    "@dataclass\n",
    "class Atom():\n",
    "    name:str\n",
    "\n",
    "\n",
    "def nom_e1t1,t2):\n",
    "    \"\"\"nom_eq checks if two ground terms are equal modulo alpha renaming\"\"\"\n",
    "    perm_ab = {}\n",
    "    perm_ba = {}\n",
    "    todo = [(t1,t2)]\n",
    "    while todo:\n",
    "        t1,t2 = todo.pop()\n",
    "        match t1,t2:\n",
    "            case Term(f,args), Term(f2,args2):\n",
    "                if f != f2 or len(arg1s) != len(args2):\n",
    "                    return None\n",
    "                else:\n",
    "                    todo.extend(zip(args1,args2))\n",
    "            case Atom(a), Atom(b):\n",
    "                if a in permab:\n",
    "                    if perm_ab[a] != b:\n",
    "                        return None\n",
    "                elif b in perm_ba:\n",
    "                    if prem_ba[b] != a:\n",
    "                        return None\n",
    "                else:\n",
    "                    perm_ab[a] = b\n",
    "                    perm_ba[b] = a\n",
    "    return perm\n",
    "\n",
    "# a knuth bendix like ordering for ground nominal terms.\n",
    "# I'd probably want to tie break with # or nominal vars first\n",
    "def nom_compare(t1,t2):\n",
    "    perm = {}\n",
    "    s1, s2 = size(t1), size(t2)\n",
    "    if s1 < s2:\n",
    "        return LE\n",
    "    if s2 > s2:\n",
    "        return GE\n",
    "    else:\n",
    "\n",
    "\n",
    "# red arrows can be labelled by a permutation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def flatten():\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (2776903280.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    lam(x, lam(y, f x y)) == lam(x, lam(y, lam z(lam q))(x,y))\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "# miller ground matching. We allow a SINGLE beta0 matching.\n",
    "# Yes. That is enough to carry arguments \n",
    "\n",
    "# Why can't we do full beta? Question is term t1 a subterm of t2 omdulo beta.\n",
    "# (\\x \\y x )() \n",
    "# If we allowed arbitrary backwards evolution, we can lose all knids of terms. Any term is a subterm of another modulo beta in this sense.\n",
    "\n",
    "# beta norm, eta long.\n",
    "\n",
    "lam(x, lam(y, f x y)) == lam(x, lam(y, lam z(lam q))(x,y))\n",
    "\n",
    "\n",
    "class App(): # multi app?\n",
    "    f:\n",
    "    x:\n",
    "class Lam(): # multi lambda?\n",
    "    x:\n",
    "    body:\n",
    "class Term():\n",
    "    head:\n",
    "    args:\n",
    "class Var():\n",
    "    name:str # de bruijn?\n",
    "\n",
    "def miller_eq(t1,t2):\n",
    "\n",
    "    #perm/subst?\n",
    "    todo = [(t1,t2)]\n",
    "    match t1, t2:\n",
    "        perm? / subst?\n",
    "\n",
    "\n",
    "\n",
    "def miller_compare(t1,t2):\n",
    "    ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Typed\n",
    "\n",
    "Simply typed let's us sepoarate out universe which is nice for querying. It also lets us serach by type. CVC5 equality engines\n",
    "\n",
    "GATs. Build in GAT typing. Dependent types light.\n",
    "\n",
    "True dependency.\n",
    "\n",
    "Typed critical pairs. Typed \n",
    "\n",
    "Liquid types\n",
    "\n",
    "Remorra.\n",
    "If we added types maybe we don't have to use as many guards.\n",
    "\n",
    "\n",
    "\n",
    "uf\n",
    "typ[eid] = \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SYnthesis\n",
    "Xinyu Wang. syntehsis of programs with local variables\n",
    "web robot - web automation by deomsntration\n",
    "\n",
    "bottom up synthesis\n",
    "observational equivalence. - we can fuse two programs that on data are the same.\n",
    "\n",
    "\n",
    "enumerrate contexts  rho |= e. fold(x,y, (acc, x) => )\n",
    "\n",
    "and enumerare terms\n",
    "compress equivalent terms and context.\n",
    "\n",
    "coegraph useful for this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bottom uip synth\n",
    "\n",
    "seen = set()\n",
    "while True:\n",
    "    new = []\n",
    "    for x in seen:\n",
    "        for y in seen:\n",
    "            if f(x,y) not in seen:\n",
    "                new.append(f(x,y))\n",
    "                yield f(x,y)\n",
    "    seen.update(new)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "proof using unsat core\n",
    "\n",
    "\n",
    "def alpha_canon(t):\n",
    "    .body()\n",
    "    # traverse term to turn into depth first\n",
    "    # egnerate permutation\n",
    "\n",
    "def alpha_eq(t1,t2):\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Community meeting notes\n",
    "## Thomas Bourgeat 10 - 17-24\n",
    "hacking egraphs in coq\n",
    "\n",
    "coq is good for false theorems.\n",
    "tweak assumptions of true theorems\n",
    "\n",
    "\n",
    "\n",
    "software  / hardware. Lots of trivial cases.\n",
    "\n",
    "coquetier egg-server\n",
    "\n",
    "Why egg and egraphs?\n",
    "\n",
    "\n",
    "modular arith example\n",
    "terms and types and judegments and metadata all in egraph\n",
    "\n",
    "minimize predicate rather than fully discharge\n",
    "\n",
    "farfetchedness. property of enode.\n",
    "\n",
    "fuzzy instructions. use modulo. remove symbold\n",
    "\n",
    "sophie germain equational proof\n",
    "https://www.cut-the-knot.org/blue/SophieGermainIdentity.shtml\n",
    "a4+4b4=(a2+2b2−2ab)(a2+2b2+2ab).\n",
    "\n",
    "make some term appear\n",
    "\n",
    "monotonic vs inflationary\n",
    "does applying a \n",
    "x <= f(X)\n",
    "vs\n",
    "x <= y -> f(x) <= f(y)\n",
    "\n",
    "do pass, tweak costs\n",
    "\n",
    "\n",
    "avoid existentials. Need to give witness.\n",
    "forall m, exists l, l ++ [4] = [1;2;3] + m + [4]\n",
    "\n",
    "existsl, lenght l = 3 + length (m) \n",
    "\n",
    "parttial application breaks this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proved\n"
     ]
    }
   ],
   "source": [
    "from z3 import *\n",
    "a,b = Ints(\"a b\")\n",
    "prove(a**4 + 4*b**4 == (a**2 + 2*b**2 + 2*a*b)*(a**2 + 2*b**2 - 2*a*b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Talk Notes\n",
    "\n",
    "https://github.com/philzook58/egraphs2024-talk/blob/main/egraphs2024.pdf\n",
    "\n",
    "- There was simplification.\n",
    "- Brute Equational Search\n",
    " All automated theorem proving technology can be viewed in a similar application driven light as egraphs\n",
    "\n",
    "Greedy:\n",
    "- Missed optimizations\n",
    "- Ordering of rules matters\n",
    "- Hard to reason about\n",
    "- Nontermination\n",
    "\n",
    "Problems:\n",
    "- Brute search is very explosive\n",
    "- \n",
    "\n",
    "+ Egraphs\n",
    "- Congruence Closure\n",
    "- e-nodes and e-classes\n",
    "- ![image.png](attachment:image.png)\n",
    "- Applications\n",
    "- Compiler/Simplification focus\n",
    "- Theorem Proving\n",
    "- Operational\n",
    "- A notch more declarative than a greedy rewrite system\n",
    "+ Philosophy\n",
    "- What is it about egg? Pictures + Compiler. Not a conceptual high bar to entry.\n",
    "- Non completeness\n",
    "- Limited scope leads to highly refined, fast, implementation\n",
    "- \n",
    "\n",
    "\n",
    "paramodulation is conditional\n",
    "\n",
    "\n",
    "Convert equations into confluent terminating rewrite systems\n",
    "\n",
    "- Basic Completion\n",
    "    + Orient equations\n",
    "    + Find critical pairs\n",
    "    + reduce\n",
    "- Don't back simplify anything (show example of this simple procedure here)\n",
    "- Confusion: I barely care about confluence or termination. \n",
    "  + Completion is an effective equational theorem proving method.\n",
    "  + a powerful and complete simplification methodology\n",
    "\n",
    "\n",
    "(x * n) / n = x\n",
    "(x * 2) = (x << 2)\n",
    "\n",
    "orient \n",
    "(x * N) / N -> x\n",
    "(x * 2) -> (x << 2)\n",
    "\n",
    "critical pair (nontrivial overlap)\n",
    "(x << 2) / 2 = x\n",
    "\n",
    "\n",
    "Confluence  ~ a formulation of phase ordering problem\n",
    "Completion solves the phase ordering problem\n",
    "\n",
    "Conversely completion is an egraph extended with\n",
    "- First class rewrite rules\n",
    "- Universal variables\n",
    "\n",
    "Advanced completion allows reducing redundant equations\n",
    "A call to arms: Completion is useful compiler tech. Automated reasoning is compiler tech\n",
    "\n",
    "show twee \n",
    "show twee proofs as an aside (a call to arms)\n",
    "\n",
    "It is a saturation algorithm.\n",
    "\n",
    "\n",
    "- Union finds can be seen as completed atomic rewrite systems\n",
    "- Running the rewrite rules is calling `find`\n",
    "- `union` is incrementally adding a new equation to R\n",
    "- Union find is dynamic system that when you run produces canonical element\n",
    "- Fixed point of f\n",
    "\n",
    "Reverse the order here. Egrapoh should be left column\n",
    "\n",
    "| Term Rewriting | Egraph |\n",
    "|------|------------|\n",
    "| Ground System     |   Egraph         |\n",
    "| Ground Atomic | Union Find |\n",
    "| Term Ordering  | Extraction Goal |\n",
    "| Running | Extraction |\n",
    "| R/L-simplify | Canonization |\n",
    "| Run rules backwards | Egraph unrolling |\n",
    "| Canonical term | Eclass |  \n",
    "\n",
    "\n",
    "- If terms keep shrinking, they have to stop.\n",
    "- [Ground KBO](https://www.philipzucker.com/ground_kbo/)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "I've noted before it isn't obvious what goes into the box next to completion in the propositional case\n",
    "\n",
    "Ground atomic superposition is a contextual union find.\n",
    "\n",
    "\n",
    "? = completion\n",
    "lambda prolog = ?\n",
    "hilog = ?\n",
    "contextual datalog = ?\n",
    "hypothetical prolog = ?\n",
    "\n",
    "- Combining Rewrite Rules and Ground Equations. Combination problems.\n",
    "- When is it ok to combine ground rules and equations? Combination problems\n",
    "\n",
    "Grobner is ground equations modulo rings. \n",
    "\n",
    "Lambda Term Orderings are not that well behaved\n",
    "\n",
    "- Lambda Term Orderings\n",
    "- SKI\n",
    "- Lambda Free\n",
    "\n",
    "\n",
    "Get example tree automata equation.\n",
    "Give example of term families of given rewrite system\n",
    "\n",
    "+ Term Orderings\n",
    "- KBO ~ Smaller is better\n",
    "- Union Find Tie Breaking\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "+ Equations\n",
    "  \n",
    "  paramodulation\n",
    "- Superposition\n",
    "- Egglog\n",
    "- Functional Logic Programming\n",
    "\n",
    "\n",
    "- Handbook of automated reasononig\n",
    "- Term Rewriting and All That\n",
    "- Automated Reasoning: An Introduction\n",
    "\n",
    "- *If* successful \"Master\" equations\n",
    "\n",
    "+ Bits and Bobbles\n",
    "\n",
    "Slides from juptyer?  I can nbconvert\n",
    "\n",
    "\n",
    "Animations?\n",
    "\n",
    "\n",
    "\n",
    "E-matching against the ground egraph\n",
    "  unification modulo egraphs or matching modulo egraphs\n",
    "\n",
    "\n",
    "+ Resolution\n",
    "- Resolution\n",
    "- Ordered Resolution\n",
    "- Datalog\n",
    "- Prolog\n",
    "\n",
    "Jasmin's cube\n",
    "\n",
    "note: have box that like fills in? Left side is propositional, right is equational. \n",
    "not a \\/ b === a -> b\n",
    "\n",
    " 🧑‍🔬🔬🔍➡️🧪💥🔥🏃‍♂️🏃‍♀️🏃‍♂️🏃‍♀️😱🚒🚑🚓💦💧🌟\n",
    "\n",
    "\n",
    "examples of phase ordering\n",
    "examplers of non termination\n",
    "\n",
    "    $$\\{foo(bar(X)) \\rightarrow biz(X);  bar(X) \\rightarrow baz(X)\\} \\Rightarrow $$\n",
    "    $$ [biz(X) = foo(baz(X))]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ground KBO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ground KBO\n",
    "from enum import Enum\n",
    "from z3 import *\n",
    "Order = Enum(\"Order\", [\"LT\", \"EQ\", \"NGE\", \"GT\"])\n",
    "def kbo(t, r):\n",
    "    pass\n",
    "\n",
    "def size(t:ExprRef):\n",
    "    return 1 + sum(size(x) for x in t.children())\n",
    "def ground_kbo(t1, t2):\n",
    "    if t1.eq(t2): # optimization\n",
    "        return Order.EQ\n",
    "    s1 = size(t1)\n",
    "    s2 = size(t2)\n",
    "    if s1 < s2:\n",
    "        return Order.LT\n",
    "    elif s1 > s2:\n",
    "        return Order.GT\n",
    "    else:\n",
    "        if t1.num_args() < t1.num_args():\n",
    "            return Order.LT\n",
    "        elif t2.num_args() > t2.num_args():\n",
    "            return Order.GT\n",
    "        n1, n2 = t1.decl().name(), t2.decl().name()\n",
    "        if n1 < n2:\n",
    "            return Order.LT\n",
    "        elif n1 > n2:\n",
    "            return Order.GT\n",
    "        else:\n",
    "            for x,y in zip(t1.children(), t2.children()):\n",
    "                o = ground_kbo(x,y)\n",
    "                if o != Order.EQ:\n",
    "                    return o\n",
    "            assert False, \"unreachable\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle b \\rightarrow a\\\\c \\rightarrow a\\\\d \\rightarrow a$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# completion\n",
    "\"\"\"\n",
    "def reduce_atom(a,R): # find\n",
    "    while True:\n",
    "        for (l,r) in R:\n",
    "            if a.eq(l):\n",
    "                a = r\n",
    "        else:\n",
    "            break\n",
    "\"\"\"\n",
    "import z3\n",
    "# https://microsoft.github.io/z3guide/programming/Example%20Programs/Formula%20Simplification/\n",
    "def subterms(t):\n",
    "    seen = {}\n",
    "    def subterms_rec(t):\n",
    "        if z3.is_app(t):\n",
    "            for ch in t.children():\n",
    "                if ch in seen:\n",
    "                    continue\n",
    "                seen[ch] = True\n",
    "                yield ch\n",
    "                yield from subterms_rec(ch)\n",
    "    return { s for s in subterms_rec(t) }\n",
    "def is_subterm(t1,t2):\n",
    "    return t1 in subterms(t2)\n",
    "def apply_rules(e, R):\n",
    "    e1 = z3.substitute(e, *R)\n",
    "    while not e.eq(e1):\n",
    "        e = e1\n",
    "        e1 = z3.substitute(e, *R)\n",
    "    return e\n",
    "\n",
    "def critical_pairs(R):\n",
    "    for (l1,r1) in R:\n",
    "        for (l2,r2) in R:\n",
    "            if is_subterm(l1, l2):\n",
    "                #r2 = z3.substitute(l2, (l1, r1))\n",
    "                r1 = apply_rules(r1, R)\n",
    "                r2 = apply_rules(r2, R)\n",
    "                if not r1.eq(r2):\n",
    "                    yield r1, r2\n",
    "\n",
    "def complete(R):\n",
    "    while True:\n",
    "        Rnew = []\n",
    "        for a,b in critical_pairs(R):\n",
    "            match ground_kbo(a,b):\n",
    "                case Order.GT:\n",
    "                    a,b = a,b\n",
    "                case Order.LT:\n",
    "                    a,b = b,a\n",
    "                case Order.EQ:\n",
    "                    pass\n",
    "            Rnew.append((a,b))\n",
    "        if len(Rnew) == 0:\n",
    "            break\n",
    "        R.extend(Rnew)\n",
    "    return R\n",
    "        \n",
    "\n",
    "def union(a,b,R):\n",
    "    a = apply_rules(a, R) #find\n",
    "    b = apply_rules(b, R) #find\n",
    "    match ground_kbo(a,b):\n",
    "        case Order.GT:\n",
    "            a,b = a,b\n",
    "        case Order.LT:\n",
    "            a,b = b,a\n",
    "        case Order.EQ:\n",
    "            return R\n",
    "    R.append((a,b))\n",
    "    return R\n",
    "\n",
    "a,b,c,d,e = z3.Ints(\"a b c d e\")\n",
    "R = []\n",
    "union(b,a,R)\n",
    "union(b,c,R)\n",
    "union(a,c,R)\n",
    "union(a,a,R)\n",
    "union(b,d,R)\n",
    "R\n",
    "from IPython.display import display, Math\n",
    "display(Math( r\"\\\\\".join(f\"{a} \\\\rightarrow {b}\" for a,b in R)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Z3 Ground KBO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.cs.man.ac.uk/~regerg/ssa/ImpSaturation.pdf Implementation of Saturating Theorem Provers  scvhulz\n",
    "https://www.youtube.com/watch?v=zja691VwfSA&ab_channel=JetBrainsResearch intro to superpsoition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lit():\n",
    "    lhs\n",
    "    rhs\n",
    "\n",
    "@dataclass\n",
    "class Clause():\n",
    "    neg:\n",
    "    pos: list[]\n",
    "\n",
    "\n",
    "# But is this factoring the most useful?\n",
    "# simp table = lhs -> rhs\n",
    "# simpe = lhs -> ctx -> rhs ? \n",
    "\n",
    "\n",
    "\n",
    "# fully z3 intragrated\n",
    "# Implies(And(neg1,neg2), pos)\n",
    "\n",
    "# Horn clause reallty.\n",
    "# https://en.wikipedia.org/wiki/Horn-satisfiability\n",
    "# but its horn sat modulo equality.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Twee\n",
    "\n",
    "mul(X,two) = shift(X, one)).\n",
    "fof(cancel, axiom, ![X,N] : div(mul(X,N),N) = X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twee sorting. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /tmp/context.p\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the input problem:\n",
      "  Axiom 1 (cancel): e = f.\n",
      "  Axiom 2 (cancel): c = d.\n",
      "  Axiom 3 (ifeq_axiom): ifeq(X, X, Y, Z) = Y.\n",
      "  Axiom 4 (shift): ifeq(a, b, b, c) = c.\n",
      "  Goal 1 (goa): true = false.\n",
      "\n",
      "1. f -> e\n",
      "2. c -> d\n",
      "3. ifeq(X, X, Y, Z) -> Y\n",
      "4. ifeq(a, b, b, d) -> d\n",
      "\n",
      "Ran out of critical pairs. This means the conjecture is not true.\n",
      "Here is the final rewrite system:\n",
      "  c -> d\n",
      "  f -> e\n",
      "  ifeq(X, X, Y, Z) -> Y\n",
      "  ifeq(a, b, b, d) -> d\n",
      "\n",
      "RESULT: CounterSatisfiable (the conjecture is false).\n"
     ]
    }
   ],
   "source": [
    "!twee /tmp/context.p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/shift.p\n"
     ]
    }
   ],
   "source": [
    "%%file /tmp/shift.p\n",
    "fof(shift, axiom, ![X] : mul(X,two) = shift(X, one)).\n",
    "fof(cancel, axiom, ![X,N] : div(mul(X,N),N) = X).\n",
    "%fof(goal, conjecture, true = false).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# done 2 iterations in 0.011s\n",
      "% Final clauses: 2\n",
      "Clauses:\n",
      "forall X0. [mul X0 two = shift X0 one*]/id:0/depth:0/penalty:1/red:false\n",
      "\n",
      "forall X0 X1. [div (mul X0 X1) X1 = X0*]/id:1/depth:0/penalty:1/red:false\n",
      "# SZS status Satisfiable for '/tmp/shift.p'\n"
     ]
    }
   ],
   "source": [
    "!/home/philip/.opam/5.1.1/bin/zipperposition /tmp/shift.p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Initializing proof state\n",
      "# Scanning for AC axioms\n",
      "#\n",
      "#cnf(i_0_2, plain, (div(mul(X1,X2),X2)=X1)).\n",
      "#\n",
      "#cnf(i_0_1, plain, (mul(X1,two)=shift(X1,one))).\n",
      "#\n",
      "#cnf(i_0_3, plain, (div(shift(X1,one),two)=X1)).\n",
      "\n",
      "# No proof found!\n",
      "# SZS status Satisfiable\n"
     ]
    }
   ],
   "source": [
    "! eprover-ho /tmp/shift.p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the input problem:\n",
      "  Axiom 1 (shift): mul(X, two) = shift(X, one).\n",
      "  Axiom 2 (cancel): div(mul(X, Y), Y) = X.\n",
      "  Goal 1 (goal): true = false.\n",
      "\n",
      "1. mul(X, two) -> shift(X, one)\n",
      "2. div(mul(X, Y), Y) -> X\n",
      "3. div(shift(X, one), two) -> X\n",
      "\n",
      "Ran out of critical pairs. This means the conjecture is not true.\n",
      "Here is the final rewrite system:\n",
      "  mul(X, two) -> shift(X, one)\n",
      "  div(mul(X, Y), Y) -> X\n",
      "  div(shift(X, one), two) -> X\n",
      "\n",
      "RESULT: CounterSatisfiable (the conjecture is false).\n"
     ]
    }
   ],
   "source": [
    "!twee /tmp/shift.p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UF Vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"260pt\" height=\"44pt\"\n",
       " viewBox=\"0.00 0.00 260.00 44.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 40)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-40 256,-40 256,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">0</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;0 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.44,-24.69C63.03,-25.15 72,-22.92 72,-18 72,-14.77 68.14,-12.7 62.49,-11.79\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"62.6,-8.29 52.44,-11.31 62.27,-15.28 62.6,-8.29\"/>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"117\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"117\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">1</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;1 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>1&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M142.44,-24.69C153.03,-25.15 162,-22.92 162,-18 162,-14.77 158.14,-12.7 152.49,-11.79\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"152.6,-8.29 142.44,-11.31 152.27,-15.28 152.6,-8.29\"/>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"207\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"207\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">2</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;2 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>2&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M232.44,-24.69C243.03,-25.15 252,-22.92 252,-18 252,-14.77 248.14,-12.7 242.49,-11.79\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"242.6,-8.29 232.44,-11.31 242.27,-15.28 242.6,-8.29\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x77aa3c2b8c10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "class UF():\n",
    "    def __init__(self):\n",
    "        self.parent = []\n",
    "    def find(self, x):\n",
    "        while x != self.parent[x]:\n",
    "           x = self.parent[x] \n",
    "        return x\n",
    "    def makeset(self):\n",
    "        n = len(self.parent)\n",
    "        self.parent.append(n)\n",
    "        return n\n",
    "    def union(self, x, y):\n",
    "        self.parent[self.find(x)] = self.find(y)\n",
    "    def __repr__(self):\n",
    "        return str(self.parent)\n",
    "    def viz(self):\n",
    "        dot = Digraph()\n",
    "        for i, p in enumerate(self.parent):\n",
    "            dot.edge(str(i), str(p))\n",
    "        return dot\n",
    "uf = UF()\n",
    "zero = uf.makeset()\n",
    "one = uf.makeset()\n",
    "two = uf.makeset()\n",
    "\n",
    "uf.viz()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uf.union(zero, one)\n",
    "uf.viz()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Iterable, Dict\n",
    "def match_(t : AstRef, pat : AstRef, vars : Iterable[ExprRef] = []) -> Optional[Dict[ExprRef,ExprRef]]:\n",
    "    subst = {}\n",
    "    todo = [(t,pat)]\n",
    "    while todo:\n",
    "        t,pat = todo.pop()\n",
    "        if pat in vars or is_var(pat): # allow var as pattern?\n",
    "            if pat in subst:\n",
    "                if subst[pat].eq(t):\n",
    "                    pass\n",
    "                else:\n",
    "                    return None\n",
    "            else:\n",
    "                subst[pat] = t\n",
    "        elif isinstance(t, QuantifierRef) or isinstance(pat, QuantifierRef):\n",
    "            raise NotImplementedError\n",
    "        else:\n",
    "            thead, targs = t.decl(), t.children()\n",
    "            phead, pargs = pat.decl(), pat.children()\n",
    "            if thead != phead or len(targs) != len(pargs): # check sorts here? The decl might check that\n",
    "                return None\n",
    "            todo.extend(zip(targs, pargs))\n",
    "    return subst\n",
    "\n",
    "E = DeclareSort(\"Expr\")\n",
    "foo = Function(\"foo\", E, E, E)\n",
    "bar = Function(\"bar\", E, E)\n",
    "a,b,c = Consts(\"a b c\", E)\n",
    "x,y,z = Consts(\"x y z\", E)\n",
    "vars = {x,y,z}\n",
    "\n",
    "assert match_(foo(x, bar(y)), foo(a, bar(b)), vars) == None\n",
    "assert match_(foo(a, bar(b)), foo(x, bar(y)), vars) == {x:a, y:b}\n",
    "assert match_(foo(a, bar(b)), foo(a, bar(b)), vars) == {}\n",
    "assert match_(foo(a, bar(b)), foo(x,x), vars) == None\n",
    "assert match_(foo(a, bar(a)), foo(x,bar(x)), vars) == {x:a}\n",
    "match_(foo(a, bar(a)), Lambda([x],foo(x,bar(x))).body()) == {Var(0, E):a}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from z3 import *\n",
    "a = BitVec(\"a\", 32)\n",
    "\n",
    "e = (a * 2) / 2\n",
    "e1 = (a << 1) / 2\n",
    "\n",
    "a * 2 == a << 1\n",
    "(a * 2) / 2 == a\n",
    "\n",
    "\n",
    "E = [\n",
    "    (a * 2, a << 1),\n",
    "    ((a * 2) / 2, a)\n",
    "]\n",
    "# what else might be a good example?\n",
    "\n",
    "# a + b + c\n",
    "# a + b + a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 12\u001b[0m\n\u001b[1;32m      3\u001b[0m a,b,c,d,e,f,g \u001b[38;5;241m=\u001b[39m Consts(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma b c d e f g\u001b[39m\u001b[38;5;124m\"\u001b[39m, IntSort())\n\u001b[1;32m      4\u001b[0m R \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      5\u001b[0m (b,a),\n\u001b[1;32m      6\u001b[0m (c,a),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m (g,f)\n\u001b[1;32m     10\u001b[0m ]\n\u001b[0;32m---> 12\u001b[0m \u001b[43mdot\u001b[49m\u001b[38;5;241m.\u001b[39medge(node(l), node(r), style\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdashed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dot' is not defined"
     ]
    }
   ],
   "source": [
    "import graphviz\n",
    "from z3 import *\n",
    "a,b,c,d,e,f,g = Consts(\"a b c d e f g\", IntSort())\n",
    "R = [\n",
    "(b,a),\n",
    "(c,a),\n",
    "(d,a),\n",
    "(e,a),\n",
    "(g,f)\n",
    "]\n",
    "\n",
    "dot.edge(node(l), node(r), style=\"dashed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Graph Vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"244pt\" height=\"357pt\"\n",
       " viewBox=\"0.00 0.00 244.00 357.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 353)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-353 240,-353 240,4 -4,4\"/>\n",
       "<!-- 5 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>5</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M84,-146.5C84,-146.5 114,-146.5 114,-146.5 120,-146.5 126,-152.5 126,-158.5 126,-158.5 126,-170.5 126,-170.5 126,-176.5 120,-182.5 114,-182.5 114,-182.5 84,-182.5 84,-182.5 78,-182.5 72,-176.5 72,-170.5 72,-170.5 72,-158.5 72,-158.5 72,-152.5 78,-146.5 84,-146.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"99\" y=\"-160.8\" font-family=\"Times,serif\" font-size=\"14.00\">a</text>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>6</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M194,-73.5C194,-73.5 224,-73.5 224,-73.5 230,-73.5 236,-79.5 236,-85.5 236,-85.5 236,-97.5 236,-97.5 236,-103.5 230,-109.5 224,-109.5 224,-109.5 194,-109.5 194,-109.5 188,-109.5 182,-103.5 182,-97.5 182,-97.5 182,-85.5 182,-85.5 182,-79.5 188,-73.5 194,-73.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"209\" y=\"-87.8\" font-family=\"Times,serif\" font-size=\"14.00\">b</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;6 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>5:head&#45;&gt;6:head</title>\n",
       "<path fill=\"none\" stroke=\"red\" stroke-dasharray=\"5,2\" d=\"M125.43,-146.38C125.62,-146.25 125.81,-146.13 126,-146 147.67,-131.62 155.46,-127.35 174.16,-115.16\"/>\n",
       "<polygon fill=\"red\" stroke=\"red\" points=\"176.14,-118.04 182.57,-109.62 172.29,-112.2 176.14,-118.04\"/>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>7</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M166,-0.5C166,-0.5 196,-0.5 196,-0.5 202,-0.5 208,-6.5 208,-12.5 208,-12.5 208,-24.5 208,-24.5 208,-30.5 202,-36.5 196,-36.5 196,-36.5 166,-36.5 166,-36.5 160,-36.5 154,-30.5 154,-24.5 154,-24.5 154,-12.5 154,-12.5 154,-6.5 160,-0.5 166,-0.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"181\" y=\"-14.8\" font-family=\"Times,serif\" font-size=\"14.00\">c</text>\n",
       "</g>\n",
       "<!-- 6&#45;&gt;7 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>6:head&#45;&gt;7:head</title>\n",
       "<path fill=\"none\" stroke=\"red\" stroke-dasharray=\"5,2\" d=\"M202.08,-73.46C198.92,-65.21 195.1,-55.25 191.58,-46.1\"/>\n",
       "<polygon fill=\"red\" stroke=\"red\" points=\"194.79,-44.67 187.94,-36.59 188.25,-47.18 194.79,-44.67\"/>\n",
       "</g>\n",
       "<!-- 23 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>23</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M67,-224.5C67,-224.5 97,-224.5 97,-224.5 103,-224.5 109,-230.5 109,-236.5 109,-236.5 109,-248.5 109,-248.5 109,-254.5 103,-260.5 97,-260.5 97,-260.5 67,-260.5 67,-260.5 61,-260.5 55,-254.5 55,-248.5 55,-248.5 55,-236.5 55,-236.5 55,-230.5 61,-224.5 67,-224.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"82\" y=\"-238.8\" font-family=\"Times,serif\" font-size=\"14.00\">d</text>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>9</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M12,-146.5C12,-146.5 42,-146.5 42,-146.5 48,-146.5 54,-152.5 54,-158.5 54,-158.5 54,-170.5 54,-170.5 54,-176.5 48,-182.5 42,-182.5 42,-182.5 12,-182.5 12,-182.5 6,-182.5 0,-176.5 0,-170.5 0,-170.5 0,-158.5 0,-158.5 0,-152.5 6,-146.5 12,-146.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-160.8\" font-family=\"Times,serif\" font-size=\"14.00\">e</text>\n",
       "</g>\n",
       "<!-- 23&#45;&gt;9 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>23:head&#45;&gt;9:head</title>\n",
       "<path fill=\"none\" stroke=\"red\" stroke-dasharray=\"5,2\" d=\"M69.24,-224.41C62.28,-214.54 53.53,-202.13 45.8,-191.17\"/>\n",
       "<polygon fill=\"red\" stroke=\"red\" points=\"48.5,-188.92 39.88,-182.76 42.78,-192.95 48.5,-188.92\"/>\n",
       "</g>\n",
       "<!-- 14 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>14</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M139,-219.5C139,-219.5 169,-219.5 169,-219.5 175,-219.5 181,-225.5 181,-231.5 181,-231.5 181,-253.5 181,-253.5 181,-259.5 175,-265.5 169,-265.5 169,-265.5 139,-265.5 139,-265.5 133,-265.5 127,-259.5 127,-253.5 127,-253.5 127,-231.5 127,-231.5 127,-225.5 133,-219.5 139,-219.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"154\" y=\"-250.3\" font-family=\"Times,serif\" font-size=\"14.00\">+</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"127,-242.5 181,-242.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"140.5\" y=\"-227.3\" font-family=\"Times,serif\" font-size=\"14.00\"> </text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"154,-219.5 154,-242.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"167.5\" y=\"-227.3\" font-family=\"Times,serif\" font-size=\"14.00\"> </text>\n",
       "</g>\n",
       "<!-- 14&#45;&gt;5 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>14:p0&#45;&gt;5:head</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M140,-219C140,-208.52 135.1,-198.77 128.69,-190.51\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"131.12,-187.97 121.9,-182.76 125.86,-192.58 131.12,-187.97\"/>\n",
       "</g>\n",
       "<!-- 14&#45;&gt;6 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>14:p1&#45;&gt;6:head</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M168,-219C168,-183.28 182.76,-144.52 194.57,-119.23\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"197.87,-120.44 199.08,-109.91 191.57,-117.39 197.87,-120.44\"/>\n",
       "</g>\n",
       "<!-- 14&#45;&gt;7 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>14:head&#45;&gt;7:head</title>\n",
       "<path fill=\"none\" stroke=\"red\" stroke-dasharray=\"5,2\" d=\"M182,-254.5C202.3,-254.5 169.14,-189.49 168,-183 159.71,-135.64 167.88,-79.67 174.49,-46.74\"/>\n",
       "<polygon fill=\"red\" stroke=\"red\" points=\"177.92,-47.4 176.56,-36.9 171.07,-45.97 177.92,-47.4\"/>\n",
       "</g>\n",
       "<!-- 10 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>10</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M67,-302.5C67,-302.5 97,-302.5 97,-302.5 103,-302.5 109,-308.5 109,-314.5 109,-314.5 109,-336.5 109,-336.5 109,-342.5 103,-348.5 97,-348.5 97,-348.5 67,-348.5 67,-348.5 61,-348.5 55,-342.5 55,-336.5 55,-336.5 55,-314.5 55,-314.5 55,-308.5 61,-302.5 67,-302.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"82\" y=\"-333.3\" font-family=\"Times,serif\" font-size=\"14.00\">&#45;</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"55,-325.5 109,-325.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"68.5\" y=\"-310.3\" font-family=\"Times,serif\" font-size=\"14.00\"> </text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"82,-302.5 82,-325.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"95.5\" y=\"-310.3\" font-family=\"Times,serif\" font-size=\"14.00\"> </text>\n",
       "</g>\n",
       "<!-- 10&#45;&gt;5 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>10:p0&#45;&gt;5:head</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M68,-302C68,-283.25 51.08,-284.05 46,-266 40.34,-245.89 38.14,-238.35 46,-219 50.49,-207.93 58.43,-197.88 66.82,-189.51\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"69.3,-191.98 74.23,-182.6 64.53,-186.86 69.3,-191.98\"/>\n",
       "</g>\n",
       "<!-- 10&#45;&gt;23 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>10:head&#45;&gt;23:head</title>\n",
       "<path fill=\"none\" stroke=\"red\" stroke-dasharray=\"5,2\" d=\"M110,-337.5C138.55,-337.5 118.29,-297.54 100.52,-269.45\"/>\n",
       "<polygon fill=\"red\" stroke=\"red\" points=\"103.29,-267.29 94.91,-260.82 97.42,-271.11 103.29,-267.29\"/>\n",
       "</g>\n",
       "<!-- 10&#45;&gt;14 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>10:p1&#45;&gt;14:head</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M96,-302C96,-301.43 111.42,-276.62 120.16,-263.09\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"123.27,-264.74 126,-254.5 117.48,-260.8 123.27,-264.74\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x77aa3448a9b0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import graphviz\n",
    "def node(e):\n",
    "    #print(e, e.get_id())\n",
    "    return str(e.get_id())\n",
    "\n",
    "def graphviz_of_z3(e, dot):\n",
    "    #label = \"<head>\" + e.decl().name() + \"| {\" + \"|\".join([ f\"<p{n}>{n}\" for n in range(e.num_args())]) + \"}\"\n",
    "    if e.num_args() == 0:\n",
    "        label = f'{{<head>{e.decl().name()}}}'\n",
    "    else:\n",
    "        label = f'{{<head>{e.decl().name()}|{{{\"|\".join([f\"<p{n}>\" for n in range(e.num_args())])}}}}}'\n",
    "    #print(label)\n",
    "    dot.node(node(e), label, shape=\"Mrecord\") # id?\n",
    "    for n,c in enumerate(e.children()):\n",
    "        a,b = node(e) + \":p\" + str(n) , node(c) + \":head\"\n",
    "        if (a,b) not in dot.added_edges:\n",
    "            dot.added_edges.add((a,b))\n",
    "            dot.edge(a,b)\n",
    "        graphviz_of_z3(c, dot)\n",
    "\n",
    "def show_rewrite(R):\n",
    "    dot = graphviz.Digraph()\n",
    "    dot.added_edges = set()\n",
    "    for l,r in R:\n",
    "\n",
    "        dot.edge(node(l) + \":head\", node(r) + \":head\", style=\"dashed\", color=\"red\")\n",
    "        graphviz_of_z3(l,dot)\n",
    "        graphviz_of_z3(r,dot)\n",
    "    return dot\n",
    "        \n",
    "from z3 import *\n",
    "a,b,c,d,e = Consts(\"a b c d e\", IntSort())\n",
    "\n",
    "show_rewrite([(a,b), (b,c), (d,e), (a  + b, c), (a-(a+b), d)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"295pt\" height=\"211pt\"\n",
       " viewBox=\"0.00 0.00 295.20 211.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 207)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-207 291.2,-207 291.2,4 -4,4\"/>\n",
       "<!-- 22 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>22</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M143.2,-156.5C143.2,-156.5 173.2,-156.5 173.2,-156.5 179.2,-156.5 185.2,-162.5 185.2,-168.5 185.2,-168.5 185.2,-190.5 185.2,-190.5 185.2,-196.5 179.2,-202.5 173.2,-202.5 173.2,-202.5 143.2,-202.5 143.2,-202.5 137.2,-202.5 131.2,-196.5 131.2,-190.5 131.2,-190.5 131.2,-168.5 131.2,-168.5 131.2,-162.5 137.2,-156.5 143.2,-156.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"158.2\" y=\"-187.3\" font-family=\"Times,serif\" font-size=\"14.00\">*</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"131.2,-179.5 185.2,-179.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"144.7\" y=\"-164.3\" font-family=\"Times,serif\" font-size=\"14.00\"> </text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"158.2,-156.5 158.2,-179.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"171.7\" y=\"-164.3\" font-family=\"Times,serif\" font-size=\"14.00\"> </text>\n",
       "</g>\n",
       "<!-- 21 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>21</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M136.2,-73.5C136.2,-73.5 166.2,-73.5 166.2,-73.5 172.2,-73.5 178.2,-79.5 178.2,-85.5 178.2,-85.5 178.2,-107.5 178.2,-107.5 178.2,-113.5 172.2,-119.5 166.2,-119.5 166.2,-119.5 136.2,-119.5 136.2,-119.5 130.2,-119.5 124.2,-113.5 124.2,-107.5 124.2,-107.5 124.2,-85.5 124.2,-85.5 124.2,-79.5 130.2,-73.5 136.2,-73.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"151.2\" y=\"-104.3\" font-family=\"Times,serif\" font-size=\"14.00\">&lt;&lt;</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"124.2,-96.5 178.2,-96.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"137.7\" y=\"-81.3\" font-family=\"Times,serif\" font-size=\"14.00\"> </text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"151.2,-73.5 151.2,-96.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"164.7\" y=\"-81.3\" font-family=\"Times,serif\" font-size=\"14.00\"> </text>\n",
       "</g>\n",
       "<!-- 22&#45;&gt;21 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>22:head&#45;&gt;21:head</title>\n",
       "<path fill=\"none\" stroke=\"red\" stroke-dasharray=\"5,2\" d=\"M130.2,-191.5C114.41,-191.5 127.02,-171.22 131.2,-156 134.94,-142.35 144.86,-139.23 149.16,-130.05\"/>\n",
       "<polygon fill=\"red\" stroke=\"red\" points=\"152.64,-130.5 151.2,-120 145.78,-129.1 152.64,-130.5\"/>\n",
       "</g>\n",
       "<!-- 13 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>13</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M50.2,-0.5C50.2,-0.5 80.2,-0.5 80.2,-0.5 86.2,-0.5 92.2,-6.5 92.2,-12.5 92.2,-12.5 92.2,-24.5 92.2,-24.5 92.2,-30.5 86.2,-36.5 80.2,-36.5 80.2,-36.5 50.2,-36.5 50.2,-36.5 44.2,-36.5 38.2,-30.5 38.2,-24.5 38.2,-24.5 38.2,-12.5 38.2,-12.5 38.2,-6.5 44.2,-0.5 50.2,-0.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"65.2\" y=\"-14.8\" font-family=\"Times,serif\" font-size=\"14.00\">a</text>\n",
       "</g>\n",
       "<!-- 22&#45;&gt;13 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>22:p0&#45;&gt;13:head</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M130.2,-167.5C123.97,-167.5 126.98,-159.99 122.2,-156 92.57,-131.28 64.07,-152.45 43.2,-120 28.84,-97.68 38.27,-67.46 48.88,-45.81\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"52.05,-47.3 53.59,-36.82 45.85,-44.05 52.05,-47.3\"/>\n",
       "</g>\n",
       "<!-- 18 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>18</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M208.2,-78.5C208.2,-78.5 238.2,-78.5 238.2,-78.5 244.2,-78.5 250.2,-84.5 250.2,-90.5 250.2,-90.5 250.2,-102.5 250.2,-102.5 250.2,-108.5 244.2,-114.5 238.2,-114.5 238.2,-114.5 208.2,-114.5 208.2,-114.5 202.2,-114.5 196.2,-108.5 196.2,-102.5 196.2,-102.5 196.2,-90.5 196.2,-90.5 196.2,-84.5 202.2,-78.5 208.2,-78.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"223.2\" y=\"-92.8\" font-family=\"Times,serif\" font-size=\"14.00\">2</text>\n",
       "</g>\n",
       "<!-- 22&#45;&gt;18 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>22:p1&#45;&gt;18:head</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M172.2,-156C172.2,-147.78 183.57,-134.05 195.58,-121.85\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"198.36,-124.02 203.04,-114.51 193.46,-119.03 198.36,-124.02\"/>\n",
       "</g>\n",
       "<!-- 21&#45;&gt;13 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>21:p0&#45;&gt;13:head</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M123.2,-84.5C116.97,-84.5 119.09,-77.86 115.2,-73 107.38,-63.23 98.26,-53 89.98,-44.08\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"92.36,-41.5 82.96,-36.62 87.26,-46.3 92.36,-41.5\"/>\n",
       "</g>\n",
       "<!-- 17 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>17</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M150.2,-0.5C150.2,-0.5 180.2,-0.5 180.2,-0.5 186.2,-0.5 192.2,-6.5 192.2,-12.5 192.2,-12.5 192.2,-24.5 192.2,-24.5 192.2,-30.5 186.2,-36.5 180.2,-36.5 180.2,-36.5 150.2,-36.5 150.2,-36.5 144.2,-36.5 138.2,-30.5 138.2,-24.5 138.2,-24.5 138.2,-12.5 138.2,-12.5 138.2,-6.5 144.2,-0.5 150.2,-0.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"165.2\" y=\"-14.8\" font-family=\"Times,serif\" font-size=\"14.00\">1</text>\n",
       "</g>\n",
       "<!-- 21&#45;&gt;17 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>21:p1&#45;&gt;17:head</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M165.2,-73C165.2,-64.48 165.2,-55.22 165.2,-46.79\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"168.7,-46.67 165.2,-36.67 161.7,-46.67 168.7,-46.67\"/>\n",
       "</g>\n",
       "<!-- 24 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>24</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M64.2,-73.5C64.2,-73.5 94.2,-73.5 94.2,-73.5 100.2,-73.5 106.2,-79.5 106.2,-85.5 106.2,-85.5 106.2,-107.5 106.2,-107.5 106.2,-113.5 100.2,-119.5 94.2,-119.5 94.2,-119.5 64.2,-119.5 64.2,-119.5 58.2,-119.5 52.2,-113.5 52.2,-107.5 52.2,-107.5 52.2,-85.5 52.2,-85.5 52.2,-79.5 58.2,-73.5 64.2,-73.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"79.2\" y=\"-104.3\" font-family=\"Times,serif\" font-size=\"14.00\">*</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"52.2,-96.5 106.2,-96.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"65.7\" y=\"-81.3\" font-family=\"Times,serif\" font-size=\"14.00\"> </text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"79.2,-73.5 79.2,-96.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"92.7\" y=\"-81.3\" font-family=\"Times,serif\" font-size=\"14.00\"> </text>\n",
       "</g>\n",
       "<!-- 24&#45;&gt;13 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>24:head&#45;&gt;13:head</title>\n",
       "<path fill=\"none\" stroke=\"red\" stroke-dasharray=\"5,2\" d=\"M51.2,-108.5C38.44,-108.5 47.33,-72.73 55.64,-46.29\"/>\n",
       "<polygon fill=\"red\" stroke=\"red\" points=\"59,-47.3 58.76,-36.71 52.34,-45.14 59,-47.3\"/>\n",
       "</g>\n",
       "<!-- 24&#45;&gt;13 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>24:p0&#45;&gt;13:head</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M65.2,-73C65.2,-64.48 65.2,-55.22 65.2,-46.79\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"68.7,-46.67 65.2,-36.67 61.7,-46.67 68.7,-46.67\"/>\n",
       "</g>\n",
       "<!-- 24&#45;&gt;17 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>24:p1&#45;&gt;17:head</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M107.2,-84.5C113.42,-84.5 111.31,-77.86 115.2,-73 123.02,-63.23 132.13,-53 140.42,-44.08\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"143.13,-46.3 147.43,-36.62 138.03,-41.5 143.13,-46.3\"/>\n",
       "</g>\n",
       "<!-- 25 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>25</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M222.2,-156.5C222.2,-156.5 252.2,-156.5 252.2,-156.5 258.2,-156.5 264.2,-162.5 264.2,-168.5 264.2,-168.5 264.2,-190.5 264.2,-190.5 264.2,-196.5 258.2,-202.5 252.2,-202.5 252.2,-202.5 222.2,-202.5 222.2,-202.5 216.2,-202.5 210.2,-196.5 210.2,-190.5 210.2,-190.5 210.2,-168.5 210.2,-168.5 210.2,-162.5 216.2,-156.5 222.2,-156.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"237.2\" y=\"-187.3\" font-family=\"Times,serif\" font-size=\"14.00\">/</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"210.2,-179.5 264.2,-179.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"223.7\" y=\"-164.3\" font-family=\"Times,serif\" font-size=\"14.00\"> </text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"237.2,-156.5 237.2,-179.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"250.7\" y=\"-164.3\" font-family=\"Times,serif\" font-size=\"14.00\"> </text>\n",
       "</g>\n",
       "<!-- 25&#45;&gt;18 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>25:p0&#45;&gt;18:head</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M223.2,-156C223.2,-145.83 223.2,-134.69 223.2,-124.83\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"226.7,-124.73 223.2,-114.73 219.7,-124.73 226.7,-124.73\"/>\n",
       "</g>\n",
       "<!-- 25&#45;&gt;18 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>25:p1&#45;&gt;18:head</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M251.2,-156C251.2,-144.67 246.99,-133.18 241.83,-123.35\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"244.75,-121.4 236.71,-114.51 238.69,-124.91 244.75,-121.4\"/>\n",
       "</g>\n",
       "<!-- 25&#45;&gt;17 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>25:head&#45;&gt;17:head</title>\n",
       "<path fill=\"none\" stroke=\"red\" stroke-dasharray=\"5,2\" d=\"M265.2,-191.5C285.44,-191.5 304.53,-143.89 259.2,-73 242.91,-47.52 225.25,-52.89 200.62,-41.46\"/>\n",
       "<polygon fill=\"red\" stroke=\"red\" points=\"202.08,-38.27 191.61,-36.64 198.78,-44.45 202.08,-38.27\"/>\n",
       "</g>\n",
       "<!-- 26 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>26</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M71.2,-156.5C71.2,-156.5 101.2,-156.5 101.2,-156.5 107.2,-156.5 113.2,-162.5 113.2,-168.5 113.2,-168.5 113.2,-190.5 113.2,-190.5 113.2,-196.5 107.2,-202.5 101.2,-202.5 101.2,-202.5 71.2,-202.5 71.2,-202.5 65.2,-202.5 59.2,-196.5 59.2,-190.5 59.2,-190.5 59.2,-168.5 59.2,-168.5 59.2,-162.5 65.2,-156.5 71.2,-156.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"86.2\" y=\"-187.3\" font-family=\"Times,serif\" font-size=\"14.00\">/</text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"59.2,-179.5 113.2,-179.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"72.7\" y=\"-164.3\" font-family=\"Times,serif\" font-size=\"14.00\"> </text>\n",
       "<polyline fill=\"none\" stroke=\"black\" points=\"86.2,-156.5 86.2,-179.5 \"/>\n",
       "<text text-anchor=\"middle\" x=\"99.7\" y=\"-164.3\" font-family=\"Times,serif\" font-size=\"14.00\"> </text>\n",
       "</g>\n",
       "<!-- 26&#45;&gt;21 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>26:p0&#45;&gt;21:head</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M72.2,-156C72.2,-131.08 98.38,-138.39 115.2,-120 116.12,-118.99 116.72,-117.88 117.16,-116.75\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"120.12,-118.64 123.2,-108.5 114.47,-114.5 120.12,-118.64\"/>\n",
       "</g>\n",
       "<!-- 26&#45;&gt;13 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>26:head&#45;&gt;13:head</title>\n",
       "<path fill=\"none\" stroke=\"red\" stroke-dasharray=\"5,2\" d=\"M58.2,-191.5C1.37,-191.5 -11.67,-125.45 10.2,-73 14.84,-61.87 22.98,-51.84 31.6,-43.52\"/>\n",
       "<polygon fill=\"red\" stroke=\"red\" points=\"34.14,-45.95 39.22,-36.66 29.45,-40.75 34.14,-45.95\"/>\n",
       "</g>\n",
       "<!-- 26&#45;&gt;18 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>26:p1&#45;&gt;18:head</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M114.2,-167.5C120.42,-167.5 117.58,-160.17 122.2,-156 147.49,-133.17 161.45,-134.29 188.08,-119.71\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"189.9,-122.7 196.78,-114.64 186.38,-116.65 189.9,-122.7\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x77aa3d493730>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BV = DeclareSort(\"BV\")\n",
    "mul = Function(\"*\", BV, BV, BV)\n",
    "shift = Function(\"\\<\\<\", BV, BV, BV)\n",
    "div = Function(\"/\", BV, BV, BV)\n",
    "a,one,two= Consts(\"a 1 2\", BV)\n",
    "start = Const(\"start\", BV)\n",
    "R = [\n",
    "  (mul(a, two) , shift(a, one)),\n",
    "  (mul(a, one) , a),\n",
    "  (div(two, two) , one),\n",
    "  (div(shift(a, one), two) , a),\n",
    "  #(start , a)\n",
    "]\n",
    "show_rewrite(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"350pt\" height=\"118pt\"\n",
       " viewBox=\"0.00 0.00 350.00 118.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 114)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-114 346,-114 346,4 -4,4\"/>\n",
       "<!-- 6 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>6</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M12,-73.5C12,-73.5 42,-73.5 42,-73.5 48,-73.5 54,-79.5 54,-85.5 54,-85.5 54,-97.5 54,-97.5 54,-103.5 48,-109.5 42,-109.5 42,-109.5 12,-109.5 12,-109.5 6,-109.5 0,-103.5 0,-97.5 0,-97.5 0,-85.5 0,-85.5 0,-79.5 6,-73.5 12,-73.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-87.8\" font-family=\"Times,serif\" font-size=\"14.00\">b</text>\n",
       "</g>\n",
       "<!-- 13 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>13</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M120,-0.5C120,-0.5 150,-0.5 150,-0.5 156,-0.5 162,-6.5 162,-12.5 162,-12.5 162,-24.5 162,-24.5 162,-30.5 156,-36.5 150,-36.5 150,-36.5 120,-36.5 120,-36.5 114,-36.5 108,-30.5 108,-24.5 108,-24.5 108,-12.5 108,-12.5 108,-6.5 114,-0.5 120,-0.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"135\" y=\"-14.8\" font-family=\"Times,serif\" font-size=\"14.00\">a</text>\n",
       "</g>\n",
       "<!-- 6&#45;&gt;13 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>6:head&#45;&gt;13:head</title>\n",
       "<path fill=\"none\" stroke=\"red\" stroke-dasharray=\"5,2\" d=\"M53.43,-73.38C53.62,-73.26 53.81,-73.13 54,-73 74.91,-58.87 82.27,-54.27 100.18,-42.26\"/>\n",
       "<polygon fill=\"red\" stroke=\"red\" points=\"102.23,-45.11 108.57,-36.62 98.32,-39.3 102.23,-45.11\"/>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>7</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M84,-73.5C84,-73.5 114,-73.5 114,-73.5 120,-73.5 126,-79.5 126,-85.5 126,-85.5 126,-97.5 126,-97.5 126,-103.5 120,-109.5 114,-109.5 114,-109.5 84,-109.5 84,-109.5 78,-109.5 72,-103.5 72,-97.5 72,-97.5 72,-85.5 72,-85.5 72,-79.5 78,-73.5 84,-73.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"99\" y=\"-87.8\" font-family=\"Times,serif\" font-size=\"14.00\">c</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;13 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>7:head&#45;&gt;13:head</title>\n",
       "<path fill=\"none\" stroke=\"red\" stroke-dasharray=\"5,2\" d=\"M107.9,-73.46C112.01,-65.12 116.98,-55.04 121.53,-45.8\"/>\n",
       "<polygon fill=\"red\" stroke=\"red\" points=\"124.79,-47.11 126.08,-36.59 118.52,-44.01 124.79,-47.11\"/>\n",
       "</g>\n",
       "<!-- 23 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>23</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M156,-73.5C156,-73.5 186,-73.5 186,-73.5 192,-73.5 198,-79.5 198,-85.5 198,-85.5 198,-97.5 198,-97.5 198,-103.5 192,-109.5 186,-109.5 186,-109.5 156,-109.5 156,-109.5 150,-109.5 144,-103.5 144,-97.5 144,-97.5 144,-85.5 144,-85.5 144,-79.5 150,-73.5 156,-73.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"171\" y=\"-87.8\" font-family=\"Times,serif\" font-size=\"14.00\">d</text>\n",
       "</g>\n",
       "<!-- 23&#45;&gt;13 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>23:head&#45;&gt;13:head</title>\n",
       "<path fill=\"none\" stroke=\"red\" stroke-dasharray=\"5,2\" d=\"M162.1,-73.46C157.99,-65.12 153.02,-55.04 148.47,-45.8\"/>\n",
       "<polygon fill=\"red\" stroke=\"red\" points=\"151.48,-44.01 143.92,-36.59 145.21,-47.11 151.48,-44.01\"/>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>9</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M228,-73.5C228,-73.5 258,-73.5 258,-73.5 264,-73.5 270,-79.5 270,-85.5 270,-85.5 270,-97.5 270,-97.5 270,-103.5 264,-109.5 258,-109.5 258,-109.5 228,-109.5 228,-109.5 222,-109.5 216,-103.5 216,-97.5 216,-97.5 216,-85.5 216,-85.5 216,-79.5 222,-73.5 228,-73.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"243\" y=\"-87.8\" font-family=\"Times,serif\" font-size=\"14.00\">e</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;13 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>9:head&#45;&gt;13:head</title>\n",
       "<path fill=\"none\" stroke=\"red\" stroke-dasharray=\"5,2\" d=\"M216.57,-73.38C216.38,-73.26 216.19,-73.13 216,-73 195.09,-58.87 187.73,-54.27 169.82,-42.26\"/>\n",
       "<polygon fill=\"red\" stroke=\"red\" points=\"171.68,-39.3 161.43,-36.62 167.77,-45.11 171.68,-39.3\"/>\n",
       "</g>\n",
       "<!-- 12 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>12</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M300,-73.5C300,-73.5 330,-73.5 330,-73.5 336,-73.5 342,-79.5 342,-85.5 342,-85.5 342,-97.5 342,-97.5 342,-103.5 336,-109.5 330,-109.5 330,-109.5 300,-109.5 300,-109.5 294,-109.5 288,-103.5 288,-97.5 288,-97.5 288,-85.5 288,-85.5 288,-79.5 294,-73.5 300,-73.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"315\" y=\"-87.8\" font-family=\"Times,serif\" font-size=\"14.00\">g</text>\n",
       "</g>\n",
       "<!-- 11 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>11</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M300,-0.5C300,-0.5 330,-0.5 330,-0.5 336,-0.5 342,-6.5 342,-12.5 342,-12.5 342,-24.5 342,-24.5 342,-30.5 336,-36.5 330,-36.5 330,-36.5 300,-36.5 300,-36.5 294,-36.5 288,-30.5 288,-24.5 288,-24.5 288,-12.5 288,-12.5 288,-6.5 294,-0.5 300,-0.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"315\" y=\"-14.8\" font-family=\"Times,serif\" font-size=\"14.00\">f</text>\n",
       "</g>\n",
       "<!-- 12&#45;&gt;11 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>12:head&#45;&gt;11:head</title>\n",
       "<path fill=\"none\" stroke=\"red\" stroke-dasharray=\"5,2\" d=\"M315,-73.46C315,-65.38 315,-55.68 315,-46.68\"/>\n",
       "<polygon fill=\"red\" stroke=\"red\" points=\"318.5,-46.59 315,-36.59 311.5,-46.59 318.5,-46.59\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x77aa344893f0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R = [\n",
    "(b,a),\n",
    "(c,a),\n",
    "(d,a),\n",
    "(e,a),\n",
    "(g,f)\n",
    "]\n",
    "show_rewrite(R)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
