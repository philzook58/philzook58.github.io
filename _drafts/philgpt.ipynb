{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# OCR PDF rag\n",
    "\n",
    "RAG - retrieval augmented generation. Use a vector database to retrieve relevant stuff from database then throw that into prompt so it can read them. Fast up to date data without finetuning it in. Perhaps larger exact data.\n",
    "\n",
    "<https://github.com/AkariAsai/self-rag>\n",
    "\n",
    "Databases that include the abilitt to do fuzzy search for vectors (from embeddings)\n",
    "\n",
    "Approximate nearest neighbor: [FAISS](https://github.com/facebookresearch/faiss),  <https://github.com/spotify/annoy> <https://github.com/nmslib/hnswlib/>\n",
    "\n",
    "- Pinecone\n",
    "- Milvus\n",
    "- Weaviate\n",
    "- Qdrant\n",
    "\n",
    "[missing where clause](https://www.pinecone.io/learn/vector-search-filtering/), pre vs post filtering\n",
    "\n",
    "sqlite vector search\n",
    "<https://github.com/asg017/sqlite-vss> <https://observablehq.com/@asg017/introducing-sqlite-vss>\n",
    "\n",
    "postgres vector pg_vector\n",
    "\n",
    "vs\n",
    "elastic search, opensearch, lucene.\n",
    "BM25\n",
    "\n",
    "<https://www.sbert.net/> sentence transformers\n",
    "<https://github.com/imartinez/privateGPT>\n",
    "\n",
    "\n",
    "https://github.com/facebookresearch/faiss\n",
    "\n",
    "https://github.com/plasticityai/magnitude\n",
    "\n",
    "Non ai text search\n",
    "\n",
    "debugger\n",
    "\n",
    "https://en.wikipedia.org/wiki/Tf%E2%80%93idf tf-idf\n",
    "\n",
    "https://www.sqlite.org/fts5.html#the_bm25_function\n",
    "bm25\n",
    "https://en.wikipedia.org/wiki/Okapi_BM25\n",
    "https://news.ycombinator.com/item?id=42190650  \tUnderstanding the BM25 full text search algorithm \n",
    "https://github.com/xhluca/bm25s\n",
    "\n",
    "\n",
    "https://sbert.net/\n",
    "\n",
    "https://huggingface.co/spaces/mteb/leaderboard\n",
    "\n",
    "new image readingthing that uses surrounding text? https://olmocr.allenai.org/ https://github.com/allenai/olmocr\n",
    "https://jonathansoma.com/words/olmocr-on-macos-with-lm-studio.html https://news.ycombinator.com/item?id=43174298\n",
    "\n",
    "https://www.llamaindex.ai/llamaparse\n",
    "\n",
    "camelotpy\n",
    "\n",
    "docling https://github.com/DS4SD/docling\n",
    "\n",
    "https://github.com/Zipstack/llmwhisperer-table-extraction\n",
    "\n",
    "https://github.com/xavctn/img2table\n",
    "\n",
    "https://github.com/Megvii-BaseDetection/YOLOX get table with yolox send to img2table https://www.reddit.com/r/LocalLLaMA/comments/1854d06/table_extraction_from_pdf/\n",
    "\n",
    "CI checker that compares human documentrs and tries to keep in sync. documentation sync\n",
    "\n",
    "https://github.com/microsoft/table-transformer \n",
    "\n",
    "https://github.com/facebookresearch/nougat Nougat: Neural Optical Understanding for Academic Documents facebook 2023\n",
    "\n",
    "https://github.com/VikParuchuri/marker like nougat. someone is working on it.\n",
    "\n",
    "https://github.com/pypdfium2-team/pypdfium2 pdf manipulation This is not high on google searhc?\n",
    "https://github.com/py-pdf/pypdf  pure python\n",
    "\n",
    "https://github.com/jsvine/pdfplumber - likes like an algirhtmic table extractor\n",
    "\n",
    "surya\n",
    "\n",
    "https://docs.unstructured.io/welcome\n",
    "\n",
    "\n",
    "OCR of my math pdfs. Get appropriate image or location I can click into?\n",
    "Lookup snippet by query.\n",
    "\n",
    "https://github.com/kirel/detexify\n",
    "\n",
    "\n",
    "https://arxiv.org/html/2406.15187v1 rag benchmark dataset https://github.com/qinchuanhui/UDA-Benchmark\n",
    "\n",
    "https://github.com/conjuncts/gmft\n",
    "\n",
    "https://dataverse.jpl.nasa.gov/file.xhtml?fileId=87433&version=2.0  LMs for Structured\n",
    "Data Extraction\n",
    "from Long Form\n",
    "Documents \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diffing\n",
    "How does one keep stuff syncronized?\n",
    "\n",
    "If I want to annotated objdump\n",
    "Or annotate C with its assembly\n",
    "\n",
    "Or connect code to it's external spec.\n",
    "\n",
    "idea: what if annotation was some kind of grep command\n",
    "\n",
    "https://docs.python.org/3/library/difflib.html\n",
    "difftastic\n",
    "https://github.com/Wilfred/difftastic/wiki/Line-Based-Diffs interesting litrature review\n",
    "\n",
    "https://github.com/dandavison/delta\n",
    "\n",
    "https://github.com/so-fancy/diff-so-fancy\n",
    "\n",
    " old dead? https://github.com/google/diff-match-patch\n",
    "\n",
    "merging\n",
    "patching\n",
    "\n",
    "CRDT\n",
    "\n",
    "A third file instead of being standalone\n",
    "Try to just be the options to rip grep or sed or something\n",
    "\n",
    "\n",
    "syncpoints.yml\n",
    "file1 :  \n",
    "    startpattern\n",
    "    endpattern\n",
    "    pattern\n",
    "file2 :\n",
    "commit : \n",
    "\n",
    "ripgrep commands to find patterns in files.\n",
    "\n",
    "https://github.com/topics/diff\n",
    "\n",
    "https://github.com/GumTreeDiff/gumtree \n",
    "https://github.com/trailofbits/graphtage\n",
    "\n",
    "json diff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sentence_transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 1. Load a pretrained Sentence Transformer model\u001b[39;00m\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m SentenceTransformer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall-MiniLM-L6-v2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sentence_transformers'"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 1. Load a pretrained Sentence Transformer model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# The sentences to encode\n",
    "sentences = [\n",
    "    \"The weather is lovely today.\",\n",
    "    \"It's so sunny outside!\",\n",
    "    \"He drove to the stadium.\",\n",
    "]\n",
    "\n",
    "# 2. Calculate embeddings by calling model.encode()\n",
    "embeddings = model.encode(sentences)\n",
    "print(embeddings.shape)\n",
    "# [3, 384]\n",
    "\n",
    "# 3. Calculate the embedding similarities\n",
    "similarities = model.similarity(embeddings, embeddings)\n",
    "print(similarities)\n",
    "# tensor([[1.0000, 0.6660, 0.1046],\n",
    "#         [0.6660, 1.0000, 0.1411],\n",
    "#         [0.1046, 0.1411, 1.0000]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ai coding tools\n",
    "Changes so often\n",
    "\n",
    "claude-code\n",
    "aider\n",
    "\n",
    "cline/roo?\n",
    "\n",
    "https://x.com/VictorTaelin/status/1894783583224553584 suggests a spec.md https://gist.github.com/VictorTaelin/903f20e0a75263edaad0a4f7e2b9fa05\n",
    "Another thing I saw reccomends a memory.md of instructions\n",
    "\n",
    "vscode agent https://code.visualstudio.com/blogs/2025/02/24/introducing-copilot-agent-mode\n",
    "\n",
    "https://aider.chat/docs/leaderboards/ \n",
    "\n",
    "https://github.blog/ai-and-ml/github-copilot/how-to-debug-code-with-github-copilot/\n",
    "\n",
    ".github/copilot-instructions.md file\n",
    "\n",
    "ctrl-alt-I\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theorem proving\n",
    "gabriel poesia\n",
    "\n",
    "peano - finite action space\n",
    "forward, backward.\n",
    "khan academy problems\n",
    "\n",
    "dreamcoder\n",
    "antiunification for cadndidate tactics. contiguous subqeseuquences.\n",
    "https://github.com/gpoesia\n",
    "\n",
    "leandojo\n",
    "tacticzero\n",
    "alpahproof\n",
    "\n",
    "novelty reward as hard explortstion problmes. montezuma revenge\n",
    "student teacher rewards. hard but not impossible goal generation. Amigo\n",
    "\n",
    "leaning from intrinsic motivation\n",
    "constrined generation.\n",
    "symncromesh. constrained semantic decoding.\n",
    "proved easy and hard. Using mcts. Use likeholld of current policy.\n",
    "hindsight relabelling\n",
    "prop logic from kleene book\n",
    "\n",
    "certified deductive reasonig with lsnguage models\n",
    "autoated discovery of tactic libraries\n",
    "when do skills help reinformanet learing\n",
    "dafny-annotator: ai assistanted verification of dafny programs\n",
    "\n",
    "\n",
    "open ended abstraction learning\n",
    "\n",
    "\n",
    "\n",
    "antiunification applied ghidra pcode\n",
    "gym for knuckeldragger. mimic peano.\n",
    "Only allow grounded and unquantfier applications maybe? To gimp z3 a little.\n",
    "A reason to make my stable of python provers.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scikit learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nltk\n",
    "https://www.nltk.org/\n",
    "\n",
    "stem - remove junk from word\n",
    "\n",
    "https://www.nltk.org/howto/nonmonotonic.html what\n",
    "https://www.nltk.org/howto/logic.html\n",
    "https://www.nltk.org/howto/resolution.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.test.setup_fixt import check_binary\n",
    "#check_binary('mace4')\n",
    "from nltk import *\n",
    "from nltk.inference.nonmonotonic import *\n",
    "from nltk.sem import logic\n",
    "logic._counter._value = 0\n",
    "read_expr = logic.Expression.fromstring\n",
    "p1 = read_expr(r'all x.(man(x) -> mortal(x))')\n",
    "p2 = read_expr(r'man(Socrates)')\n",
    "c = read_expr(r'mortal(Socrates)')\n",
    "prover = Prover9Command(c, [p1,p2])\n",
    "prover.prove()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdp = ClosedDomainProver(prover)\n",
    "for a in cdp.assumptions(): \n",
    "    print(a) \n",
    "\n",
    "cdp.prove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import sqlite_vec\n",
    "\n",
    "from typing import List\n",
    "import struct\n",
    "\n",
    "\n",
    "def serialize_f32(vector: List[float]) -> bytes:\n",
    "    \"\"\"serializes a list of floats into a compact \"raw bytes\" format\"\"\"\n",
    "    return struct.pack(\"%sf\" % len(vector), *vector)\n",
    "\n",
    "\n",
    "db = sqlite3.connect(\":memory:\")\n",
    "db.enable_load_extension(True)\n",
    "sqlite_vec.load(db)\n",
    "db.enable_load_extension(False)\n",
    "\n",
    "sqlite_version, vec_version = db.execute(\n",
    "    \"select sqlite_version(), vec_version()\"\n",
    ").fetchone()\n",
    "print(f\"sqlite_version={sqlite_version}, vec_version={vec_version}\")\n",
    "\n",
    "items = [\n",
    "    (1, [0.1, 0.1, 0.1, 0.1]),\n",
    "    (2, [0.2, 0.2, 0.2, 0.2]),\n",
    "    (3, [0.3, 0.3, 0.3, 0.3]),\n",
    "    (4, [0.4, 0.4, 0.4, 0.4]),\n",
    "    (5, [0.5, 0.5, 0.5, 0.5]),\n",
    "]\n",
    "query = [0.3, 0.3, 0.3, 0.3]\n",
    "\n",
    "db.execute(\"CREATE VIRTUAL TABLE vec_items USING vec0(embedding float[4])\")\n",
    "\n",
    "with db:\n",
    "    for item in items:\n",
    "        db.execute(\n",
    "            \"INSERT INTO vec_items(rowid, embedding) VALUES (?, ?)\",\n",
    "            [item[0], serialize_f32(item[1])],\n",
    "        )\n",
    "\n",
    "rows = db.execute(\n",
    "    \"\"\"\n",
    "      SELECT\n",
    "        rowid,\n",
    "        distance\n",
    "      FROM vec_items\n",
    "      WHERE embedding MATCH ?\n",
    "      ORDER BY distance\n",
    "      LIMIT 3\n",
    "    \"\"\",\n",
    "    [serialize_f32(query)],\n",
    ").fetchall()\n",
    "\n",
    "print(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# duckdb \n",
    "\n",
    "https://duckdb.org/docs/extensions/vss.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sqlite vector https://github.com/asg017/sqlite-vec\n",
    "\n",
    "ducdb fvss\n",
    "\n",
    "\n",
    "\n",
    "https://radimrehurek.com/gensim/models/doc2vec.html\n",
    "https://github.com/ddangelov/Top2Vec\n",
    "dsppy\n",
    "\n",
    "\n",
    "Knuckedlragger search\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
