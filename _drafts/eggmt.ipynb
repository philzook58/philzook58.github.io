{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draft\n",
    "\n",
    "\n",
    "Baking it in: Steps towards egraphs modulo theories.\n",
    "\n",
    "What is \"modulo theories\"\n",
    "\n",
    "SMT solvers - sat and egraph glue together diasprarte bits\n",
    "EMT = SMT - SAT\n",
    "\n",
    "\n",
    "eid + ctx as structured eid?\n",
    "\n",
    "\n",
    "There are two tacts\n",
    "\n",
    "- Generalize Enodes\n",
    "- Generalize Union Find (which implies generalizing eids)\n",
    "\n",
    "A simplier starting problem is Terms Modulo Theories\n",
    "\n",
    "- canonizers\n",
    "- Baking in unorderedness\n",
    "- Hash Consing. Interned vs uninterned. The hash cons itself is a term bank\n",
    "\n",
    "This is the same discussion as generalized enodes. With the caveat that smart constructors can deep match.\n",
    "\n",
    "\n",
    "Do the theory subterms go in the term bank or don't they? If they do, are we saving anything?\n",
    "\n",
    "Two other tacts:\n",
    "- extract and assert\n",
    "- SMT piggy back\n",
    "\n",
    "\n",
    "You can turn a set of ground equations into an egraph.\n",
    "And also extract them. Equations extraction. egraph(equations(e)) = e? If we don't included the term bank concept, no.\n",
    "a = a can be used to enocde term bank, but I think it is conceptually interesting to make it seperate.\n",
    "E + T = Egraph\n",
    "extract_equations(egraph(eqs)) give a canonical form perhaps\n",
    "\n",
    "Group union find is a generalized union find.\n",
    "Idempotent theory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Introduction\n",
    "Simplification is the replacing of a complex thing with a better or simpler thing.\n",
    "\n",
    "A rich framework to use to declare a simplification system is that of equational logic, where equivalences are specified as quantified equational axioms over terms.\n",
    "\n",
    "\n",
    "\n",
    "These axioms and a signature implicitly define sets of equivalent terms. All the possible terms can be generated from the signature. Any two terms that match the left hand and right hand side of an equation.\n",
    "\n",
    "Simplification is selecting a nicer term out of the set of equivalent ones.\n",
    "\n",
    "\n",
    "A slightly different question is to start from one particular term and produce the set of equivalent terms. One can maintain a growing set of terms and increase it by applying the equational axioms to any term in the set at any subposition.\n",
    "\n",
    "\n",
    "Egraphs are a data structure for compactly representing a set of ground equalities that supplies a fast check if equality between two terms are implied by the ground axioms. In a different mode, Egraphs can also be used to efficiently enumerate the set of equivalent terms for a given term. Via a dynamic programming approach, one can also extract a good equivalent term.\n",
    "\n",
    "Ground terms by and large are easier to deal with and more well behaved that non-ground terms. Non ground terms can be seen as an optimization or abstract domain for finitely and effectively representing certain possibly infinite sets of terms, but this power does not come entirely for free.\n",
    "\n",
    "For example, it is the case that for some equational systems, the equalities can be oriented into rewrite rules with good properties. When the rules are confluent and terminating, running then on a ground term will produce a unique normal form. In this manner, these rules can be used for equivalence checking of ground terms modulo this equational theory.\n",
    "\n",
    "For non-ground terms, the situation is more complicated. A non-ground term represents in a sense a possibly infinite set of terms. It is not guaranteed that all of these terms are in the same equivalence class under the rules R. Therefore, there must be some kind of splitting possible. The only way for a rule to split the set represented by the term is to have a non trivial overlap with the non-ground function symbols of the term. This is called narrowing.\n",
    "\n",
    "Q: Does narrowing terminate? Yes, maybe it does. The term ordering does not admit an infinite descending chain of even non ground terms. And it has substitiution property f(f(X)) -> X.   consider narrowing c(f(X), X). This cycles.  https://www.sciencedirect.com/science/article/pii/S0304397509005246? termination of narrowing revisisted\n",
    "\n",
    "\n",
    "\n",
    "Another related approach to solving an equational system is Knuth bendix completion. Knuth Bendix completion when it succeeds produces a conlfuent and terminating rewrite systems out of an equational system. Knuth bendix completion of a ground equational system will succeed. The resulting ground rewrite system represents the set of ground equalities that produced it, but also gives a fast way to check if two ground terms are equal.\n",
    "\n",
    "\n",
    "\n",
    "- Given two terms, are they equal?\n",
    "- Given a term, generate all equal terms\n",
    "- Given a term generate a best equal term\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Egraph techniques from the get go have had a pragmatic bent. It is convenient from an implementation perspective to use ground terms. It is an easy to implement reasonable search heuristic to seek rewrite coming out of the cloud resulting from the initial term.\n",
    "From this perspective, the completeness or incompleteness of some new variation of egraph rewriting compared to the stack version does not seem that interesting.\n",
    "\n",
    "\n",
    "Some theories, important ones being associativity and commutativity, feel structural. They are difficult to orient, one direction of commutativty or assocation is arguably not \"simpler\" than the other, it being more of a marginal tie breaking case. These axioms are also related to commonly avaiable and studied data structures such as sets, multisets, and permutatations.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Ground + A Does not work\n",
    "\n",
    "\n",
    "An important no-go counterexample is the following:\n",
    "\n",
    "String rewriting is in general a nasty thing. It is a sufficiently flexible framework to encode the execution of a turing machine (fairly directly by encoding the tape and the head). If one could somehow generate a terminating normalization procedure for any string equation system, then one could solve the halting problem by asking if the halt state and the initial state are equal. \n",
    "\n",
    "String rewriting can be encoded in term rewriting. A string can be represented as a binary concatenation on constants obeying associativity. The particular equations of a string rewrite system can be represented by ground equations. The only non ground rule needed is the associativity axiom.\n",
    "\n",
    "A different encoding that in a sense bakes in a choice of the associativity (a more \"list-like\" representation) turns each character into a unary function symbol. This is sort of the fused combo of the character constants and concatenation  `a(X) = (a0 . X)`. To faithfully ecnode the string rewriting system, all the rules are now not ground, allowing an arbitrary suffix or prefix.\n",
    "\n",
    "\n",
    "\n",
    "The ground Ground Knuth Bendix is terminating and will succeed. The reason is that the overlap relation between ground terms is the subterm relationship. A critical pair will be generated, but both terms in it will always be bounded from above by a term already existing in the system. There are only finitely many terms less than a given term in a particular well founded (... is this true? No. It is not. Maybe if I require it to be a substituion ordering?) What is produced is less than the original term in a total well founded order. Because the ordering is well founded this cannot go on forever. Because the order is total, all critical pairs produced can be oriented.\n",
    "\n",
    "Regular Knuth bendix on non ground terms does not always succeed. Critical pairs can be generated which are not orientable because the pairs are unordered with respect to the necessarily partial term ordering on non ground terms. Ground terms orders can be total.\n",
    "\n",
    "It is the totality of the term ordering which is crucial here.\n",
    "\n",
    "Ground knuth bendix + the single associativity axiom will not always succeed. How does it fail?\n",
    "\n",
    "\n",
    "Supposed we have a a set of equations which can be separated into a ground part G and and non ground part E. Using ordinary completion by assumtion let us say we can convert this system to G;R by merely choosing to ingore Knuth Bendix rules that manipulate G. Now we may do the same for G ,which will succeed.\n",
    "Now the only critical pairs are between members of R and G.\n",
    "\n",
    "When a non ground term (in R) makes a critical pair with one in G, it may do it by being a true subterm or by grounding one or more of the variables in an overlap.\n",
    "This critical pair may not be fully grounded.\n",
    "\n",
    "Ground Knuth Bendix + AC will succeed. Why is this the case? No it cannot. C cannot be oriented. But the system can be _ground completed_. A system is ground confluent if any ground term can be joined and ground terminating if .\n",
    "\n",
    "An ordered rewriting system allows ordering constraints between the variables.\n",
    "f(x,y) | f(x,y) > f(y,x) --> f(y,x)  # this is actually an infinite family of ground rules (?) But so was it always? No there's some kind of game being play about when the terms filling the variables get to be chosen. Static vs dynamic.\n",
    "push inequalities down (we never hit variable rules. x,y are metavariables)\n",
    "f(x,y) | x > y -> f(y,x).\n",
    "These orderings may prune some critical pairs. We need to push constraints down to the variables by symbolically executing the term orderings.\n",
    "Overlaps between these rules and ground rules are easy to prune.\n",
    "```\n",
    "f(x,y) > p(x,y) \n",
    "---------------   \n",
    "```\n",
    "\n",
    "AC is probably ground confluent by left associng and sorting. It is not so easy to see this directly from the definition.\n",
    "ground + AC is less clear\n",
    "\n",
    "We can split any rule without ordering constraints into multiple rules of the possible ordering constraints. This is because we assume the ground ordering is total.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "For syntactic pattern matching of a single pattern against a term, anything except the top down approach feels ludicrous.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "EIds as models\n",
    "\n",
    "A term model is considered sometimes to be just syntax is disguise.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Baking in theories in term rewriting occurs in the form of E-unification or you can move off of the structural definitions of terms.\n",
    "\n",
    "Unfailing completion is a theorem proving method. It is parametrized on a notion of E used for unification.\n",
    "E-unficiation can be parametrized on a set of rewrite rules for the theory.\n",
    "\n",
    "\n",
    "\n",
    "A different intution is \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unifying Enodes and Containers\n",
    "\n",
    "A basic Enodes are an ordered record of eclass ids inputs arguments, a function symbol, and an output eclass id.\n",
    "\n",
    "Extensions of the egraph have also allows Enodes to have arguments that are primitive datatypes like ground integers, strings, bools or rationals.\n",
    "\n",
    "## Hash consing modulo theories\n",
    "https://www.philipzucker.com/hashing-modulo/\n",
    "\n",
    "A simpler case to consider is that of interning / hash consing terms. A useful features of hash consing is that equality of terms can be determined by a constant time pointer equality check.\n",
    "\n",
    "To say that a binary symbol is commutative is to say that the order of the children does not matter. It is a simple matter to normalize a binary symbol of this kind by sorting it's arguments. This puts the term into a canonical form.\n",
    "\n",
    "Given a confluent and terminating rewrite rule set, one can hash cons a term using smart constructors which check to see if the to be constructed term has rewrite rules applicable to it. As an optimization, these intermediate states do not necessarily have to be hash consed. There is an invariant of a hash consed tree that it is fully normalized with respect ot the rules. Placing a new function symbol at the head via a smart constructor may unlock some new rewrite rule at the head.\n",
    "\n",
    "Sorting the arguments can be seen as applying ordered rewriting. The commutativity axiom cannot ever be ordered because it is too symmetric. One can wait until \"runtime\" to apply the rule using ordered rewriting.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "For associative functions, it is very natural to allow multi-arity function symbols and flatten the arguments into a canonical list. When the smaert constructor peeks at the arguments, it can flatten.\n",
    "\n",
    "A related but different approach is to maintain the binary function symbols but recursively do a list append.\n",
    "\n",
    "The arguments of a function symbol can be\n",
    "\n",
    "- multisets\n",
    "- sets\n",
    "- polynomials\n",
    "\n",
    "\n",
    "- sorting\n",
    "- flattening\n",
    "- deduplicating\n",
    "- polynomials\n",
    "\n",
    "### Bottom up E-matching\n",
    "\n",
    "Suppose we have some rules that are less well behaved we keep off separate from our built in ones.\n",
    "\n",
    "We could take an equational theory and one by one add them to a Knuth Bendix procedure according to some heuristic. When or if the procedure fails, we can take the best subset of the equations that we can complete and leave the rest for later.\n",
    "Or perhaps do Knuth Bendix, stop at some intermediate state `E;R` where E has all the critical pairs of R. We then treat R intrinsically, and treat E externally.\n",
    "\n",
    "E-matching is finding a substitution such that the instantiated pattern is equal to a term modulo the equational theory. \n",
    "\n",
    "A top down strategy involves narrowing the pattern against the rules. The ground term is presumed fully normalized so the rules do not apply in the forward direction.\n",
    "\n",
    "Q: What is the stopping condition?\n",
    "\n",
    "A bottom up strategy involves expanding the ground term using the reversed rules. \n",
    "\n",
    "\n",
    "A matching problem starts with a pattern and a term supposed equal. This is a natural place to start top down matching.\n",
    "\n",
    "In E-matching, there is not generic a priori guarantee that the variables will be bound to a subterm of the target. They may not even be bound to a term in an equivalence class of the subterms of the target. The terms must be generated by the rules and searched over somehow. This sucks.\n",
    "\n",
    "Pattern matching is nice because the procedure is basically obvious. Pattern matching is not the most general thing we can conceive of. The pattern matching problem is an intermediary that is mathematically statable and efficiently implementable. If it was missing either of there characters, it would not be as prominent an idea.\n",
    "\n",
    "Reasonable mechanisms can probably eventually be reconciled with a logical view. Case is point is negation as failure in logic programming. This is an extremely antural operational move in Prolog, but reconciling it with a logical perspective took a great many years of work.\n",
    "\n",
    "Is the only logic worth talking about something close to classical first order logic? Maybe.\n",
    "\n",
    "\n",
    "A different matching problems starts with a pattern and a bank of terms with which one might wish to bind the variables. \n",
    "\n",
    "I believe it is the case in general that one may encounter an infinite number of equivalence classes as subterms. This appears to be asking too much.\n",
    "\n",
    "As an example, consider baking in linear expressions subject to a background theory of linear equalities. Anything on a line/hyperplane would also be a possible term.\n",
    "We could perhaps cook up a way of describing \n",
    "\n",
    "\n",
    "Bottom up ematching is a particular strategy for relational ematching. \n",
    "In it's most brute force form, one scans over the databank of terms for every variable appearing in the pattern.\n",
    "It can be optimized by pruning the possible bindings of variables by first doing a pass that finds an overapproximation of the sets the variables can bind to. This is akin to a WCOJ.\n",
    "\n",
    "\n",
    "## What patterns are easily implementable\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Replacing Union Finds with Other Canonizers\n",
    "\n",
    "The union find is a data structure for solving atomic equations. Given an atom, it can return a canonical atom for it's equivalence class.\n",
    "\n",
    "There are other canonizer producing algorithms.\n",
    "\n",
    "Atomic Knuth Bendix completion solves the same problem. The rewrite rules can be viewed as a manifestation of the union find. One difference is that Knuth Bendix is parametrized on a a term ordering. The union find can be done in a style with a determinstic ordering, or it can be done in a way that depends on the order of the asserted unions. This freedom enables the union find to keep itself flat and achieve a better asymptotic complexity.\n",
    "\n",
    "Atomic multiset completion also  \n",
    "\n",
    "Taking the row echelon form of a set of linear equations builds a canonizer.\n",
    "\n",
    "Grobner bases form a canonizer for polynomial expressions under the assumption polynomial equalities.\n",
    "\n",
    "You can also use an entire knuth bendix procedure as your canonizer producer. In this case it may not terminate though.\n",
    "\n",
    "\n",
    "By enriching the union find to one of these other canonizers, we can push some of the work that would be done by the egraph or rules into the union find.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Some function symbols can be chosen to belong to enodes, and some to the structured eids. Baked in rules can only talk about the symbols in the strucutred eids and the other symbols are considered foreign.\n",
    "\n",
    "At one extreme we have the egraph with it's atomic integer ids.\n",
    "\n",
    "At the other extreme, the \n",
    "\n",
    "\n",
    "# Extract and Rewrite\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=udORacqkExg&ab_channel=ACMSIGPLAN  WiP: Labeled Union-Find for Constraint Factorization\n",
    "\n",
    "\n",
    "Completion without failure  https://www.cs.tau.ac.il/~nachum/papers/unfail-paper.pdf\n",
    "\n",
    "Le Chenadec - Canonical Forms in Finitely Presented Algebras . Can't find it\n",
    "\n",
    "\n",
    "normalized completion\n",
    "https://drops.dagstuhl.de/storage/00lipics/lipics-vol021-rta2013/LIPIcs.RTA.2013.319/LIPIcs.RTA.2013.319.pdf\n",
    "https://www.sciencedirect.com/science/article/pii/S0747717196900115  Normalized Rewriting: an Alternative to Rewriting modulo a Set of Equations\n",
    "\n",
    "https://dl.acm.org/doi/pdf/10.1145/800017.800519 jouannaud kirchner - completion of a set of rules modlo a set of equations.\n",
    "\n",
    "https://wiki.bordeaux.inria.fr/Helene-Kirchner/lib/exe/fetch.php?media=wiki:rsp.pdf  rewriting solving proving. Quite a table of contents.\n",
    "\n",
    "t < s as an inference rule system\n",
    "\n",
    "\n",
    "if finitariness is good, then actually strongly simplifying R is bad.\n",
    "\n",
    "\n",
    "Build my own smt solver? My own outer Z3 solver.\n",
    "Make a command line form that takes in smtlib (via z3's parser)\n",
    "\n",
    "assoc has 3 variables.\n",
    "A + C is ok though.\n",
    "Bubble sort completion (a + (b + c)) -> (b (+ a + c))\n",
    "\n",
    "You can N-theory it by statically choosing a depth N that rules have been theorified.\n",
    "\n",
    "The ordering can never be stable.\n",
    "\n",
    "\n",
    "Group union find. g x = g2 x. Overlap can be achieved by inverted the group element.\n",
    "monoid union find is fine. Or string/semigroup action (no unit). This maps onto working always at the end of the string. And is ground equations basically. f(ff(endsymbols))\n",
    "\n",
    "\n",
    "Completeness\n",
    "herbrand https://www.youtube.com/watch?v=bdmigQsf_uY&ab_channel=LawrencePaulson\n",
    "https://math.stackexchange.com/questions/2468277/intuition-about-herbrand-models\n",
    "\n",
    "https://terrytao.wordpress.com/2009/04/10/the-completeness-and-compactness-theorems-of-first-order-logic/\n",
    "\n",
    "\n",
    "Making complete refutation system out of egglog. You can collect all needed generations. Enough to simulate the needed\n",
    "f(x) --> \n",
    "\n",
    "\n",
    "Set rewriting for idempotent. Set term orders? Set rewriting is very close or identities with ground ordered resolution. But set rewriting doesn't have removal replacement semantics. It has monotonic semantics.\n",
    "\n",
    "Knuth bendix with constraints is an interesting paradigm. Overlap is normal overlap + combined contraints being satisfiable / consistent.\n",
    "\n",
    "ground + C is maybe the simplest one. I basically ocnsidered the container version. But if we push C into the eid, we make eids C symbols  + opqaue constants. \n",
    "\n",
    "ground + I. everything is either opqaue a, or f(a). It might be better to rewrite _to_ `b -> f(a)` because absortbing is better than not.\n",
    "\n",
    "Multiple I symbosls. f(g(f(a))). It is excessive to do this. As soon as we go to \n",
    "\n",
    "We can do top down ematching by delegating to a theory specific matching for the eid when the pattern hits symbols that belong to the eids.\n",
    "\n",
    "You put everything into the eid that you'd need to identify\n",
    "\n",
    "stratified egraphs - consider a structured eid who's theory is _also_ ground equations. The structured eid is now a term who only contains function symbols from lower egraphs or atomics eids. Canonization is by extraction from the lower theory.\n",
    "\n",
    "G1 + G2 is obviosuly confluent and terminating even though a modular problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- relational AC-matching\n",
    "- https://www.philipzucker.com/egraph2024_talk_done/\n",
    "- https://www.philipzucker.com/linear_grobner_egraph/\n",
    "- https://www.philipzucker.com/coegraph/\n",
    "- https://www.philipzucker.com/string_knuth/\n",
    "- https://www.philipzucker.com/smart_constructor_aegraph/\n",
    "- https://www.philipzucker.com/multiset_rw/\n",
    "- https://www.philipzucker.com/bottom_up/ normalizing containers\n",
    "\n",
    "\n",
    "a theory is ground exendable if it remains completeable with ground equations.\n",
    "\n",
    "Associativity is not groud extendable.\n",
    "\n",
    "\n",
    "Enodes ~ syntax\n",
    "eids ~ semantics\n",
    "\n",
    "When you ask z3 for a model, it returns numerical semantic values.\n",
    "\n",
    "Equality saturation starts as pure syntactic equality and gradually moves over closer to semantic equality (which is often never reaches).\n",
    "\n",
    "There are two main approaches I've been trying (or that I'm aware of):\n",
    "\n",
    "- Containers as smarter enodes\n",
    "- structured / semantic eids\n",
    "\n",
    "The enode structure I used is a function symbol + an ordered list of arguments. This is already kind of smart compared to a function symbol + a binary tuple of args. This tuple form can be interesting if you want to talk about partial application.\n",
    "\n",
    "\n",
    "Destructive rewriting as a theory.\n",
    "If you have a good rewrite theory over a system (terminating and confluent), you can already effectively quotient the syntax trees by that notion of equivalence be greedily applying the rules.\n",
    "\n",
    "```\n",
    "add(succ(X), Y) -> succ(add(X,Y))\n",
    "add(Z, Y) -> Y\n",
    "```\n",
    "\n",
    "\n",
    "Combination problems\n",
    "https://www.sciencedirect.com/science/article/pii/S0890540101931189 Deciding the Word Problem in the Union of Equational Theories\n",
    "\n",
    "Canonization for disjoint unions of theories Sava Krstic  Sylvain Conchon\n",
    "\n",
    "Disjoint signatures\n",
    "Termination is not modular\n",
    "Confluence is modular\n",
    "Normalizing is modular\n",
    "\n",
    "Ground rewrite rules / equalities are quite special.\n",
    "\n",
    "The following no go counterexample crushes most hopes of generic good results.\n",
    "\n",
    "Associativity + ground term equations = string equality and is undecidable.\n",
    "\n",
    "You can build a normalizing rewrite rule system for associativity by associating to the right.\n",
    "`(x . y) . z -> x . (y . z)`\n",
    "\n",
    "Ok but actually string rewrite systems are rules of form (yadayda . X) -> (something . X). They are not ground. Yeah but associatvity can make them non ground\n",
    "A ground _term_ equation corresponds more directly to the string rewrite that only applies at the end of the string. \n",
    "\n",
    "For disjoint signatures, there doesn't seem like their should be a problem. Why were eids integers in the first place? Who cares? The pieces of the other signature completely block rules from applying.\n",
    "\n",
    "\n",
    "Pick a ground total termination ordering compatible with the rewrite relation.\n",
    "COnsider the particular stratgey: orient the built in rules, All confluence pairs are now redundant by assumption.\n",
    "Start orienting the ground equations. simplify reduces by the built ins. This is \"putting into the egraph\".\n",
    "Any symbol not part of built ins can have a generated guy.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Ok, so we want non-disjoint signatures.\n",
    "\n",
    "\n",
    "What terms are \"in\" the egraph:\n",
    "1. A generative point of view (top down production of terms). Given an eid, we can build. The view is related to top down ematching\n",
    "2. An acceptor point of view. If we insert of term bottom up into the egraph, does it change it or leave it the same? This view is related to bottom up ematching.\n",
    "\n",
    "We can consider maybe adding new function symbols over \n",
    "\n",
    "\n",
    "Can we put eids and enodes on the same footing? If I had 3 theories I wanted to bake in. Does baked in go into eids? Or 2 notions of eids?\n",
    "\n",
    "https://egraphs.zulipchat.com/#narrow/channel/328972-general/topic/Linear.20and.20Polynomial.20Equations/near/477290392\n",
    "\n",
    "\n",
    "https://egraphs.zulipchat.com/#narrow/channel/424128-egg.2Fgeneral/topic/Translating.20between.20languages.20in.20an.20egraph/near/499111241 translation between mutiple languages slows stuff down.\n",
    "Related to refinement egraph ideas?\n",
    "\n",
    "\n",
    "|           |   UF    | Theory |\n",
    "| ---       |  ---    |        |\n",
    "| Sorts     | \n",
    "| Enodes    |\n",
    "| Eclasses  |\n",
    "| Functions |\n",
    "| Rules     |   \n",
    "\n",
    "\n",
    "Egraph modulo theories:\n",
    "What is a theory?\n",
    "\n",
    "https://pldi25.sigplan.org/home/egraphs-2025\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SortRef():\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.uf = []\n",
    "        # self.utree # to maintain eclasses\n",
    "        # self.reason # to maintain provencnance / proof\n",
    "    def find(self, i): # canonize\n",
    "        while self.uf[i] != i:\n",
    "            i = self.uf[i]\n",
    "        return i\n",
    "    def union(self, i, j):\n",
    "        i = self.find(i)\n",
    "        j = self.find(j)\n",
    "        if i != j:\n",
    "            self.uf[i] = j\n",
    "    def make(self): # Const?\n",
    "        self.uf.append(len(self.uf))\n",
    "        return len(self.uf)-1\n",
    "        \n",
    "\n",
    "class FuncDeclRef(): # MergeDict\n",
    "    def __init__(self, name, *sorts):\n",
    "        self.data = {}\n",
    "        self.sorts = sorts\n",
    "    def __getitem__(self, args):\n",
    "        assert len(args) == len(self.sorts) - 1\n",
    "        return self.data[args]\n",
    "    def __call__(self, *args):\n",
    "        assert len(args) == len(self.sorts) - 1\n",
    "        res = self.data.get(args)\n",
    "        if res is None:\n",
    "            res = self.default()\n",
    "            self.data[args] = res\n",
    "            return res\n",
    "        else:\n",
    "            return res\n",
    "    def __setitem__(self, key, value):\n",
    "        assert len(key) == len(self.sorts) - 1\n",
    "        res = self.data.get(key)\n",
    "        if res is None:\n",
    "            self.data[key] = value\n",
    "        else:\n",
    "            self.data[key] = self.merge(res, value)\n",
    "    def default(self):\n",
    "        return self.sorts[-1].make()\n",
    "    def merge(self,i,j):\n",
    "        return self.sorts[-1].union(i,j)\n",
    "    def rebuild(self):\n",
    "        fnew = FuncDeclRef(self.name, *self.sorts)\n",
    "        for k,v in self.data.items():\n",
    "            newk = tuple(sort.canon(k) for k,sort in zip(k,self.sorts[:-1]))\n",
    "            fnew[newk] = self.sorts[-1].canon(v)\n",
    "        return fnew\n",
    " \n",
    "Int = SortRef(\"Int\")\n",
    "add = FuncDeclRef(\"add\", Int, Int, Int)\n",
    "x = FuncDeclRef(\"x\", Int)()\n",
    "add(x,x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MergeDict(dict):\n",
    "    def __setitem__(self, key, value):\n",
    "        res = self.get(key)\n",
    "        if res is None:\n",
    "            super().__setitem__(key, value)\n",
    "        else:\n",
    "            super().__setitem__(key, self.merge(res, value))\n",
    "    def merge(self, i, j):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "class MinDict(MergeDict):\n",
    "    def merge(self, i, j):\n",
    "        return min(i, j)\n",
    "\n",
    "class MaxDict(MergeDict):\n",
    "    def merge(self, i, j):\n",
    "        return max(i, j)\n",
    "\n",
    "d = MinDict()\n",
    "d[1] = 2\n",
    "d[1] = 3\n",
    "assert d[1] == 2\n",
    "d[1] = 1\n",
    "assert d[1] == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSFuncDeclRef(FuncDeclRef):\n",
    "    # multiset funcdecl ref. \n",
    "    # Putting the \"ACness\" into the functions\n",
    "    # Containers ~ enodes\n",
    "    def rebuild(self):\n",
    "        fnew = MSFuncDeclRef(self.name, *self.sorts)\n",
    "        for k,v in self.data.items():\n",
    "            fnew[k] = v\n",
    "        return fnew\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACSort():\n",
    "    # putting the \"ACness\" in the sort.\n",
    "    # structured eids which are multisets\n",
    "    # containers ~ eids\n",
    "    def __init__(self, name, inner):\n",
    "        self.name = name\n",
    "        self.inner = inner\n",
    "    def rebuild(self, xs : tuple): # rebuild / re-canonize\n",
    "        return tuple(sorted([self.inner.rebuild(x) for x in xs]))\n",
    "\n",
    "class ACISort():\n",
    "    # ACI ~ sets\n",
    "    def __init__(self, name, inner):\n",
    "        self.name = name\n",
    "        self.inner = inner\n",
    "    def rebuild(self, xs : tuple): # rebuild / re-canonize\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A notion of \"kinds\". Sorts may have different structured ids. The description or data of thse may lie in the Kind data strucctures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UninterpretedKind():\n",
    "    def __init__(self, name):\n",
    "        self.uf = []\n",
    "    def make(self):\n",
    "        self.uf.append(len(self.uf))\n",
    "        return len(self.uf)-1\n",
    "    def union(self, i, j):\n",
    "        return i\n",
    "    def canon(self, i):\n",
    "        return i\n",
    "    def rebuild(self, i):\n",
    "        return self.find(i)\n",
    "\n",
    "\n",
    "class SortRef():\n",
    "    def __init__(self, name, kind=None):\n",
    "        self.name = name\n",
    "        if kind is None:\n",
    "            kind = UninterpretedKind()\n",
    "        else:\n",
    "            self.kind = kind\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could easily have multiple notions of equality.\n",
    "They don't necessarily have to have much to do with each other. But we can't normalize across them except at the commonality.\n",
    "There is a truly baked in equality that can be destructive.\n",
    "\n",
    "We could only canonize when they agree they canonize. Or maintain a fixpoint of the canonization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtomicEq():\n",
    "    def __init__(self, sort):\n",
    "        self.sort = sort\n",
    "        self.uf = []\n",
    "    def canon(self, i):\n",
    "        pass\n",
    "    def rebuild():\n",
    "\n",
    "class LinearEq():\n",
    "    def __init__(self, sort):\n",
    "        self.sort = sort\n",
    "        self.uf = []\n",
    "    def canon(self, i):\n",
    "        pass\n",
    "    def rebuild():\n",
    "        pass # do gauss elim?\n",
    "\n",
    "class RecursiveEq(): # FixEq\n",
    "    pass # todo\n",
    "\n",
    "\n",
    "class GroupActEq():\n",
    "    pass # Can have group actions on the side of the UF\n",
    "\n",
    "class StringEq(): ...\n",
    "\n",
    "class FixEq(): ...\n",
    "\n",
    "class UnionEq():\n",
    "    def __init__():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are quotienting by different equalities.\n",
    "Having them play nice with each other is a theory combination problem.\n",
    "\n",
    "Partitions ~ equaiovlanece relations.\n",
    "Finest partition or corasest partition that contains the equivalence relations.\n",
    "Equivalence relations form a lattice.\n",
    "https://en.wikipedia.org/wiki/Equivalence_relation\n",
    "\n",
    "We can bounce around two equivalence relations is we have a notion of well founded ordering we are following and get the canonizer for the combo (the coarser partition with more equalities).\n",
    "\n",
    "I don't know how we'd get a canonizer for the intersection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuotientSort():\n",
    "    def __init__(self, sort, eq):\n",
    "        self.sort = sort\n",
    "        self.eq = eq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueKind():\n",
    "    # literals like literal integers\n",
    "    def rebuild(self, i):\n",
    "        return i\n",
    "    \n",
    "# This makes sense for Int where we have abstract Int expressions, but also sometimes literal int values. \"Lifting\" is somewhat awkward.\n",
    "# Perhaps related to my notion of observation table for coegraphs. A ground value is a very powerful observation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnionKind(): # union or disjoint union / tagged union?\n",
    "    def __init__(self, *kinds):\n",
    "        self.kinds = kinds\n",
    "    def rebuild(self, i):\n",
    "        tag, inner = i\n",
    "        return (tag, self.kinds[tag].rebuild(inner))\n",
    "    def make(self, tag): # Hmm. Make takes stuff now...\n",
    "        return (tag, self.kinds[tag].make())\n",
    "\n",
    "class IntersectionKind():\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the ground rewrite system perspective, what are we doing?\n",
    "\n",
    "A specialized notion of overlap\n",
    "a compositional form of term ordering. Making a term ordering from a term ordering of the contained elements.\n",
    "a notion of pattern matching and replacement / removal\n",
    "\n",
    "\n",
    "\n",
    "TermKind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ACDict():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ExprRef():\n",
    "    def __init__(self, id_, sort):\n",
    "        self.sort = sort\n",
    "        self.id_ = id_\n",
    "    def __eq__(self, other):\n",
    "        assert self.sort is other.sort\n",
    "        self.sort.union(self, other)\n",
    "    def eq(self, other):\n",
    "        assert self.sort is other.sort\n",
    "        return self.sort.find(self.id_) == self.sort.find(other.id_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# herbrand\n",
    "\n",
    "Why do the first order sentences have to be quantifier free to apply propositional logic theorems?\n",
    "\n",
    "https://mathweb.ucsd.edu/~sbuss/ResearchWeb/herbrandtheorem/paper.pdf buss hebrand theorem\n",
    "\n",
    "Graham says two forms of herbrand.\n",
    "A direct proof translation form?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eprover\n",
    "\n",
    "Run eprover to saturation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Preprocessing class: HSSSSMSSSSSNFFN.\n",
      "# Scheduled 4 strats onto 8 cores with 300 seconds (2400 total)\n",
      "# Starting new_ho_10 with 1500s (5) cores\n",
      "# Starting ho_unfolding_6 with 300s (1) cores\n",
      "# Starting sh4l with 300s (1) cores\n",
      "# Starting ehoh_best_nonlift_rwall with 300s (1) cores\n",
      "# new_ho_10 with pid 2024504 completed with status 9\n",
      "# sh4l with pid 2024506 completed with status 9\n",
      "# ehoh_best_nonlift_rwall with pid 2024507 completed with status 9\n",
      "# ho_unfolding_6 with pid 2024505 completed with status 9\n",
      "# Schedule exhausted\n",
      "# SZS status GaveUp\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from kdrag.all import *\n",
    "\n",
    "T = smt.DeclareSort(\"T\")\n",
    "x,y,z = smt.Consts(\"x y z\", T)\n",
    "add = smt.Function(\"add\", T, T, T)\n",
    "kd.notation.add.register(T,add)\n",
    "\n",
    "s = kd.solvers.EProverTHFSolver()\n",
    "s.add(smt.ForAll([x,y,z], x + ( y + z) == (x + y) + z))\n",
    "s.check()\n",
    "print(s.res.stdout.decode())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /tmp/ac.p\n"
     ]
    }
   ],
   "source": [
    "%%file /tmp/ac.p\n",
    "\n",
    "cnf(assoc, axiom, f(X,f(Y,Z)) = f(f(X,Y),Z)).\n",
    "cnf(comm, axiom, f(X,Y) = f(Y,X)).\n",
    "cnf(biz, axiom, f(a,f(b,c)) = f(c,f(a,b))).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /tmp/terms.p\n"
     ]
    }
   ],
   "source": [
    "%%file /tmp/terms.p\n",
    "\n",
    "f(c,f(q,r))\n",
    "f(f(a,c),a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Initializing proof state\n",
      "# Scanning for AC axioms\n",
      "# f is AC\n",
      "# AC handling enabled\n",
      "#\n",
      "#cnf(i_0_5, plain, (f(X1,X2)=f(X2,X1))).\n",
      "#\n",
      "#cnf(i_0_4, plain, (f(f(X1,X2),X3)=f(X1,f(X2,X3)))).\n",
      "#\n",
      "#cnf(i_0_7, plain, (f(X1,f(X2,X3))=f(X3,f(X1,X2)))).\n",
      "##\n",
      "#cnf(i_0_10, plain, (f(X2,f(X1,X3))=f(X1,f(X2,X3)))).\n",
      "######\n",
      "#cnf(i_0_27, plain, (f(X3,f(X2,X1))=f(X1,f(X2,X3)))).\n",
      "######################################################################################\n",
      "# No proof found!\n",
      "# SZS status Satisfiable\n",
      "# Processed positive unit clauses:\n",
      "cnf(i_0_4, plain, (f(f(X1,X2),X3)=f(X1,f(X2,X3)))).\n",
      "cnf(i_0_5, plain, (f(X1,X2)=f(X2,X1))).\n",
      "cnf(i_0_7, plain, (f(X1,f(X2,X3))=f(X3,f(X1,X2)))).\n",
      "cnf(i_0_10, plain, (f(X1,f(X2,X3))=f(X2,f(X1,X3)))).\n",
      "cnf(i_0_27, plain, (f(X1,f(X2,X3))=f(X3,f(X2,X1)))).\n",
      "\n",
      "# Processed negative unit clauses:\n",
      "\n",
      "# Processed non-unit clauses:\n",
      "\n",
      "# Unprocessed positive unit clauses:\n",
      "\n",
      "# Unprocessed negative unit clauses:\n",
      "\n",
      "# Unprocessed non-unit clauses:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! eprover-ho /tmp/ac.p -S #| enormalizer -t /tmp/terms.p -f -\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
