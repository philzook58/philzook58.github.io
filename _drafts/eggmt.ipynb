{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lattices should be a theory\n",
    "a <= b  <---->  a = a meet b\n",
    "\n",
    "kleene algerba as a theory. Not quiiiite equational. But who cares. Kleene algebra modulo tests. KAT mod theory\n",
    "I met Mark Moller. works with Silva, Foster\n",
    "Greebnberg\n",
    "https://en.wikipedia.org/wiki/Kleene_algebra related to floyd warshall algorithm. ALgerbaic paths. Provenance semirings.\n",
    "Jives with the groupoid proof objects thing.\n",
    "\n",
    "brzokowski is a \"derivative\" operation also\n",
    "Hmm. propositional hoare logic. We were suggesting that undefined bevhavior can be seen as a hoare rule that \n",
    "```\n",
    "{P /\\ ass} S {Q}\n",
    "--------------\n",
    " {P} S {Q}\n",
    "```\n",
    "\n",
    "```\n",
    "{P /\\ x != 0} z := y / x {Q}\n",
    "----------------------------\n",
    " {P} z := y / x {Q} \n",
    "```\n",
    "\n",
    "relational hoare logic and rewriting.\n",
    "\n",
    "\n",
    "\n",
    "We can encode string rewriting into finitely presented kleene. We're screwed. aba = cd  using concat / multiply.\n",
    "Hmm. Yeah, I said \"strings are good for sequencing\" That's exactly wat kleene modelling does. Can coegraphs be seen in this light?\n",
    "\n",
    "But maybe tests are better since commute?\n",
    "Also if we only add atomic equality `e1 = e2` We could probably noramlize\n",
    "\n",
    "When I google, I get my blog posts aaying that minimized automata that accept regex are indeed a kind of normal form structure. Hmm.\n",
    "\n",
    "Kozen and Mamouras’s Kleene algebra with equations [35]\n",
    "is perhaps the most closely related work:  https://link.springer.com/chapter/10.1007/978-3-662-43951-7_24\n",
    "\n",
    "\n",
    "Smolka et al. [52] find an almost linear algorithm for checking equivalence of guarded KAT terms (O(n · α(n)), where\n",
    "α is the inverse Ackermann function),\n",
    "https://dl.acm.org/doi/10.1145/3371129  Guarded Kleene algebra with tests: verification of uninterpreted programs in nearly linear time\n",
    "hw to write a coequation https://arxiv.org/abs/2109.11967\n",
    "\n",
    "https://dl.acm.org/doi/pdf/10.1145/3656454 a fast smolic verifier for \n",
    "\n",
    "\n",
    "Symkat is a powerful decision procedure for symbolic KAT,\n",
    "but doesn’t work in our concrete setting [43]. It’s possible to\n",
    "give symkat extra equations, and it can solve some equivalences that KMT can, b\n",
    "http://doi.acm.org/10.1145/2676726.2677007 https://hal.archives-ouvertes.fr/hal-01021497v2/document\n",
    "and\n",
    "more specifically the BDD unification sub-algorithm (Figure 3.3)\n",
    "is reminiscent of Remy’s extension of this unification algorithm ´\n",
    "for dealing with row types—\n",
    "BDDs + UF... very intriuging\n",
    "\n",
    "\n",
    "KATs could be a great source of egraph benchmarks too\n",
    "\n",
    "Goens suggested difference rings s better beaved than differential rings.\n",
    "\n",
    "\n",
    "\n",
    "asbtract congruence closure. Bachmair\n",
    "\n",
    "\n",
    "Nelson Oppen style. The labelled union find dicussed the ability to discover a = b equations on the nose that appear. index parent back to children\n",
    "register theories\n",
    "```python\n",
    "class Theory(Protocol):\n",
    "\n",
    "\n",
    "class EGraph():\n",
    "    theories : list[Theory]\n",
    "```\n",
    "Rudi's collecting the symmettry gropu on the root thing is probably analogous to having a single variable anstract domain on the root. The commutative semigroup action offered by the union find dict\n",
    "\n",
    "Multiple parallel Group union finds. They can propagate to the value analyses and propagate to each other (?). Maybe being compataible requires that they are in actually a refininig relationship. Twoers.\n",
    "\n",
    "\n",
    "Every judgement comes in both asking and asserting modes\n",
    "\n",
    "\n",
    "Frex. Free extensions of algerba. Is evocative of stehphen dolan stuff. Is evocative of the idea of \"foreign\" entitites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kdrag.solvers.egraph import EGraph\n",
    "\n",
    "class DependentEGraph(EGraph):\n",
    "    sets : dict[smt.ExprRef, set[smt.ExprRef]]\n",
    "    def is_type(self, A: smt.ExprRef) -> bool:\n",
    "        return self.find(A) in self.sets\n",
    "    def make_type(self, expr: smt.ExprRef) -> None:\n",
    "        if expr not in self.sets:\n",
    "            self.sets[expr] = set()\n",
    "    def has_type(self, t : smt.ExprRef, A : smt.ExprRef) -> bool:\n",
    "        A = self.find(A)\n",
    "        if self.is_type(A):\n",
    "            return self.find(t) in self.sets[A]\n",
    "        else:\n",
    "            return False\n",
    "    def assign_type(self, t: smt.ExprRef, A: smt.ExprRef) -> None:\n",
    "        A = self.find(A)\n",
    "        if not self.is_type(A):\n",
    "            raise ValueError(f\"Cannot assign type {A} to {t}, {A} is not a type\")\n",
    "        t = self.find(t)\n",
    "        self.sets[A].add(t)\n",
    "    def is_def_eq(self, t1: smt.ExprRef, t2: smt.ExprRef, A : smt.ExprRef) -> bool:\n",
    "        return self.has_type(t1, A) and self.has_type(t2, A) and self.find(t1).eq(self.find(t2))\n",
    "    def assert_def_eq(self, t1: smt.ExprRef, t2: smt.ExprRef, A: smt.ExprRef) -> None:\n",
    "        A = self.find(A)\n",
    "        self.assign_type(t1, A)\n",
    "        self.assign_type(t2, A)\n",
    "        self.union(t1, t2)\n",
    "    def rebuild(self):\n",
    "        super().rebuild()\n",
    "        newsets = defaultdict(set)\n",
    "        for k, v in self.sets.items():\n",
    "            newsets[self.find(k)] |= {self.find(x) for x in v}\n",
    "        self.sets = newsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "# 06/2025\n",
    "\n",
    "\n",
    "(N/E)  is a \"theory Factor\" F\n",
    "If F becomes inifinty, you are screwed\n",
    "\n",
    "\n",
    "\n",
    "Why grobner bases? I porbably am interested in solutions over R or C. Why not Wu / Charactersitic Set methods? Well because I am confused by them.\n",
    "\n",
    "Proofs and extraction\n",
    "Proofs require proof tracking row echelon or grobner.\n",
    "proof objects are linear combination of the original equations.\n",
    "\n",
    "abelian categoires. formal sums of strings. If there are no loops in the category, then might be decidable. That is a raraity though. Formal sums of strings.\n",
    "\n",
    "\n",
    "\n",
    "App as a built in theorem. Partial application. These are just lists? But you get immediate access to the head which is nice. So you can know when fully applied.\n",
    "\n",
    "\n",
    "HORPO\n",
    "CPO\n",
    "https://www.tcs.ifi.lmu.de/mitarbeiter/jasmin-blanchette/lambda_free_rpo_rep.pdf lfrpo\n",
    "\n",
    "\"Rewriting of λ-free higher-order terms has been amply studied in the literature, under\n",
    "various names such as applicative term rewriting [27] and simply typed term rewriting [41]. Translations from higher-order to first-order term rewriting systems were designed by Aoto and Yamada [1], Toyama [39], Hirokawa et al. [20], and others. Toyama\n",
    "also studied S-expressions, a formalism that regards ((f a) b) and (f a b) as distinct.\n",
    "For higher-order terms with λ-abstraction, various frameworks have been proposed, including Nipkow’s higher-order rewrite systems [34], Jouannaud and Okada’s algebraic\n",
    "functional systems [24], Blanqui’s inductive data type systems [9], and Kop’s algebraic\n",
    "functional systems with metavariables [28]. Kop’s thesis [28, Chapter 3] includes a\n",
    "comprehensive overview.\n",
    "\"\n",
    "\n",
    "\n",
    "van kampen \n",
    "\n",
    "Bottom up ematching on contextual eids.\n",
    "sweep all (contexts, eid). total context is sum of all contexts?\n",
    "eid has context, but then also equation between eids could have context.\n",
    "Gamma |- E(t)  existsence/wellformed  and  Gamma |- t1 = t2 as separate judgements\n",
    "\n",
    "refinement egraph? Different version of the egraph are in refinmement of each other. Maybe the aegraph union nodes allows us to refer to different versions of the egraph?\n",
    "\n",
    "A bool type with values that are true,false, and equational clauses / horn.   |   | => eq(a,b). The eqautions are normalized by ground superposition.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Impossible Patterns\n",
    "\n",
    "A formulation of the pattern matching problem is given a ground term t and a pattern p find a substitution $\\sigma$ such that\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Constructors as a theory.\n",
    "foo(cons(a), cons(b)) = cons()\n",
    "Seems to be similar in many respects to co-egraphs\n",
    "foo(X,Y) = Z  really does have unification vibes. But we are in 1 world.\n",
    "X does really have to be \"eventually\" cons. This is not really what we want. (?)\n",
    "X has multiple branching worlds coming out from it. could be z or succ.\n",
    "\n",
    "\n",
    "\n",
    "## rewriting modulo macros.\n",
    "f(x,y,z) = body.\n",
    "linear, single definition term. We could inline\n",
    "\n",
    "eids can be f expressions. They can be forced on demand.\n",
    "A silly example. Useful? Expansion can bloat the egraph.\n",
    "This is like having primitive funcalls in the actions.\n",
    "\n",
    "Primtiives and containers are in the mix. \n",
    "\n",
    "\n",
    "\"macros\" + constructors? That's functional programming (?). Close. We can't do case discrimination.\n",
    "\n",
    "\n",
    "Enodes\n",
    "Containers  --> Gneralized enodes\n",
    "primitives\n",
    "\n",
    "Eids\n",
    "Primitives ---> generzlied eids\n",
    "containers\n",
    "\n",
    "closure(lam, rho)\n",
    "\n",
    "\n",
    "cons(a, nil) = cons(b, nil)\n",
    "If we assert this... a = b...\n",
    "\n",
    "and yet I claimed that I want to treat\n",
    "x + y = y + z  as grobner? Not E-unification problem?\n",
    "I could treat as either?\n",
    "x + y as an observation vs x + y as a thing?\n",
    "co-egraphs + eggmt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "was f(f(f(x))) ->  x and f(g(x)) -> g(f(x)) I think\n",
    "\n",
    "What examples are good?\n",
    "BitVector eqs\n",
    "Math eqs  (will AC enable x - x = 0?)\n",
    "Set eqs\n",
    "vector eqs\n",
    "\n",
    "43.6 = ?x + ?y . This is pattern is not answerable as a finite or even countable number of substitutions\n",
    "7 = x + y\n",
    "\n",
    "{1,2,3,4} = ?x \\bigcup ?y\n",
    "\n",
    "egraphs are both equations and a term bank\n",
    "\n",
    "eqsat is ad hoc\n",
    "\n",
    "\n",
    "extract and rewrite is kind of similar to having term-like eids.\n",
    "\n",
    "eid as a set of deeper eids {eid1, eid2}\n",
    "\n",
    "\n",
    "eids mod an equivalence relatiuon on eids would be sets of eids.\n",
    "\n",
    "\n",
    "If knuth bendix completion was my theory solver\n",
    "\n",
    "Pick symbols to treat as enodes\n",
    "Picks symbols to \n",
    "\n",
    "\n",
    "master -> theory1\n",
    "master -> theory2\n",
    "masetr -> theory3\n",
    "\n",
    "\n",
    "vs\n",
    "\n",
    "outer -> inner1 -> inner2 -> inner3\n",
    "\n",
    "vs\n",
    "master -> (union/fixpoint th1 th2 th3)\n",
    "\n",
    "vs a tree?\n",
    "\n",
    "Is it not the A but the idenityt element that is a problem?\n",
    "\n",
    "\n",
    "Are ground set equations completeable? This was the idea maybe of how clauses are completion\n",
    "\n",
    "a | b | c ~ {a,b,c}\n",
    "\n",
    "{a, b, c} = {a, b}\n",
    "\n",
    "\n",
    "\n",
    "strong induction is\n",
    "\n",
    "(forall m (forall n, (n < m) -> P(n) -> P(m))) -> forall x, P(x)\n",
    "\n",
    "forall n (forall k, P(n) -> P(n + k)) -> \n",
    "\n",
    "\n",
    "\n",
    "If theories ~ sorts, then interpreting the sort as sum of different possiblityes makes sense.\n",
    "\n",
    "If I can add terms to termbank, I can remove them from termbank also without destroying the equality. This may not destroy them for record keeping. But they will be supressed as cndidates for further ematching.\n",
    "\n",
    "This is a slightly more principled notion of egraph deletion.\n",
    "We're deleting eclasses rather than enodes though. Hmm.  Usually people delete enodes.\n",
    "\n",
    "This interface does support me using \n",
    "sympy and z3 combined?\n",
    "z3values\n",
    "sympy values\n",
    "\n",
    "Really I want z3 to have access to all the theories it supports and also sympy.\n",
    "So I'd want to glue sympy and z3 on the reals.\n",
    "\n",
    "nelson oppen, but a union find as mediator, not egraph.\n",
    "Well, in the \"inner egraph\" style I could.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evidence for talk\n",
    "Count or time stuff?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kdrag.solvers.egraph import EGraph\n",
    "from kdrag.theories.real.sympy import simplify\n",
    "\n",
    "class SympyEGraph(EGraph):\n",
    "    \"\"\"\n",
    "    A specialized EGraph for SymPy expressions.\n",
    "    This class extends the base EGraph to handle SymPy's expression trees.\n",
    "    \"\"\"\n",
    "\n",
    "    def symplify(self, expr):\n",
    "        \"\"\"\n",
    "        Simplifies a SymPy expression using the simplify function.\n",
    "        \n",
    "        Args:\n",
    "            expr: The SymPy expression to simplify.\n",
    "        \n",
    "        Returns:\n",
    "            The simplified SymPy expression.\n",
    "        \"\"\"\n",
    "        for t in self.terms.values():\n",
    "            self.union(t, simplify(t))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smith differentiation\n",
    "\n",
    "polynomials over one variable are PID and eaisier than generic case.\n",
    "\n",
    "partial_t  e_1\n",
    "Is more like a module over PID single polynomials\n",
    "\n",
    "https://en.wikipedia.org/wiki/Smith_normal_form \n",
    "https://en.wikipedia.org/wiki/Principal_ideal_domain\n",
    "\n",
    "The old stuff about relational control systems\n",
    "\n",
    "Taking linear relation R circuits to LC circuits.\n",
    "\n",
    "partial_t sin(g) = -cos(g) * (partial_t g)\n",
    "\n",
    "* is unintepreted symbol here.\n",
    "\n",
    "It would be nicer to go full differential alg, but whatever.\n",
    "\n",
    "  (w^2 + 1) x0 + (...) = f\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# top down\n",
    "\n",
    "I should build a top down ematcher\n",
    "\n",
    "\n",
    "Ok, so I did a semi random thing.\n",
    "I could use recursion instead of all the todo.copy\n",
    "a purely functional dict might be better than subst.copy\n",
    "also, \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(x + x, 20)] {}\n",
      "[(x + x, 16)] {}\n",
      "[(x + x, 39)] {}\n",
      "[(x, 38), (x, 20)] {}\n",
      "[(x, 38)] {x: 20}\n",
      "[(x + x, 38)] {}\n",
      "[(x, 16), (x, 16)] {}\n",
      "[(x, 16)] {x: 16}\n",
      "[] {x: 16}\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.20190912.0211 (20190912.0211)\n",
       " -->\n",
       "<!-- Title: egraph Pages: 1 -->\n",
       "<svg width=\"335pt\" height=\"220pt\"\n",
       " viewBox=\"0.00 0.00 335.00 220.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 216)\">\n",
       "<title>egraph</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-216 331,-216 331,4 -4,4\"/>\n",
       "<g id=\"clust1\" class=\"cluster\">\n",
       "<title>cluster_39</title>\n",
       "<path fill=\"none\" stroke=\"black\" stroke-dasharray=\"1,5\" d=\"M201,-152C201,-152 269,-152 269,-152 275,-152 281,-158 281,-164 281,-164 281,-192 281,-192 281,-198 275,-204 269,-204 269,-204 201,-204 201,-204 195,-204 189,-198 189,-192 189,-192 189,-164 189,-164 189,-158 195,-152 201,-152\"/>\n",
       "</g>\n",
       "<g id=\"clust2\" class=\"cluster\">\n",
       "<title>cluster_38</title>\n",
       "<path fill=\"none\" stroke=\"black\" stroke-dasharray=\"1,5\" d=\"M139,-80C139,-80 207,-80 207,-80 213,-80 219,-86 219,-92 219,-92 219,-120 219,-120 219,-126 213,-132 207,-132 207,-132 139,-132 139,-132 133,-132 127,-126 127,-120 127,-120 127,-92 127,-92 127,-86 133,-80 139,-80\"/>\n",
       "</g>\n",
       "<g id=\"clust3\" class=\"cluster\">\n",
       "<title>cluster_16</title>\n",
       "<path fill=\"none\" stroke=\"black\" stroke-dasharray=\"1,5\" d=\"M20,-8C20,-8 160,-8 160,-8 166,-8 172,-14 172,-20 172,-20 172,-48 172,-48 172,-54 166,-60 160,-60 160,-60 20,-60 20,-60 14,-60 8,-54 8,-48 8,-48 8,-20 8,-20 8,-14 14,-8 20,-8\"/>\n",
       "</g>\n",
       "<g id=\"clust4\" class=\"cluster\">\n",
       "<title>cluster_20</title>\n",
       "<path fill=\"none\" stroke=\"black\" stroke-dasharray=\"1,5\" d=\"M239,-80C239,-80 307,-80 307,-80 313,-80 319,-86 319,-92 319,-92 319,-120 319,-120 319,-126 313,-132 307,-132 307,-132 239,-132 239,-132 233,-132 227,-126 227,-120 227,-120 227,-92 227,-92 227,-86 233,-80 239,-80\"/>\n",
       "</g>\n",
       "<!-- 39_+_38_20 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>39_+_38_20</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M261,-196C261,-196 231,-196 231,-196 225,-196 219,-190 219,-184 219,-184 219,-172 219,-172 219,-166 225,-160 231,-160 231,-160 261,-160 261,-160 267,-160 273,-166 273,-172 273,-172 273,-184 273,-184 273,-190 267,-196 261,-196\"/>\n",
       "<text text-anchor=\"middle\" x=\"246\" y=\"-174.3\" font-family=\"Times,serif\" font-size=\"14.00\">+</text>\n",
       "</g>\n",
       "<!-- e_rep_38 -->\n",
       "<!-- 39_+_38_20&#45;&gt;e_rep_38 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>39_+_38_20&#45;&gt;e_rep_38</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M236.85,-159.7C229.72,-146.19 220.06,-127.92 214.19,-116.83\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"217.19,-115.01 209.43,-107.81 211.01,-118.28 217.19,-115.01\"/>\n",
       "</g>\n",
       "<!-- e_rep_20 -->\n",
       "<!-- 39_+_38_20&#45;&gt;e_rep_20 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>39_+_38_20&#45;&gt;e_rep_20</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M270.1,-159.88C279.7,-152.18 290.27,-142.47 298,-132 301.14,-127.75 303.63,-122.47 305.44,-117.85\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"308.87,-118.63 308.72,-108.04 302.23,-116.41 308.87,-118.63\"/>\n",
       "</g>\n",
       "<!-- e_rep_39 -->\n",
       "<!-- 38_+_16_16 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>38_+_16_16</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M177,-124C177,-124 147,-124 147,-124 141,-124 135,-118 135,-112 135,-112 135,-100 135,-100 135,-94 141,-88 147,-88 147,-88 177,-88 177,-88 183,-88 189,-94 189,-100 189,-100 189,-112 189,-112 189,-118 183,-124 177,-124\"/>\n",
       "<text text-anchor=\"middle\" x=\"162\" y=\"-102.3\" font-family=\"Times,serif\" font-size=\"14.00\">+</text>\n",
       "</g>\n",
       "<!-- e_rep_16 -->\n",
       "<!-- 38_+_16_16&#45;&gt;e_rep_16 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>38_+_16_16&#45;&gt;e_rep_16</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M156.08,-87.7C154.7,-74.26 155.04,-56.1 157.11,-44.99\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"160.41,-46.16 160.65,-35.56 153.85,-43.69 160.41,-46.16\"/>\n",
       "</g>\n",
       "<!-- 38_+_16_16&#45;&gt;e_rep_16 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>38_+_16_16&#45;&gt;e_rep_16</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M167.92,-87.7C169.3,-74.26 168.96,-56.1 166.89,-44.99\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"170.15,-43.69 163.35,-35.56 163.59,-46.16 170.15,-43.69\"/>\n",
       "</g>\n",
       "<!-- 16_x_ -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>16_x_</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M130,-52C130,-52 100,-52 100,-52 94,-52 88,-46 88,-40 88,-40 88,-28 88,-28 88,-22 94,-16 100,-16 100,-16 130,-16 130,-16 136,-16 142,-22 142,-28 142,-28 142,-40 142,-40 142,-46 136,-52 130,-52\"/>\n",
       "<text text-anchor=\"middle\" x=\"115\" y=\"-30.3\" font-family=\"Times,serif\" font-size=\"14.00\">x</text>\n",
       "</g>\n",
       "<!-- 16_y_ -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>16_y_</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M58,-52C58,-52 28,-52 28,-52 22,-52 16,-46 16,-40 16,-40 16,-28 16,-28 16,-22 22,-16 28,-16 28,-16 58,-16 58,-16 64,-16 70,-22 70,-28 70,-28 70,-40 70,-40 70,-46 64,-52 58,-52\"/>\n",
       "<text text-anchor=\"middle\" x=\"43\" y=\"-30.3\" font-family=\"Times,serif\" font-size=\"14.00\">y</text>\n",
       "</g>\n",
       "<!-- 20_z_ -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>20_z_</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M277,-124C277,-124 247,-124 247,-124 241,-124 235,-118 235,-112 235,-112 235,-100 235,-100 235,-94 241,-88 247,-88 247,-88 277,-88 277,-88 283,-88 289,-94 289,-100 289,-100 289,-112 289,-112 289,-118 283,-124 277,-124\"/>\n",
       "<text text-anchor=\"middle\" x=\"262\" y=\"-102.3\" font-family=\"Times,serif\" font-size=\"14.00\">z</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7315cdec8d10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kdrag.solvers.egraph import EGraph\n",
    "from kdrag.all import *\n",
    "from typing import NamedTuple\n",
    "from collections import defaultdict\n",
    "#class ENode()\n",
    "\n",
    "class EGraphTop(EGraph):\n",
    "    def topdown(self, vs, pat): # top_searcher\n",
    "        def is_var(t):\n",
    "            return any(v.eq(t) for v in vs)\n",
    "        eclasses = self.eclasses()\n",
    "        branches = [([(pat, eid)], {}) for eid in self.roots[pat.sort()]]\n",
    "        res = []\n",
    "        while branches:\n",
    "            todo, subst = branches.pop()\n",
    "            print(todo,subst)\n",
    "            if len(todo) == 0:\n",
    "                res.append(subst) # successful match\n",
    "            else:\n",
    "                p,eid = todo.pop()\n",
    "                if is_var(p):\n",
    "                    if p not in subst:\n",
    "                        subst[p] = eid\n",
    "                        branches.append((todo, subst))\n",
    "                    elif subst[p] == eid:\n",
    "                        branches.append((todo, subst))\n",
    "                    else:\n",
    "                        continue\n",
    "                else:\n",
    "                    pf, pargs = p.decl(), p.children()\n",
    "                    for eid_args in eclasses[eid][pf]:\n",
    "                        if len(pargs) == len(eid_args):\n",
    "                            branches.append((todo + list(zip(pargs, eid_args)), subst.copy()))\n",
    "        return res\n",
    "\n",
    "    \n",
    "\n",
    "E = EGraphTop()\n",
    "x,y,z = smt.Ints(\"x y z\")\n",
    "E.add_term(x + y + z)\n",
    "E.union(y, x)\n",
    "E.rebuild()\n",
    "E.topdown([x], x + x)\n",
    "E.eclasses()\n",
    "E.dot()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kdrag.all import *\n",
    "x,y = smt.Ints(\"x y\")\n",
    "(x + y).decl() is (x + y).decl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.20190912.0211 (20190912.0211)\n",
       " -->\n",
       "<!-- Title: egraph Pages: 1 -->\n",
       "<svg width=\"320pt\" height=\"220pt\"\n",
       " viewBox=\"0.00 0.00 320.00 220.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 216)\">\n",
       "<title>egraph</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-216 316,-216 316,4 -4,4\"/>\n",
       "<g id=\"clust1\" class=\"cluster\">\n",
       "<title>cluster_15</title>\n",
       "<path fill=\"none\" stroke=\"black\" stroke-dasharray=\"1,5\" d=\"M20,-80C20,-80 160,-80 160,-80 166,-80 172,-86 172,-92 172,-92 172,-120 172,-120 172,-126 166,-132 160,-132 160,-132 20,-132 20,-132 14,-132 8,-126 8,-120 8,-120 8,-92 8,-92 8,-86 14,-80 20,-80\"/>\n",
       "</g>\n",
       "<g id=\"clust2\" class=\"cluster\">\n",
       "<title>cluster_19</title>\n",
       "<path fill=\"none\" stroke=\"black\" stroke-dasharray=\"1,5\" d=\"M117,-152C117,-152 257,-152 257,-152 263,-152 269,-158 269,-164 269,-164 269,-192 269,-192 269,-198 263,-204 257,-204 257,-204 117,-204 117,-204 111,-204 105,-198 105,-192 105,-192 105,-164 105,-164 105,-158 111,-152 117,-152\"/>\n",
       "</g>\n",
       "<g id=\"clust3\" class=\"cluster\">\n",
       "<title>cluster_14</title>\n",
       "<path fill=\"none\" stroke=\"black\" stroke-dasharray=\"1,5\" d=\"M83,-8C83,-8 151,-8 151,-8 157,-8 163,-14 163,-20 163,-20 163,-48 163,-48 163,-54 157,-60 151,-60 151,-60 83,-60 83,-60 77,-60 71,-54 71,-48 71,-48 71,-20 71,-20 71,-14 77,-8 83,-8\"/>\n",
       "</g>\n",
       "<g id=\"clust4\" class=\"cluster\">\n",
       "<title>cluster_18</title>\n",
       "<path fill=\"none\" stroke=\"black\" stroke-dasharray=\"1,5\" d=\"M224,-80C224,-80 292,-80 292,-80 298,-80 304,-86 304,-92 304,-92 304,-120 304,-120 304,-126 298,-132 292,-132 292,-132 224,-132 224,-132 218,-132 212,-126 212,-120 212,-120 212,-92 212,-92 212,-86 218,-80 224,-80\"/>\n",
       "</g>\n",
       "<!-- 15_div_19_14 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>15_div_19_14</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M130,-124C130,-124 100,-124 100,-124 94,-124 88,-118 88,-112 88,-112 88,-100 88,-100 88,-94 94,-88 100,-88 100,-88 130,-88 130,-88 136,-88 142,-94 142,-100 142,-100 142,-112 142,-112 142,-118 136,-124 130,-124\"/>\n",
       "<text text-anchor=\"middle\" x=\"115\" y=\"-102.3\" font-family=\"Times,serif\" font-size=\"14.00\">div</text>\n",
       "</g>\n",
       "<!-- e_rep_19 -->\n",
       "<!-- 15_div_19_14&#45;&gt;e_rep_19 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>15_div_19_14&#45;&gt;e_rep_19</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M115,-124.21C115,-137.12 115,-154.44 115,-165.66\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"111.5,-166.01 115,-176.01 118.5,-166.01 111.5,-166.01\"/>\n",
       "</g>\n",
       "<!-- e_rep_14 -->\n",
       "<!-- 15_div_19_14&#45;&gt;e_rep_14 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>15_div_19_14&#45;&gt;e_rep_14</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M126.04,-87.77C131.14,-79.51 137.14,-69.39 142,-60 144.43,-55.31 146.82,-49.97 148.76,-45.43\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"152.08,-46.55 152.62,-35.97 145.6,-43.91 152.08,-46.55\"/>\n",
       "</g>\n",
       "<!-- e_rep_15 -->\n",
       "<!-- 15_a_ -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>15_a_</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M58,-124C58,-124 28,-124 28,-124 22,-124 16,-118 16,-112 16,-112 16,-100 16,-100 16,-94 22,-88 28,-88 28,-88 58,-88 58,-88 64,-88 70,-94 70,-100 70,-100 70,-112 70,-112 70,-118 64,-124 58,-124\"/>\n",
       "<text text-anchor=\"middle\" x=\"43\" y=\"-102.3\" font-family=\"Times,serif\" font-size=\"14.00\">a</text>\n",
       "</g>\n",
       "<!-- 19_*_15_14 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>19_*_15_14</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M177,-196C177,-196 147,-196 147,-196 141,-196 135,-190 135,-184 135,-184 135,-172 135,-172 135,-166 141,-160 147,-160 147,-160 177,-160 177,-160 183,-160 189,-166 189,-172 189,-172 189,-184 189,-184 189,-190 183,-196 177,-196\"/>\n",
       "<text text-anchor=\"middle\" x=\"162\" y=\"-174.3\" font-family=\"Times,serif\" font-size=\"14.00\">*</text>\n",
       "</g>\n",
       "<!-- 19_*_15_14&#45;&gt;e_rep_14 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>19_*_15_14&#45;&gt;e_rep_14</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M168.7,-159.76C171.57,-151.5 174.57,-141.38 176,-132 179.47,-109.15 181.41,-102.47 176,-80 172.84,-66.86 165.16,-53.25 159.54,-44.48\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"162.3,-42.32 153.77,-36.04 156.52,-46.27 162.3,-42.32\"/>\n",
       "</g>\n",
       "<!-- 19_*_15_14&#45;&gt;e_rep_15 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>19_*_15_14&#45;&gt;e_rep_15</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M162,-159.7C162,-146.63 162,-129.11 162,-117.93\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"165.5,-117.81 162,-107.81 158.5,-117.81 165.5,-117.81\"/>\n",
       "</g>\n",
       "<!-- 19_&lt;&lt;_15_18 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>19_&lt;&lt;_15_18</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M249,-196C249,-196 219,-196 219,-196 213,-196 207,-190 207,-184 207,-184 207,-172 207,-172 207,-166 213,-160 219,-160 219,-160 249,-160 249,-160 255,-160 261,-166 261,-172 261,-172 261,-184 261,-184 261,-190 255,-196 249,-196\"/>\n",
       "<text text-anchor=\"middle\" x=\"234\" y=\"-174.3\" font-family=\"Times,serif\" font-size=\"14.00\">&lt;&lt;</text>\n",
       "</g>\n",
       "<!-- 19_&lt;&lt;_15_18&#45;&gt;e_rep_15 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>19_&lt;&lt;_15_18&#45;&gt;e_rep_15</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M216.2,-159.7C201.56,-145.46 181.46,-125.92 170.31,-115.08\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"172.44,-112.27 162.83,-107.81 167.56,-117.29 172.44,-112.27\"/>\n",
       "</g>\n",
       "<!-- e_rep_18 -->\n",
       "<!-- 19_&lt;&lt;_15_18&#45;&gt;e_rep_18 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>19_&lt;&lt;_15_18&#45;&gt;e_rep_18</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M256.42,-159.87C265.52,-152.09 275.61,-142.33 283,-132 286.07,-127.71 288.56,-122.42 290.38,-117.81\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"293.8,-118.62 293.71,-108.03 287.17,-116.36 293.8,-118.62\"/>\n",
       "</g>\n",
       "<!-- 14_2_ -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>14_2_</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M121,-52C121,-52 91,-52 91,-52 85,-52 79,-46 79,-40 79,-40 79,-28 79,-28 79,-22 85,-16 91,-16 91,-16 121,-16 121,-16 127,-16 133,-22 133,-28 133,-28 133,-40 133,-40 133,-46 127,-52 121,-52\"/>\n",
       "<text text-anchor=\"middle\" x=\"106\" y=\"-30.3\" font-family=\"Times,serif\" font-size=\"14.00\">2</text>\n",
       "</g>\n",
       "<!-- 18_1_ -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>18_1_</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M262,-124C262,-124 232,-124 232,-124 226,-124 220,-118 220,-112 220,-112 220,-100 220,-100 220,-94 226,-88 232,-88 232,-88 262,-88 262,-88 268,-88 274,-94 274,-100 274,-100 274,-112 274,-112 274,-118 268,-124 262,-124\"/>\n",
       "<text text-anchor=\"middle\" x=\"247\" y=\"-102.3\" font-family=\"Times,serif\" font-size=\"14.00\">1</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7315cc725cd0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kdrag.solvers.egraph import EGraph\n",
    "from kdrag.all import *\n",
    "E = EGraph()\n",
    "a = smt.Int(\"a\")\n",
    "E.add_term((a * 2) / 2)\n",
    "#E.simplify_terms()\n",
    "shift = smt.Function(\"<<\", smt.IntSort(), smt.IntSort(), smt.IntSort())\n",
    "E.union(a * 2, shift(a,1))\n",
    "E.rebuild()\n",
    "E.dot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Int"
      ],
      "text/plain": [
       "Int"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smt.IntVal(1).decl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.20190912.0211 (20190912.0211)\n",
       " -->\n",
       "<!-- Title: egraph_example Pages: 1 -->\n",
       "<svg width=\"335pt\" height=\"220pt\"\n",
       " viewBox=\"0.00 0.00 335.00 220.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 216)\">\n",
       "<title>egraph_example</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-216 331,-216 331,4 -4,4\"/>\n",
       "<g id=\"clust1\" class=\"cluster\">\n",
       "<title>cluster_18</title>\n",
       "<path fill=\"none\" stroke=\"black\" stroke-dasharray=\"1,5\" d=\"M201,-152C201,-152 269,-152 269,-152 275,-152 281,-158 281,-164 281,-164 281,-192 281,-192 281,-198 275,-204 269,-204 269,-204 201,-204 201,-204 195,-204 189,-198 189,-192 189,-192 189,-164 189,-164 189,-158 195,-152 201,-152\"/>\n",
       "</g>\n",
       "<g id=\"clust2\" class=\"cluster\">\n",
       "<title>cluster_17</title>\n",
       "<path fill=\"none\" stroke=\"black\" stroke-dasharray=\"1,5\" d=\"M139,-80C139,-80 207,-80 207,-80 213,-80 219,-86 219,-92 219,-92 219,-120 219,-120 219,-126 213,-132 207,-132 207,-132 139,-132 139,-132 133,-132 127,-126 127,-120 127,-120 127,-92 127,-92 127,-86 133,-80 139,-80\"/>\n",
       "</g>\n",
       "<g id=\"clust3\" class=\"cluster\">\n",
       "<title>cluster_15</title>\n",
       "<path fill=\"none\" stroke=\"black\" stroke-dasharray=\"1,5\" d=\"M20,-8C20,-8 160,-8 160,-8 166,-8 172,-14 172,-20 172,-20 172,-48 172,-48 172,-54 166,-60 160,-60 160,-60 20,-60 20,-60 14,-60 8,-54 8,-48 8,-48 8,-20 8,-20 8,-14 14,-8 20,-8\"/>\n",
       "</g>\n",
       "<g id=\"clust4\" class=\"cluster\">\n",
       "<title>cluster_16</title>\n",
       "<path fill=\"none\" stroke=\"black\" stroke-dasharray=\"1,5\" d=\"M239,-80C239,-80 307,-80 307,-80 313,-80 319,-86 319,-92 319,-92 319,-120 319,-120 319,-126 313,-132 307,-132 307,-132 239,-132 239,-132 233,-132 227,-126 227,-120 227,-120 227,-92 227,-92 227,-86 233,-80 239,-80\"/>\n",
       "</g>\n",
       "<!-- 18_+_17_16 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>18_+_17_16</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M261,-196C261,-196 231,-196 231,-196 225,-196 219,-190 219,-184 219,-184 219,-172 219,-172 219,-166 225,-160 231,-160 231,-160 261,-160 261,-160 267,-160 273,-166 273,-172 273,-172 273,-184 273,-184 273,-190 267,-196 261,-196\"/>\n",
       "<text text-anchor=\"middle\" x=\"246\" y=\"-174.3\" font-family=\"Times,serif\" font-size=\"14.00\">+</text>\n",
       "</g>\n",
       "<!-- e_rep_17 -->\n",
       "<!-- 18_+_17_16&#45;&gt;e_rep_17 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>18_+_17_16&#45;&gt;e_rep_17</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M236.85,-159.7C229.72,-146.19 220.06,-127.92 214.19,-116.83\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"217.19,-115.01 209.43,-107.81 211.01,-118.28 217.19,-115.01\"/>\n",
       "</g>\n",
       "<!-- e_rep_16 -->\n",
       "<!-- 18_+_17_16&#45;&gt;e_rep_16 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>18_+_17_16&#45;&gt;e_rep_16</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M270.1,-159.88C279.7,-152.18 290.27,-142.47 298,-132 301.14,-127.75 303.63,-122.47 305.44,-117.85\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"308.87,-118.63 308.72,-108.04 302.23,-116.41 308.87,-118.63\"/>\n",
       "</g>\n",
       "<!-- e_rep_18 -->\n",
       "<!-- 17_+_15_15 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>17_+_15_15</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M177,-124C177,-124 147,-124 147,-124 141,-124 135,-118 135,-112 135,-112 135,-100 135,-100 135,-94 141,-88 147,-88 147,-88 177,-88 177,-88 183,-88 189,-94 189,-100 189,-100 189,-112 189,-112 189,-118 183,-124 177,-124\"/>\n",
       "<text text-anchor=\"middle\" x=\"162\" y=\"-102.3\" font-family=\"Times,serif\" font-size=\"14.00\">+</text>\n",
       "</g>\n",
       "<!-- e_rep_15 -->\n",
       "<!-- 17_+_15_15&#45;&gt;e_rep_15 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>17_+_15_15&#45;&gt;e_rep_15</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M156.08,-87.7C154.7,-74.26 155.04,-56.1 157.11,-44.99\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"160.41,-46.16 160.65,-35.56 153.85,-43.69 160.41,-46.16\"/>\n",
       "</g>\n",
       "<!-- 17_+_15_15&#45;&gt;e_rep_15 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>17_+_15_15&#45;&gt;e_rep_15</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M167.92,-87.7C169.3,-74.26 168.96,-56.1 166.89,-44.99\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"170.15,-43.69 163.35,-35.56 163.59,-46.16 170.15,-43.69\"/>\n",
       "</g>\n",
       "<!-- 15_x_ -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>15_x_</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M130,-52C130,-52 100,-52 100,-52 94,-52 88,-46 88,-40 88,-40 88,-28 88,-28 88,-22 94,-16 100,-16 100,-16 130,-16 130,-16 136,-16 142,-22 142,-28 142,-28 142,-40 142,-40 142,-46 136,-52 130,-52\"/>\n",
       "<text text-anchor=\"middle\" x=\"115\" y=\"-30.3\" font-family=\"Times,serif\" font-size=\"14.00\">x</text>\n",
       "</g>\n",
       "<!-- 15_y_ -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>15_y_</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M58,-52C58,-52 28,-52 28,-52 22,-52 16,-46 16,-40 16,-40 16,-28 16,-28 16,-22 22,-16 28,-16 28,-16 58,-16 58,-16 64,-16 70,-22 70,-28 70,-28 70,-40 70,-40 70,-46 64,-52 58,-52\"/>\n",
       "<text text-anchor=\"middle\" x=\"43\" y=\"-30.3\" font-family=\"Times,serif\" font-size=\"14.00\">y</text>\n",
       "</g>\n",
       "<!-- 16_z_ -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>16_z_</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M277,-124C277,-124 247,-124 247,-124 241,-124 235,-118 235,-112 235,-112 235,-100 235,-100 235,-94 241,-88 247,-88 247,-88 277,-88 277,-88 283,-88 289,-94 289,-100 289,-100 289,-112 289,-112 289,-118 283,-124 277,-124\"/>\n",
       "<text text-anchor=\"middle\" x=\"262\" y=\"-102.3\" font-family=\"Times,serif\" font-size=\"14.00\">z</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x714653f635f0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# refinement egraph\n",
    "https://microsoft.github.io/z3guide/docs/theories/Special%20Relations/\n",
    "What about having \n",
    "\n",
    "refines() == TransitiveClosure(refine0)\n",
    "\n",
    "def refines(self, t1, t2):\n",
    "    s.add(refine0())\n",
    "\n",
    "def is_refinement(self, t1, t2):\n",
    "    with self.solver:\n",
    "        s.add(smt.Not(trans_refine(t1,t2)))\n",
    "        res = s.check()\n",
    "        return res == smt.unsat\n",
    "\n",
    "You need to be working in uninterpreted domains with undefined elements. Or in option monad?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "doubling function symbols.\n",
    "f0(x0) == f1(x1)  means that f(x) is defined\n",
    "\n",
    "You can't really ask if a value/IntSort is defined or not. Makes no sense.\n",
    "You can ask if it must be correlated in two worlds.\n",
    "\n",
    "\n",
    "squares\n",
    "\n",
    "def defined(t):\n",
    "    return t == twin(t)\n",
    "\n",
    "Is this good enough?\n",
    "defined(t) and defined(t2) => t = t2\n",
    "\n",
    "\n",
    "defined(x), defined(y), y != 0 => div(x,y) == div0(x0,y0)\n",
    "\n",
    "Hmm. Other exAMPLEWAS\n",
    "\n",
    "\n",
    "\n",
    "Is this a way of modelling a setoid? partial setoid? We get automatic congruence\n",
    "x = x0 -> f(x0) = f(x)\n",
    "\n",
    "To assert \n",
    "x = x0 -> f(x0) = f1(x) aka forall x, f(x) == f0(x)   \n",
    "asserts that the f palys nice with setoid.\n",
    "We automatically get synnettry and transitivity \n",
    "Nooooo...\n",
    "\n",
    "No it's more like a way of estabilishing a subset of terms without.\n",
    "\n",
    "Maybe it's a meta non-det. x == x0\n",
    "f(0)(x) == f(1)(x)\n",
    "\n",
    "\n",
    "R(x,y) -> R(y,z) -> R(x,z)\n",
    "\n",
    "(x == x0) == (y == y0)\n",
    "\n",
    "\n",
    "\n",
    "whereas R(x,y) -> \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## quantifier elim\n",
    "I had simplified to the form that it seemed unsolvable.\n",
    "x + y = pi\n",
    "\n",
    "IF you insist on ground substitutions, you are screwed.\n",
    "\n",
    "Also this might be part of a larger problem. For which elimination x = pi - y might ground enough to get a solution.\n",
    "\n",
    "Theories kind of bring us away from pattern matthing to unification? U vars represent the set of all terms. Pavel's cells represent an abstract domain / sets of values. These sets my need to be split by further considerations (analog of narrowing the uvars)\n",
    "Theories deground a side?\n",
    "\n",
    "On some level, his perspective isn't that different from the chew equations up perspective on unification. Unification is quantifier elimination for purely existential problems?\n",
    "\n",
    "Where is the egraph here? What information do we use from it?\n",
    "\n",
    "But if you allow domains of solutions, maybe not\n",
    "\n",
    "Cooper method\n",
    "omega test\n",
    "CAD\n",
    "Gauss elim\n",
    "quadratic equation solvability.\n",
    "\n",
    "\n",
    "https://pavpanchekha.com/blog/lin-graphs.html\n",
    "https://pavpanchekha.com/blog/egraph-t.html\n",
    "https://pavpanchekha.com/blog/p-graphs.html\n",
    "\n",
    "\n",
    "The termbank as a model. It's a partially built herbrand model? Model based quantifier instantiation for a herbrand model is datalogy or something\n",
    "\n",
    "theoires provide patterns. yes, this is another perspective on that \"top down\" requires extra facilities. Patterns are the \"Flat form\". That does seem like a nice interface.\n",
    "\n",
    "This is evocative of contraints but it allows more chewing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## differential rings\n",
    "https://homepages.math.uic.edu/~marker/Banff/AMS_Baltimore.pdf\n",
    "models of differential fields - david marker\n",
    "Kolchin\n",
    "Ritt\n",
    "http://www.dm.uba.ar/DiffAlg/\n",
    "https://codeberg.org/francois.boulier/DifferentialAlgebra\n",
    "Mora\n",
    "\n",
    "noncommutative algebra\n",
    "differentiaion via derivations\n",
    "\n",
    "What about that plotkin stuff? derivatives at particular points not fields.\n",
    "\n",
    "Some way to mix in with my manifold stuff? fiber bundles? Dependt types?\n",
    "\n",
    "Put differential indices on eids or on enodes?  sin' = cos\n",
    "\n",
    "https://www.sciencedirect.com/science/article/abs/pii/S0001870824001919 The fundamental theorem of calculus in differential rings\n",
    "\n",
    "\n",
    "https://mathstodon.xyz/@dpiponi/114336641549958487 \"Who originally came up with the idea that a vector field is a type of derivation? I remember being \n",
    "amazed by this idea when I learnt it as an undergrad. Probably the first technical construction that I was impressed with as a technical construction.\"\n",
    "\n",
    "\"I hit hard and start with the derivation definition of vector field and tangent vector in my old book \"Gauge Fields, Knots and Gravity\".   Then I think I show curves have tangent vectors (i.e. you can differentiate a function on space as you move along a curve).   It's quite efficient so you just have to say the right words and draw the right pictures to get the intuition across.\"\n",
    "\n",
    "https://en.wikipedia.org/wiki/Derivation_(differential_algebra)\n",
    "\n",
    "characteristic sets and trignaular sets. Its like solving\n",
    "\n",
    "Knuth bendix: overlap inlcudes modulo derivatives?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://rg1-teaching.mpi-inf.mpg.de/autrea2-ss20/script-1.11.pdf \n",
    "\n",
    "interpolation\n",
    "It is the core of the Nelson Oppen thingy\n",
    "If A => B and A and B only share symbol F, G then there is a C with A => C => B and C only uses F, G\n",
    "Seems intuitive \n",
    "\n",
    "convexity Gam => A \\/ A \\/ A  implies it actually picks one. A\n",
    "\n",
    "https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=b20b57cd3bcfd94408dfca3c032266aed2e543e2 shotask light"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACTheory():\n",
    "    def __init__(self):\n",
    "        self.counter = 0\n",
    "        self.rw = {}\n",
    "\n",
    "    def canon(self, v):\n",
    "    def freshconst(self, v):\n",
    "\n",
    "\n",
    "class Z3Theory():\n",
    "    def __init__(self, solver, sort):\n",
    "        self.ts = []\n",
    "        self.uf = {}\n",
    "        self.solver = solver\n",
    "        self.sort = sort\n",
    "    def eq(self, v1, v2):\n",
    "        if v1.eq(v2):\n",
    "            return True\n",
    "    def fresh(self):\n",
    "        t = smt.FreshConst(self.sort)\n",
    "        self.ts.append(t)\n",
    "        return t\n",
    "    def canon(self, v):\n",
    "    def rebuild(self):\n",
    "        for t in ts:\n",
    "            for s in ts:\n",
    "                if self.eq(t,s):\n",
    "                    self.union(t,s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Theory(): # Sort?\n",
    "    V : type\n",
    "    def eq(self, a : V, b: V) -> bool: ...   # eq\n",
    "    def makeset(self) -> V: ...  # \n",
    "    def canon(self, V) -> V: ... # find. Actually optional? We can do brute force compression.\n",
    "    def topmatch(self, V, pat) -> list[V]:...  # if supports top down matchin\n",
    "    def rebuild(self): ...\n",
    "\n",
    "def EGraph():\n",
    "    sorts: dict[str, Theory]\n",
    "    enodes : dict[FuncDecl, dict[tuple[Any, ...], Any]]\n",
    "\n",
    "\n",
    "# is this sum thoery nelson oppen combination?\n",
    "class SumTheory(Theory):\n",
    "    #uf : dict[int, int | tuple[int, Any]]\n",
    "    uf : dict[Any,Any] # cross theory rewrites. Orient Towards tag 0. uf is kind of an enode table\n",
    "    #? Refinement tower cast(a) = b? Yes equalities at low levels do not effect those at high. Uhhh. No maybe they do. \n",
    "    # (1, x) -> (0, y).  (1, z) -> (0, q).   (0, y) = (0, q). Then yes.\n",
    "    theories : list[Theory]\n",
    "    def eq(self, a, b):\n",
    "        if a.tag == b.tag:\n",
    "            return self.theories[a.tag].eq(a, b)\n",
    "        else:\n",
    "            return False\n",
    "    def makeset(self): #makeset(self, tag):\n",
    "        # make a new uf?\n",
    "        # pick random tag? High or low?\n",
    "        (0, self.theories[0].makeset())\n",
    "    def union(self, a, b):\n",
    "        a,b = self.find(a), self.find(b)\n",
    "        if a == b:\n",
    "            return a\n",
    "        if a.tag == b.tag:\n",
    "            return self.theories[a.tag].union(a.val, b.val)\n",
    "        else:\n",
    "            a,b = a,b if a.tag < b.tag else b,a\n",
    "            self.uf[b] = a\n",
    "\n",
    "    def apply(self, ) # can we apply a throy specific constructor now though? Hmm. so maybe this doesn't work\n",
    "    # maybe can flue together by making a new identifier in the right\n",
    "\n",
    "class ProductTheory(Theory):\n",
    "    # this does work. Doesn't seem that useful? \n",
    "\n",
    "class ArrowTheory(Theory)\n",
    "\n",
    "class PrimtiveUF(Thoery):\n",
    "\n",
    "class Primitive(Theory): ...\n",
    "    def eq(self, a, b):\n",
    "        return a == b\n",
    "    def find(self, a):\n",
    "        return a\n",
    "    def union(self, a, b):\n",
    "        if a != b:\n",
    "            raise Exception(\"Union on unequal elements\")\n",
    "    def makeset(self):\n",
    "        raise Exception(\"makeset on primitive theory\")\n",
    "\n",
    "class Uninterp(Theory):\n",
    "    uf\n",
    "\n",
    "SumTheory(Uninterp, Primitive)\n",
    "    \n",
    "\n",
    "\n",
    "class GluedTheory # maybe only say we're glued on saome precespecified part of the domain, the uf part for example. some kind of pullbacky thing?\n",
    "class ISOTheory # maybe theories are glued by prespecified isomorphisms so that we can also move between them when need be. Glued by an isomorph group union find?\n",
    "\n",
    "class SMTTheory\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# drafts\n",
    "It is possible to encode reasoning about familiar mathematical objects into this framework. \n",
    "e operations like addition and multiplication. These identities are difficult to orient once and for all as some optimizations are only unlocked once the appropriate terms have bubbled together.\n",
    "\n",
    "It is also possible to encode Gaussian elimination into egraph rewrite rules, at extreme overhead compared to a custom implementation. A natural question arises: How can one bake in these special mechanisms while retaining the generality and flexibility of the egraph?\n",
    "\n",
    "Destructive rewrite rules\n",
    "Whereas the inidividual rules manipulating de Bruijn indices, or assocaitgivty and commutativity would lead to perhaps undesirble administrative junk in the egraph, this technique enables big leaps to be taken.\n",
    "Maybe aextract and simn,poify is most like \n",
    "\n",
    "On the downside, there intermediate steps are sometimes not actually junk and enable rewrites.\n",
    "\n",
    "As an example, consider aossicaitivyt and commutativity. These axioms alone will result in (n ) enodes and eclasses.\n",
    "\n",
    "$ x_0 \\odot x_1 \\odot x_2 \\odot ... \\odot x_n$ and running the rewrite rules to saturation will result in \n",
    "\n",
    " Nelson-Oppen Combinations and the Shostak procedure.\n",
    "\n",
    " The success of Satisfiability Modulo Theories and the expressivity it offers in terms of built in theories leads one to wonder how to bake in reasoning. Indeed the\n",
    "Equality saturation as used by egg is not a backtracking search.\n",
    "\n",
    "The motivating schematic equations is EMT = SMT - SAT.\n",
    "\n",
    "If one has a has a backtrackable egraph implementation, one may choose to treat some constraints lazily, taking on the optimistic constraint until it is proven contradictory. This technique can be viewed as synthesizing a set of constraints sufficient to achieve.\n",
    "\n",
    "Example:\n",
    "\n",
    "Two designs for dealing with constraints are either eaergly discharge the constraint obligation before asserting the rule or the allow the rule to fire but note the equality as happening under as lazily discharged constraint. The latter requires a notion of context.\n",
    "\n",
    "SMT solvers support the generation of new constants and can have learned equalities asserted into them.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "isomorphism between AC f sort and AC g sort.... opaquifying  to(from(a)) functions. But maybe we could bake these in too?\n",
    "\n",
    "... |= M |= Egraph |= eqs\n",
    "The eids remains abstract entities. How are we supposed to think about `concrete_int | eid`? Well, this is an entitite that can be further refined. \n",
    "\n",
    "|= polynomial_eid , poly_eqs |= eqs\n",
    "\n",
    "\n",
    "\n",
    "I feel like i'm almost saying nothing. But that's good right?\n",
    "\n",
    "Ones man syntax is another mans semantics. Maybe partially refined terms?\n",
    "\n",
    "14 |= f(3) |= f(x)\n",
    "\n",
    "partial evaluation. Maybe a seed of an idea to refinement egraphs. The egraphs themselves are in refinement, which models definedness.\n",
    "\n",
    "Primitives -> Containers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approaches to Egraphs Modulo Theories\n",
    "\n",
    "Egraphs and equality saturation are a useful approach to equational reasoning and optimization. \n",
    "Egraph rewriting is a methodology for optimizing expression. A known problem is that some of the rewrite rules explode the egraph in size for what feels like common administrative manipulations like a + b = b + a or a * (b * c) = (a * b) * c.\n",
    "\n",
    "Pedagogcial notions\n",
    "- Terms Modulo Theories\n",
    "- Hash Consing Modulo Theories\n",
    "\n",
    "Actual approaches\n",
    "- Extract, simplify, and reinsert\n",
    "- SMT side-carring. Constraints\n",
    "- Generalized Enodes\n",
    "- Structured Semantic Eids. Generalized Union Finds and Eids\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draft\n",
    "\n",
    "\n",
    "Baking it in: Steps towards egraphs modulo theories.\n",
    "\n",
    "What is \"modulo theories\"\n",
    "\n",
    "SMT solvers - sat and egraph glue together diasprarte bits\n",
    "EMT = SMT - SAT\n",
    "\n",
    "\n",
    "eid + ctx as structured eid?\n",
    "\n",
    "\n",
    "There are two tacts\n",
    "\n",
    "- Generalize Enodes\n",
    "- Generalize Union Find (which implies generalizing eids)\n",
    "\n",
    "A simplier starting problem is Terms Modulo Theories\n",
    "\n",
    "- canonizers\n",
    "- Baking in unorderedness\n",
    "- Hash Consing. Interned vs uninterned. The hash cons itself is a term bank\n",
    "\n",
    "This is the same discussion as generalized enodes. With the caveat that smart constructors can deep match.\n",
    "\n",
    "\n",
    "Do the theory subterms go in the term bank or don't they? If they do, are we saving anything?\n",
    "\n",
    "Two other tacts:\n",
    "- extract and assert\n",
    "- SMT piggy back\n",
    "\n",
    "\n",
    "You can turn a set of ground equations into an egraph.\n",
    "And also extract them. Equations extraction. egraph(equations(e)) = e? If we don't included the term bank concept, no.\n",
    "a = a can be used to enocde term bank, but I think it is conceptually interesting to make it seperate.\n",
    "E + T = Egraph\n",
    "extract_equations(egraph(eqs)) give a canonical form perhaps\n",
    "\n",
    "Group union find is a generalized union find.\n",
    "Idempotent theory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Introduction\n",
    "Simplification is the replacing of a complex thing with a better or simpler thing.\n",
    "\n",
    "A rich framework to use to declare a simplification system is that of equational logic, where equivalences are specified as quantified equational axioms over terms.\n",
    "\n",
    "\n",
    "\n",
    "These axioms and a signature implicitly define sets of equivalent terms. All the possible terms can be generated from the signature. Any two terms that match the left hand and right hand side of an equation.\n",
    "\n",
    "Simplification is selecting a nicer term out of the set of equivalent ones.\n",
    "\n",
    "\n",
    "A slightly different question is to start from one particular term and produce the set of equivalent terms. One can maintain a growing set of terms and increase it by applying the equational axioms to any term in the set at any subposition.\n",
    "\n",
    "\n",
    "Egraphs are a data structure for compactly representing a set of ground equalities that supplies a fast check if equality between two terms are implied by the ground axioms. In a different mode, Egraphs can also be used to efficiently enumerate the set of equivalent terms for a given term. Via a dynamic programming approach, one can also extract a good equivalent term.\n",
    "\n",
    "Ground terms by and large are easier to deal with and more well behaved that non-ground terms. Non ground terms can be seen as an optimization or abstract domain for finitely and effectively representing certain possibly infinite sets of terms, but this power does not come entirely for free.\n",
    "\n",
    "For example, it is the case that for some equational systems, the equalities can be oriented into rewrite rules with good properties. When the rules are confluent and terminating, running then on a ground term will produce a unique normal form. In this manner, these rules can be used for equivalence checking of ground terms modulo this equational theory.\n",
    "\n",
    "For non-ground terms, the situation is more complicated. A non-ground term represents in a sense a possibly infinite set of terms. It is not guaranteed that all of these terms are in the same equivalence class under the rules R. Therefore, there must be some kind of splitting possible. The only way for a rule to split the set represented by the term is to have a non trivial overlap with the non-ground function symbols of the term. This is called narrowing.\n",
    "\n",
    "Q: Does narrowing terminate? Yes, maybe it does. The term ordering does not admit an infinite descending chain of even non ground terms. And it has substitiution property f(f(X)) -> X.   consider narrowing c(f(X), X). This cycles.  https://www.sciencedirect.com/science/article/pii/S0304397509005246? termination of narrowing revisisted\n",
    "\n",
    "\n",
    "\n",
    "Another related approach to solving an equational system is Knuth bendix completion. Knuth Bendix completion when it succeeds produces a conlfuent and terminating rewrite systems out of an equational system. Knuth bendix completion of a ground equational system will succeed. The resulting ground rewrite system represents the set of ground equalities that produced it, but also gives a fast way to check if two ground terms are equal.\n",
    "\n",
    "\n",
    "\n",
    "- Given two terms, are they equal?\n",
    "- Given a term, generate all equal terms\n",
    "- Given a term generate a best equal term\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Egraph techniques from the get go have had a pragmatic bent. It is convenient from an implementation perspective to use ground terms. It is an easy to implement reasonable search heuristic to seek rewrite coming out of the cloud resulting from the initial term.\n",
    "From this perspective, the completeness or incompleteness of some new variation of egraph rewriting compared to the stack version does not seem that interesting.\n",
    "\n",
    "\n",
    "Some theories, important ones being associativity and commutativity, feel structural. They are difficult to orient, one direction of commutativty or assocation is arguably not \"simpler\" than the other, it being more of a marginal tie breaking case. These axioms are also related to commonly avaiable and studied data structures such as sets, multisets, and permutatations.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Ground + A Does not work\n",
    "\n",
    "\n",
    "An important no-go counterexample is the following:\n",
    "\n",
    "String rewriting is in general a nasty thing. It is a sufficiently flexible framework to encode the execution of a turing machine (fairly directly by encoding the tape and the head). If one could somehow generate a terminating normalization procedure for any string equation system, then one could solve the halting problem by asking if the halt state and the initial state are equal. \n",
    "\n",
    "String rewriting can be encoded in term rewriting. A string can be represented as a binary concatenation on constants obeying associativity. The particular equations of a string rewrite system can be represented by ground equations. The only non ground rule needed is the associativity axiom.\n",
    "\n",
    "A different encoding that in a sense bakes in a choice of the associativity (a more \"list-like\" representation) turns each character into a unary function symbol. This is sort of the fused combo of the character constants and concatenation  `a(X) = (a0 . X)`. To faithfully ecnode the string rewriting system, all the rules are now not ground, allowing an arbitrary suffix or prefix.\n",
    "\n",
    "\n",
    "\n",
    "The ground Ground Knuth Bendix is terminating and will succeed. The reason is that the overlap relation between ground terms is the subterm relationship. A critical pair will be generated, but both terms in it will always be bounded from above by a term already existing in the system. There are only finitely many terms less than a given term in a particular well founded (... is this true? No. It is not. Maybe if I require it to be a substituion ordering?) What is produced is less than the original term in a total well founded order. Because the ordering is well founded this cannot go on forever. Because the order is total, all critical pairs produced can be oriented.\n",
    "\n",
    "Regular Knuth bendix on non ground terms does not always succeed. Critical pairs can be generated which are not orientable because the pairs are unordered with respect to the necessarily partial term ordering on non ground terms. Ground terms orders can be total.\n",
    "\n",
    "It is the totality of the term ordering which is crucial here.\n",
    "\n",
    "Ground knuth bendix + the single associativity axiom will not always succeed. How does it fail?\n",
    "\n",
    "\n",
    "Supposed we have a a set of equations which can be separated into a ground part G and and non ground part E. Using ordinary completion by assumtion let us say we can convert this system to G;R by merely choosing to ingore Knuth Bendix rules that manipulate G. Now we may do the same for G ,which will succeed.\n",
    "Now the only critical pairs are between members of R and G.\n",
    "\n",
    "When a non ground term (in R) makes a critical pair with one in G, it may do it by being a true subterm or by grounding one or more of the variables in an overlap.\n",
    "This critical pair may not be fully grounded.\n",
    "\n",
    "Ground Knuth Bendix + AC will succeed. Why is this the case? No it cannot. C cannot be oriented. But the system can be _ground completed_. A system is ground confluent if any ground term can be joined and ground terminating if .\n",
    "\n",
    "An ordered rewriting system allows ordering constraints between the variables.\n",
    "f(x,y) | f(x,y) > f(y,x) --> f(y,x)  # this is actually an infinite family of ground rules (?) But so was it always? No there's some kind of game being play about when the terms filling the variables get to be chosen. Static vs dynamic.\n",
    "push inequalities down (we never hit variable rules. x,y are metavariables)\n",
    "f(x,y) | x > y -> f(y,x).\n",
    "These orderings may prune some critical pairs. We need to push constraints down to the variables by symbolically executing the term orderings.\n",
    "Overlaps between these rules and ground rules are easy to prune.\n",
    "```\n",
    "f(x,y) > p(x,y) \n",
    "---------------   \n",
    "```\n",
    "\n",
    "AC is probably ground confluent by left associng and sorting. It is not so easy to see this directly from the definition.\n",
    "ground + AC is less clear\n",
    "\n",
    "We can split any rule without ordering constraints into multiple rules of the possible ordering constraints. This is because we assume the ground ordering is total.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "For syntactic pattern matching of a single pattern against a term, anything except the top down approach feels ludicrous.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "EIds as models\n",
    "\n",
    "A term model is considered sometimes to be just syntax is disguise.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Baking in theories in term rewriting occurs in the form of E-unification or you can move off of the structural definitions of terms.\n",
    "\n",
    "Unfailing completion is a theorem proving method. It is parametrized on a notion of E used for unification.\n",
    "E-unficiation can be parametrized on a set of rewrite rules for the theory.\n",
    "\n",
    "\n",
    "\n",
    "A different intution is \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unifying Enodes and Containers\n",
    "\n",
    "A basic Enodes are an ordered record of eclass ids inputs arguments, a function symbol, and an output eclass id.\n",
    "\n",
    "Extensions of the egraph have also allows Enodes to have arguments that are primitive datatypes like ground integers, strings, bools or rationals.\n",
    "\n",
    "## Hash consing modulo theories\n",
    "https://www.philipzucker.com/hashing-modulo/\n",
    "\n",
    "A simpler case to consider is that of interning / hash consing terms. A useful features of hash consing is that equality of terms can be determined by a constant time pointer equality check.\n",
    "\n",
    "To say that a binary symbol is commutative is to say that the order of the children does not matter. It is a simple matter to normalize a binary symbol of this kind by sorting it's arguments. This puts the term into a canonical form.\n",
    "\n",
    "Given a confluent and terminating rewrite rule set, one can hash cons a term using smart constructors which check to see if the to be constructed term has rewrite rules applicable to it. As an optimization, these intermediate states do not necessarily have to be hash consed. There is an invariant of a hash consed tree that it is fully normalized with respect ot the rules. Placing a new function symbol at the head via a smart constructor may unlock some new rewrite rule at the head.\n",
    "\n",
    "Sorting the arguments can be seen as applying ordered rewriting. The commutativity axiom cannot ever be ordered because it is too symmetric. One can wait until \"runtime\" to apply the rule using ordered rewriting.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "For associative functions, it is very natural to allow multi-arity function symbols and flatten the arguments into a canonical list. When the smaert constructor peeks at the arguments, it can flatten.\n",
    "\n",
    "A related but different approach is to maintain the binary function symbols but recursively do a list append.\n",
    "\n",
    "The arguments of a function symbol can be\n",
    "\n",
    "- multisets\n",
    "- sets\n",
    "- polynomials\n",
    "\n",
    "\n",
    "- sorting\n",
    "- flattening\n",
    "- deduplicating\n",
    "- polynomials\n",
    "\n",
    "### Bottom up E-matching\n",
    "\n",
    "Suppose we have some rules that are less well behaved we keep off separate from our built in ones.\n",
    "\n",
    "We could take an equational theory and one by one add them to a Knuth Bendix procedure according to some heuristic. When or if the procedure fails, we can take the best subset of the equations that we can complete and leave the rest for later.\n",
    "Or perhaps do Knuth Bendix, stop at some intermediate state `E;R` where E has all the critical pairs of R. We then treat R intrinsically, and treat E externally.\n",
    "\n",
    "E-matching is finding a substitution such that the instantiated pattern is equal to a term modulo the equational theory. \n",
    "\n",
    "A top down strategy involves narrowing the pattern against the rules. The ground term is presumed fully normalized so the rules do not apply in the forward direction.\n",
    "\n",
    "Q: What is the stopping condition?\n",
    "\n",
    "A bottom up strategy involves expanding the ground term using the reversed rules. \n",
    "\n",
    "\n",
    "A matching problem starts with a pattern and a term supposed equal. This is a natural place to start top down matching.\n",
    "\n",
    "In E-matching, there is not generic a priori guarantee that the variables will be bound to a subterm of the target. They may not even be bound to a term in an equivalence class of the subterms of the target. The terms must be generated by the rules and searched over somehow. This sucks.\n",
    "\n",
    "Pattern matching is nice because the procedure is basically obvious. Pattern matching is not the most general thing we can conceive of. The pattern matching problem is an intermediary that is mathematically statable and efficiently implementable. If it was missing either of there characters, it would not be as prominent an idea.\n",
    "\n",
    "Reasonable mechanisms can probably eventually be reconciled with a logical view. Case is point is negation as failure in logic programming. This is an extremely antural operational move in Prolog, but reconciling it with a logical perspective took a great many years of work.\n",
    "\n",
    "Is the only logic worth talking about something close to classical first order logic? Maybe.\n",
    "\n",
    "\n",
    "A different matching problems starts with a pattern and a bank of terms with which one might wish to bind the variables. \n",
    "\n",
    "I believe it is the case in general that one may encounter an infinite number of equivalence classes as subterms. This appears to be asking too much.\n",
    "\n",
    "As an example, consider baking in linear expressions subject to a background theory of linear equalities. Anything on a line/hyperplane would also be a possible term.\n",
    "We could perhaps cook up a way of describing \n",
    "\n",
    "\n",
    "Bottom up ematching is a particular strategy for relational ematching. \n",
    "In it's most brute force form, one scans over the databank of terms for every variable appearing in the pattern.\n",
    "It can be optimized by pruning the possible bindings of variables by first doing a pass that finds an overapproximation of the sets the variables can bind to. This is akin to a WCOJ.\n",
    "\n",
    "\n",
    "## What patterns are easily implementable\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Replacing Union Finds with Other Canonizers\n",
    "\n",
    "The union find is a data structure for solving atomic equations. Given an atom, it can return a canonical atom for it's equivalence class.\n",
    "\n",
    "There are other canonizer producing algorithms.\n",
    "\n",
    "Atomic Knuth Bendix completion solves the same problem. The rewrite rules can be viewed as a manifestation of the union find. One difference is that Knuth Bendix is parametrized on a a term ordering. The union find can be done in a style with a determinstic ordering, or it can be done in a way that depends on the order of the asserted unions. This freedom enables the union find to keep itself flat and achieve a better asymptotic complexity.\n",
    "\n",
    "Atomic multiset completion also  \n",
    "\n",
    "Taking the row echelon form of a set of linear equations builds a canonizer.\n",
    "\n",
    "Grobner bases form a canonizer for polynomial expressions under the assumption polynomial equalities.\n",
    "\n",
    "You can also use an entire knuth bendix procedure as your canonizer producer. In this case it may not terminate though.\n",
    "\n",
    "\n",
    "By enriching the union find to one of these other canonizers, we can push some of the work that would be done by the egraph or rules into the union find.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Some function symbols can be chosen to belong to enodes, and some to the structured eids. Baked in rules can only talk about the symbols in the strucutred eids and the other symbols are considered foreign.\n",
    "\n",
    "At one extreme we have the egraph with it's atomic integer ids.\n",
    "\n",
    "At the other extreme, the \n",
    "\n",
    "\n",
    "# Extract and Rewrite\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=udORacqkExg&ab_channel=ACMSIGPLAN  WiP: Labeled Union-Find for Constraint Factorization\n",
    "\n",
    "\n",
    "Completion without failure  https://www.cs.tau.ac.il/~nachum/papers/unfail-paper.pdf\n",
    "\n",
    "Le Chenadec - Canonical Forms in Finitely Presented Algebras . Can't find it\n",
    "\n",
    "\n",
    "normalized completion\n",
    "https://drops.dagstuhl.de/storage/00lipics/lipics-vol021-rta2013/LIPIcs.RTA.2013.319/LIPIcs.RTA.2013.319.pdf\n",
    "https://www.sciencedirect.com/science/article/pii/S0747717196900115  Normalized Rewriting: an Alternative to Rewriting modulo a Set of Equations\n",
    "\n",
    "https://dl.acm.org/doi/pdf/10.1145/800017.800519 jouannaud kirchner - completion of a set of rules modlo a set of equations.\n",
    "\n",
    "https://wiki.bordeaux.inria.fr/Helene-Kirchner/lib/exe/fetch.php?media=wiki:rsp.pdf  rewriting solving proving. Quite a table of contents.\n",
    "\n",
    "t < s as an inference rule system\n",
    "\n",
    "\n",
    "if finitariness is good, then actually strongly simplifying R is bad.\n",
    "\n",
    "\n",
    "Build my own smt solver? My own outer Z3 solver.\n",
    "Make a command line form that takes in smtlib (via z3's parser)\n",
    "\n",
    "assoc has 3 variables.\n",
    "A + C is ok though.\n",
    "Bubble sort completion (a + (b + c)) -> (b (+ a + c))\n",
    "\n",
    "You can N-theory it by statically choosing a depth N that rules have been theorified.\n",
    "\n",
    "The ordering can never be stable.\n",
    "\n",
    "\n",
    "Group union find. g x = g2 x. Overlap can be achieved by inverted the group element.\n",
    "monoid union find is fine. Or string/semigroup action (no unit). This maps onto working always at the end of the string. And is ground equations basically. f(ff(endsymbols))\n",
    "\n",
    "\n",
    "Completeness\n",
    "herbrand https://www.youtube.com/watch?v=bdmigQsf_uY&ab_channel=LawrencePaulson\n",
    "https://math.stackexchange.com/questions/2468277/intuition-about-herbrand-models\n",
    "\n",
    "https://terrytao.wordpress.com/2009/04/10/the-completeness-and-compactness-theorems-of-first-order-logic/\n",
    "\n",
    "\n",
    "Making complete refutation system out of egglog. You can collect all needed generations. Enough to simulate the needed\n",
    "f(x) --> \n",
    "\n",
    "\n",
    "Set rewriting for idempotent. Set term orders? Set rewriting is very close or identities with ground ordered resolution. But set rewriting doesn't have removal replacement semantics. It has monotonic semantics.\n",
    "\n",
    "Knuth bendix with constraints is an interesting paradigm. Overlap is normal overlap + combined contraints being satisfiable / consistent.\n",
    "\n",
    "ground + C is maybe the simplest one. I basically ocnsidered the container version. But if we push C into the eid, we make eids C symbols  + opqaue constants. \n",
    "\n",
    "ground + I. everything is either opqaue a, or f(a). It might be better to rewrite _to_ `b -> f(a)` because absortbing is better than not.\n",
    "\n",
    "Multiple I symbosls. f(g(f(a))). It is excessive to do this. As soon as we go to \n",
    "\n",
    "We can do top down ematching by delegating to a theory specific matching for the eid when the pattern hits symbols that belong to the eids.\n",
    "\n",
    "You put everything into the eid that you'd need to identify\n",
    "\n",
    "stratified egraphs - consider a structured eid who's theory is _also_ ground equations. The structured eid is now a term who only contains function symbols from lower egraphs or atomics eids. Canonization is by extraction from the lower theory.\n",
    "\n",
    "G1 + G2 is obviosuly confluent and terminating even though a modular problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- relational AC-matching\n",
    "- https://www.philipzucker.com/egraph2024_talk_done/\n",
    "- https://www.philipzucker.com/linear_grobner_egraph/\n",
    "- https://www.philipzucker.com/coegraph/\n",
    "- https://www.philipzucker.com/string_knuth/\n",
    "- https://www.philipzucker.com/smart_constructor_aegraph/\n",
    "- https://www.philipzucker.com/multiset_rw/\n",
    "- https://www.philipzucker.com/bottom_up/ normalizing containers\n",
    "\n",
    "\n",
    "a theory is ground exendable if it remains completeable with ground equations.\n",
    "\n",
    "Associativity is not groud extendable.\n",
    "\n",
    "\n",
    "Enodes ~ syntax\n",
    "eids ~ semantics\n",
    "\n",
    "When you ask z3 for a model, it returns numerical semantic values.\n",
    "\n",
    "Equality saturation starts as pure syntactic equality and gradually moves over closer to semantic equality (which is often never reaches).\n",
    "\n",
    "There are two main approaches I've been trying (or that I'm aware of):\n",
    "\n",
    "- Containers as smarter enodes\n",
    "- structured / semantic eids\n",
    "\n",
    "The enode structure I used is a function symbol + an ordered list of arguments. This is already kind of smart compared to a function symbol + a binary tuple of args. This tuple form can be interesting if you want to talk about partial application.\n",
    "\n",
    "\n",
    "Destructive rewriting as a theory.\n",
    "If you have a good rewrite theory over a system (terminating and confluent), you can already effectively quotient the syntax trees by that notion of equivalence be greedily applying the rules.\n",
    "\n",
    "```\n",
    "add(succ(X), Y) -> succ(add(X,Y))\n",
    "add(Z, Y) -> Y\n",
    "```\n",
    "\n",
    "\n",
    "Combination problems\n",
    "https://www.sciencedirect.com/science/article/pii/S0890540101931189 Deciding the Word Problem in the Union of Equational Theories\n",
    "\n",
    "Canonization for disjoint unions of theories Sava Krstic  Sylvain Conchon\n",
    "\n",
    "Disjoint signatures\n",
    "Termination is not modular\n",
    "Confluence is modular\n",
    "Normalizing is modular\n",
    "\n",
    "Ground rewrite rules / equalities are quite special.\n",
    "\n",
    "The following no go counterexample crushes most hopes of generic good results.\n",
    "\n",
    "Associativity + ground term equations = string equality and is undecidable.\n",
    "\n",
    "You can build a normalizing rewrite rule system for associativity by associating to the right.\n",
    "`(x . y) . z -> x . (y . z)`\n",
    "\n",
    "Ok but actually string rewrite systems are rules of form (yadayda . X) -> (something . X). They are not ground. Yeah but associatvity can make them non ground\n",
    "A ground _term_ equation corresponds more directly to the string rewrite that only applies at the end of the string. \n",
    "\n",
    "For disjoint signatures, there doesn't seem like their should be a problem. Why were eids integers in the first place? Who cares? The pieces of the other signature completely block rules from applying.\n",
    "\n",
    "\n",
    "Pick a ground total termination ordering compatible with the rewrite relation.\n",
    "COnsider the particular stratgey: orient the built in rules, All confluence pairs are now redundant by assumption.\n",
    "Start orienting the ground equations. simplify reduces by the built ins. This is \"putting into the egraph\".\n",
    "Any symbol not part of built ins can have a generated guy.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Ok, so we want non-disjoint signatures.\n",
    "\n",
    "\n",
    "What terms are \"in\" the egraph:\n",
    "1. A generative point of view (top down production of terms). Given an eid, we can build. The view is related to top down ematching\n",
    "2. An acceptor point of view. If we insert of term bottom up into the egraph, does it change it or leave it the same? This view is related to bottom up ematching.\n",
    "\n",
    "We can consider maybe adding new function symbols over \n",
    "\n",
    "\n",
    "Can we put eids and enodes on the same footing? If I had 3 theories I wanted to bake in. Does baked in go into eids? Or 2 notions of eids?\n",
    "\n",
    "https://egraphs.zulipchat.com/#narrow/channel/328972-general/topic/Linear.20and.20Polynomial.20Equations/near/477290392\n",
    "\n",
    "\n",
    "https://egraphs.zulipchat.com/#narrow/channel/424128-egg.2Fgeneral/topic/Translating.20between.20languages.20in.20an.20egraph/near/499111241 translation between mutiple languages slows stuff down.\n",
    "Related to refinement egraph ideas?\n",
    "\n",
    "\n",
    "|           |   UF    | Theory |\n",
    "| ---       |  ---    |        |\n",
    "| Sorts     | \n",
    "| Enodes    |\n",
    "| Eclasses  |\n",
    "| Functions |\n",
    "| Rules     |   \n",
    "\n",
    "\n",
    "Egraph modulo theories:\n",
    "What is a theory?\n",
    "\n",
    "https://pldi25.sigplan.org/home/egraphs-2025\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SortRef():\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.uf = []\n",
    "        # self.utree # to maintain eclasses\n",
    "        # self.reason # to maintain provencnance / proof\n",
    "    def find(self, i): # canonize\n",
    "        while self.uf[i] != i:\n",
    "            i = self.uf[i]\n",
    "        return i\n",
    "    def union(self, i, j):\n",
    "        i = self.find(i)\n",
    "        j = self.find(j)\n",
    "        if i != j:\n",
    "            self.uf[i] = j\n",
    "    def make(self): # Const?\n",
    "        self.uf.append(len(self.uf))\n",
    "        return len(self.uf)-1\n",
    "        \n",
    "\n",
    "class FuncDeclRef(): # MergeDict\n",
    "    def __init__(self, name, *sorts):\n",
    "        self.data = {}\n",
    "        self.sorts = sorts\n",
    "    def __getitem__(self, args):\n",
    "        assert len(args) == len(self.sorts) - 1\n",
    "        return self.data[args]\n",
    "    def __call__(self, *args):\n",
    "        assert len(args) == len(self.sorts) - 1\n",
    "        res = self.data.get(args)\n",
    "        if res is None:\n",
    "            res = self.default()\n",
    "            self.data[args] = res\n",
    "            return res\n",
    "        else:\n",
    "            return res\n",
    "    def __setitem__(self, key, value):\n",
    "        assert len(key) == len(self.sorts) - 1\n",
    "        res = self.data.get(key)\n",
    "        if res is None:\n",
    "            self.data[key] = value\n",
    "        else:\n",
    "            self.data[key] = self.merge(res, value)\n",
    "    def default(self):\n",
    "        return self.sorts[-1].make()\n",
    "    def merge(self,i,j):\n",
    "        return self.sorts[-1].union(i,j)\n",
    "    def rebuild(self):\n",
    "        fnew = FuncDeclRef(self.name, *self.sorts)\n",
    "        for k,v in self.data.items():\n",
    "            newk = tuple(sort.canon(k) for k,sort in zip(k,self.sorts[:-1]))\n",
    "            fnew[newk] = self.sorts[-1].canon(v)\n",
    "        return fnew\n",
    " \n",
    "Int = SortRef(\"Int\")\n",
    "add = FuncDeclRef(\"add\", Int, Int, Int)\n",
    "x = FuncDeclRef(\"x\", Int)()\n",
    "add(x,x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MergeDict(dict):\n",
    "    def __setitem__(self, key, value):\n",
    "        res = self.get(key)\n",
    "        if res is None:\n",
    "            super().__setitem__(key, value)\n",
    "        else:\n",
    "            super().__setitem__(key, self.merge(res, value))\n",
    "    def merge(self, i, j):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "class MinDict(MergeDict):\n",
    "    def merge(self, i, j):\n",
    "        return min(i, j)\n",
    "\n",
    "class MaxDict(MergeDict):\n",
    "    def merge(self, i, j):\n",
    "        return max(i, j)\n",
    "\n",
    "d = MinDict()\n",
    "d[1] = 2\n",
    "d[1] = 3\n",
    "assert d[1] == 2\n",
    "d[1] = 1\n",
    "assert d[1] == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSFuncDeclRef(FuncDeclRef):\n",
    "    # multiset funcdecl ref. \n",
    "    # Putting the \"ACness\" into the functions\n",
    "    # Containers ~ enodes\n",
    "    def rebuild(self):\n",
    "        fnew = MSFuncDeclRef(self.name, *self.sorts)\n",
    "        for k,v in self.data.items():\n",
    "            fnew[k] = v\n",
    "        return fnew\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACSort():\n",
    "    # putting the \"ACness\" in the sort.\n",
    "    # structured eids which are multisets\n",
    "    # containers ~ eids\n",
    "    def __init__(self, name, inner):\n",
    "        self.name = name\n",
    "        self.inner = inner\n",
    "    def rebuild(self, xs : tuple): # rebuild / re-canonize\n",
    "        return tuple(sorted([self.inner.rebuild(x) for x in xs]))\n",
    "\n",
    "class ACISort():\n",
    "    # ACI ~ sets\n",
    "    def __init__(self, name, inner):\n",
    "        self.name = name\n",
    "        self.inner = inner\n",
    "    def rebuild(self, xs : tuple): # rebuild / re-canonize\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A notion of \"kinds\". Sorts may have different structured ids. The description or data of thse may lie in the Kind data strucctures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UninterpretedKind():\n",
    "    def __init__(self, name):\n",
    "        self.uf = []\n",
    "    def make(self):\n",
    "        self.uf.append(len(self.uf))\n",
    "        return len(self.uf)-1\n",
    "    def union(self, i, j):\n",
    "        return i\n",
    "    def canon(self, i):\n",
    "        return i\n",
    "    def rebuild(self, i):\n",
    "        return self.find(i)\n",
    "\n",
    "\n",
    "class SortRef():\n",
    "    def __init__(self, name, kind=None):\n",
    "        self.name = name\n",
    "        if kind is None:\n",
    "            kind = UninterpretedKind()\n",
    "        else:\n",
    "            self.kind = kind\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could easily have multiple notions of equality.\n",
    "They don't necessarily have to have much to do with each other. But we can't normalize across them except at the commonality.\n",
    "There is a truly baked in equality that can be destructive.\n",
    "\n",
    "We could only canonize when they agree they canonize. Or maintain a fixpoint of the canonization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtomicEq():\n",
    "    def __init__(self, sort):\n",
    "        self.sort = sort\n",
    "        self.uf = []\n",
    "    def canon(self, i):\n",
    "        pass\n",
    "    def rebuild():\n",
    "\n",
    "class LinearEq():\n",
    "    def __init__(self, sort):\n",
    "        self.sort = sort\n",
    "        self.uf = []\n",
    "    def canon(self, i):\n",
    "        pass\n",
    "    def rebuild():\n",
    "        pass # do gauss elim?\n",
    "\n",
    "class RecursiveEq(): # FixEq\n",
    "    pass # todo\n",
    "\n",
    "\n",
    "class GroupActEq():\n",
    "    pass # Can have group actions on the side of the UF\n",
    "\n",
    "class StringEq(): ...\n",
    "\n",
    "class FixEq(): ...\n",
    "\n",
    "class UnionEq():\n",
    "    def __init__():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are quotienting by different equalities.\n",
    "Having them play nice with each other is a theory combination problem.\n",
    "\n",
    "Partitions ~ equaiovlanece relations.\n",
    "Finest partition or corasest partition that contains the equivalence relations.\n",
    "Equivalence relations form a lattice.\n",
    "https://en.wikipedia.org/wiki/Equivalence_relation\n",
    "\n",
    "We can bounce around two equivalence relations is we have a notion of well founded ordering we are following and get the canonizer for the combo (the coarser partition with more equalities).\n",
    "\n",
    "I don't know how we'd get a canonizer for the intersection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuotientSort():\n",
    "    def __init__(self, sort, eq):\n",
    "        self.sort = sort\n",
    "        self.eq = eq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueKind():\n",
    "    # literals like literal integers\n",
    "    def rebuild(self, i):\n",
    "        return i\n",
    "    \n",
    "# This makes sense for Int where we have abstract Int expressions, but also sometimes literal int values. \"Lifting\" is somewhat awkward.\n",
    "# Perhaps related to my notion of observation table for coegraphs. A ground value is a very powerful observation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnionKind(): # union or disjoint union / tagged union?\n",
    "    def __init__(self, *kinds):\n",
    "        self.kinds = kinds\n",
    "    def rebuild(self, i):\n",
    "        tag, inner = i\n",
    "        return (tag, self.kinds[tag].rebuild(inner))\n",
    "    def make(self, tag): # Hmm. Make takes stuff now...\n",
    "        return (tag, self.kinds[tag].make())\n",
    "\n",
    "class IntersectionKind():\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the ground rewrite system perspective, what are we doing?\n",
    "\n",
    "A specialized notion of overlap\n",
    "a compositional form of term ordering. Making a term ordering from a term ordering of the contained elements.\n",
    "a notion of pattern matching and replacement / removal\n",
    "\n",
    "\n",
    "\n",
    "TermKind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ACDict():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ExprRef():\n",
    "    def __init__(self, id_, sort):\n",
    "        self.sort = sort\n",
    "        self.id_ = id_\n",
    "    def __eq__(self, other):\n",
    "        assert self.sort is other.sort\n",
    "        self.sort.union(self, other)\n",
    "    def eq(self, other):\n",
    "        assert self.sort is other.sort\n",
    "        return self.sort.find(self.id_) == self.sort.find(other.id_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# herbrand\n",
    "\n",
    "Why do the first order sentences have to be quantifier free to apply propositional logic theorems?\n",
    "\n",
    "https://mathweb.ucsd.edu/~sbuss/ResearchWeb/herbrandtheorem/paper.pdf buss hebrand theorem\n",
    "\n",
    "Graham says two forms of herbrand.\n",
    "A direct proof translation form?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eprover\n",
    "\n",
    "Run eprover to saturation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Preprocessing class: HSSSSMSSSSSNFFN.\n",
      "# Scheduled 4 strats onto 8 cores with 300 seconds (2400 total)\n",
      "# Starting new_ho_10 with 1500s (5) cores\n",
      "# Starting ho_unfolding_6 with 300s (1) cores\n",
      "# Starting sh4l with 300s (1) cores\n",
      "# Starting ehoh_best_nonlift_rwall with 300s (1) cores\n",
      "# new_ho_10 with pid 2024504 completed with status 9\n",
      "# sh4l with pid 2024506 completed with status 9\n",
      "# ehoh_best_nonlift_rwall with pid 2024507 completed with status 9\n",
      "# ho_unfolding_6 with pid 2024505 completed with status 9\n",
      "# Schedule exhausted\n",
      "# SZS status GaveUp\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from kdrag.all import *\n",
    "\n",
    "T = smt.DeclareSort(\"T\")\n",
    "x,y,z = smt.Consts(\"x y z\", T)\n",
    "add = smt.Function(\"add\", T, T, T)\n",
    "kd.notation.add.register(T,add)\n",
    "\n",
    "s = kd.solvers.EProverTHFSolver()\n",
    "s.add(smt.ForAll([x,y,z], x + ( y + z) == (x + y) + z))\n",
    "s.check()\n",
    "print(s.res.stdout.decode())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /tmp/ac.p\n"
     ]
    }
   ],
   "source": [
    "%%file /tmp/ac.p\n",
    "\n",
    "cnf(assoc, axiom, f(X,f(Y,Z)) = f(f(X,Y),Z)).\n",
    "cnf(comm, axiom, f(X,Y) = f(Y,X)).\n",
    "cnf(biz, axiom, f(a,f(b,c)) = f(c,f(a,b))).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /tmp/terms.p\n"
     ]
    }
   ],
   "source": [
    "%%file /tmp/terms.p\n",
    "\n",
    "f(c,f(q,r))\n",
    "f(f(a,c),a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Initializing proof state\n",
      "# Scanning for AC axioms\n",
      "# f is AC\n",
      "# AC handling enabled\n",
      "#\n",
      "#cnf(i_0_5, plain, (f(X1,X2)=f(X2,X1))).\n",
      "#\n",
      "#cnf(i_0_4, plain, (f(f(X1,X2),X3)=f(X1,f(X2,X3)))).\n",
      "#\n",
      "#cnf(i_0_7, plain, (f(X1,f(X2,X3))=f(X3,f(X1,X2)))).\n",
      "##\n",
      "#cnf(i_0_10, plain, (f(X2,f(X1,X3))=f(X1,f(X2,X3)))).\n",
      "######\n",
      "#cnf(i_0_27, plain, (f(X3,f(X2,X1))=f(X1,f(X2,X3)))).\n",
      "######################################################################################\n",
      "# No proof found!\n",
      "# SZS status Satisfiable\n",
      "# Processed positive unit clauses:\n",
      "cnf(i_0_4, plain, (f(f(X1,X2),X3)=f(X1,f(X2,X3)))).\n",
      "cnf(i_0_5, plain, (f(X1,X2)=f(X2,X1))).\n",
      "cnf(i_0_7, plain, (f(X1,f(X2,X3))=f(X3,f(X1,X2)))).\n",
      "cnf(i_0_10, plain, (f(X1,f(X2,X3))=f(X2,f(X1,X3)))).\n",
      "cnf(i_0_27, plain, (f(X1,f(X2,X3))=f(X3,f(X2,X1)))).\n",
      "\n",
      "# Processed negative unit clauses:\n",
      "\n",
      "# Processed non-unit clauses:\n",
      "\n",
      "# Unprocessed positive unit clauses:\n",
      "\n",
      "# Unprocessed negative unit clauses:\n",
      "\n",
      "# Unprocessed non-unit clauses:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! eprover-ho /tmp/ac.p -S #| enormalizer -t /tmp/terms.p -f -\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
