---
author: philzook58
date: 2020-12-06
layout: post
title: "EGraphs Part III "
---


- [Ideas](#ideas)
  - [Related systems](#related-systems)
  - [Harrop Clauses](#harrop-clauses)
  - [Top down evaluation](#top-down-evaluation)
  - [Constraint handling rules CHR](#constraint-handling-rules-chr)
  - [CAS](#cas)
  - [Quoting out of egraph, reflection](#quoting-out-of-egraph-reflection)
  - [Bisimulation finest partition](#bisimulation-finest-partition)
  - [Typeclasses](#typeclasses)
  - [provenance](#provenance)
  - [Semiring smenatics](#semiring-smenatics)
  - [Lambda](#lambda)
    - [Extract and do stuff](#extract-and-do-stuff)
  - [Scoped union find](#scoped-union-find)
  - [Higher order rules](#higher-order-rules)
  - [Semantics of Egraphs](#semantics-of-egraphs)
  - [/](#)
  - [NE graph](#ne-graph)
  - [Egraphs over programs. Program analyiys](#egraphs-over-programs-program-analyiys)
  - [Metatheory and EGraphs](#metatheory-and-egraphs)
    - [Bits and Bobbles](#bits-and-bobbles)
- [A More Naive EGraph](#a-more-naive-egraph)



# Ideas
Ideas:
Applications I guess
- staged egg
- higher order egglog, skolemization
- A = C - B :- A + B = C. A = Lit(1/C) * B :- Lit(C) * A = B, C != 0 (* Hmm. Recipricating C over and over probably isn't good. *)
- Coq tactic
- Dependent programming proof relevant union find
- Applications: Category theory proving, solving systems of equations, patching compiler
- Lambda terms / binding forms
- efficient encoding to souffle

- instruction matching modulo equality
- xor linked list pointer analsyis
- cos^2 + sin^2 = 1+ interval analysis. Some expression better than others. Tighter interval bounds/
- `pos(x), sqrt(x**2) -> x, neg(x), sqrt(x**2) -> -x`

- instruction matching over egraph of cfg. Maybe just a single block
- knot diagrams as braiding algebra? Prove unknot

## Related systems
See datalog notes
datafun, flix, rel
DDlog 
propagators
lvars?

## Harrop Clauses
See also higher order rules
Gensym for quantified stuff. But you can't keep gensyming wantonly. So combine with not guard so you only do gensym once.

## Top down evaluation

## Constraint handling rules CHR
A question I never had an answer for https://twitter.com/notjfmc/status/1422215450675535877?s=20&t=RyHMtBS3ySaALLC9MuGmUA . CHR afaik are a way of integrating eager rewriting into prolog https://en.wikipedia.org/wiki/Constraint_Handling_Rules (edited) 

http://www.informatik.uni-ulm.de/pm/fileadmin/pm/home/fruehwirth/constraint-handling-rules-book.html
[chr what else](https://arxiv.org/pdf/1701.02668.pdf)

compiler to sql statements. Makes sense.
Multiset rewriting?
- [A More Naive EGraph](#a-more-naive-egraph)


## CAS

Hmm. You know, I feel like egglog is very close to being able to express this concept via a rule already. If you tag terms with context
 foo(ctx2, ?x,?y) = bar(ctx2,?z)  :- foo(ctx1, ?x,?y) = bar(ctx1,?z), ctxs(ctx2), ctx1 <= ctx2  
It is wasteful to some degree. But not obviously more wasteful than just keeping a labelled graph as a disjoint set datastructure
If we can store lattices in the range of foo, it might be even less wateful

carette discussion https://julialang.zulipchat.com/#narrow/stream/236639-symbolic-programming/topic/x-x.20is.20not.20necessarily.20equal.20to.200/near/232167346
https://arxiv.org/abs/1904.02729 - specifying symbolic computation, carette and farmer


Hmmm. Is extraction some kind of quote operation? If I extract and reinsert, it does nothing. Maybe it needs to be guarded somehow. With like a metalevel annotation? foo -> :foo -> ::foo -> :::foo
a = extract(foo)

##  Quoting out of egraph, reflection

extract(foo(a,b)) = cons(:foo, :a, :b) which is a valid term to put back in.
extract = quote
This is how to extract and reduce lambda calc?

see also higher order rules

## Bisimulation finest partition
https://cstheory.stackexchange.com/questions/37177/partition-refinement-in-transition-state-systems-bisimulation-contraction partition refinment bisimulation is an iterative fixpoint algorithm. Sure we could encode transition system as relation. We need reified notion of partition (like eclass id did). I could model using bitsets in souffle. pairition(bs) <= parition(bs) :- SUBSET(bs2,bs1). Goes twoards finest partition. 

## Typeclasses

Something to think about: Applications of egglog to typeclass resolution modulo equality. https://arxiv.org/abs/2001.04301 I don't really have a feel for the relation between tabled top down and magic set transformed bottom up although there clearly is one. I guess I'm also not clear on how magic set works in egglog for that matter. I don't know how to execute egglog top down in the first place.
An example would be finding a typeclass for Vec n a where you take the assoc/comm mult and add  axioms on the size index. Presumably the equational axioms must also be associated with instance dictionaries. Hmm. Maybe this is not as straightforward as I thought. You also need instances for congruence?
There isn't a problem with ordinary bottom up egglog, it just seems very wasteful on a query driven application like this

Chalk is also a very good point. [The cahlk blog posts](https://github.com/rust-lang/chalk) exliciitly talk about specialized equality. That is intriguing. I should contact Niko Matsakis.
https://smallcultfollowing.com/babysteps/blog/2017/01/26/lowering-rust-traits-to-logic/ Prolog + euqliaty reasoning he syas he's looking for

## provenance

## Semiring smenatics
prvoenance
smemiring smenaitcs
[provenance semirings](https://dl.acm.org/doi/10.1145/1265530.1265535)
[lecture on this](https://courses.cs.washington.edu/courses/cse544/12sp/lectures/lecture18-provenance.pdf)
oh. the max plus semiring. I see.
[souffle provenance](https://arxiv.org/abs/1907.05045)
[provennce guided synthesis](https://www.cis.upenn.edu/~mhnaik/papers/popl20.pdf) - is this related to ruler?

Hmmm. Could you write linear tensor recurrences this way? atoms are still indices carries amplitudes though. How do you avoid recomputation? Collecting up inifintie loops via memoization.
Kind of a feynman diagram thing.
Base propagator list 0f u(0,0, ampl). u(0,1,amp).
Magic set. If we want to query just a single vector?
linear algerba and graph algorithms are known to be connected in this way.
Maybe you need the product semiring to also track index provenance.
The lattice character of max seems important
dynamic programming

shortest path
foo( , iter+1) :-  foo() ,iter < MAX
foo(cost) <= foo(cost1) :- cost <= cost1.

Maybe monotonic increase of probability?.. Eh. Still seems hard to mannage
[https://arxiv.org/pdf/2105.14435.pdf](convergence of datalo over pre semirings - remy papers)
https://arxiv.org/pdf/2103.06376.pdf semiring dictionraries


()
()
()

r
delta_r

r :- delta_r
r :- yada yada r
No that's not good enough. It doesn't know r is already saturated over the rules. Is ther any way to trick it? Directly accessing delta relation is a good point.


[efficient encodings of first irder horn to equation logic](https://smallbone.se/papers/horn.pdf)

[equational term graph rewriting](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.53.6428&rep=rep1&type=pdf)

I've still never tried just throwin into vampire. It has a horn clause thing right?

https://github.com/nadia-polikarpova/cyclegg

yihong https://drive.google.com/file/d/18m0_fCT21-RLxB8EedmC1MuzhAM4In6X/view
Profile driven optimization. Better scheduling?

Lvars - Guard conditions are filters

What if we rearranged the storage of analysis in egg.

[twitter egraphs for query planning?](https://twitter.com/justinjaffray/status/1501294656239329281?s=20&t=qlKx5dL5bXCILsNAtcDYfw) "like cascades but better"

parent( x : Id, p : Id)


parent(x,p) <= parent(FIND(x),FIND(p)).
parent(x,p) :- add(x,_,p).
parent(y,p) :- add(_,y,p).
 :- parent(x,$Add(x,y)), parent(y,$Add())

## Lambda 

The hash cons modulo equia version uses maps at lampba sites. But then we need to manipulate the maps at the same time we manipulate expressions.
co debruijn? See jasper

https://pavpanchekha.com/blog/egg-bindings.html brilliant. let is synatcticfied substitution. But do these actually have semantics?
This is related to unmooring of succ in de bruijn. Kiselyov finally tagless combinator conversion did something similar.

What about SPJ's points?

### Extract and do stuff
Extract lambdas, reduce, reinsert. It does seem simple. Doesn't produce as much junk.
Seek
app(lam(?x, ?b), ?y) => eval()

app(lam(?x1,?b1), q) = app(lam(?x2, ?b2), q) :- lam(?x1, ?b1) = lam(?x2, ?b2).
No, only is q fresh. forall q. This might as well be "extract and check alpha". I'm not even sure it's right.



lam(?x1, ?b1) = lam(?x2, ?b2) :- lam(?x1,  app(lam(?x2, ?b2), ?x1)), = lam(?x1, ?b1) 



HVM does jive
- graph representation is reminiscent of egraph already (biartite to eclass nodes)
- read out sounds like extraction
- victor's point about only dealing with things that can result from avalaution of lambda terms sounds like only doing egraphs that come from terms.
- 

Two seperate problems:
Alpha recovery vs substitution/normalization
For sums/intergals alpha is what you need.
sum(xfresh, ) isn't _that_ bad. You need to manually recover alpha though. Sometimes this is important. But I've been willing to make bigger composromises

Some nodes can be turned upside down by (multiported going up) by precomposing with a projection. plus(a,b)probably can't since it doesn't have a unique decomposition once we start rewriting.

Lambdas need? incremental copying?
Different "modes". LambdaVar(l), LambdaBod(l) Lambda(body) you need to tie the not with a fresh var.
Lambda(body) where body was contructed using freshv. Then set freshv = LambdaVar(Lambda(body)). Knot successfuly tied. We have an alpha equiavlant rep.
But how does it know which var is it's... Hmm. This is suspicious. Uhhh. No I guess this is ok.
But it isn't alpha equivalent. to a second construction of the same kind. So be it I guess.

parent pointers is kind of an interesting point generally. We can use parent pointers to drive up one level from a concrete term to a non concrete term (variable in one of it's other leaves) efficiently.

I feel like the dup operation makes it so that we have no sharing anymore.


Interesting for substitution. The parents pointers mean we can easily access at least the immediate context in which x occurs.


just requiring uniqueness of var spots doesn't help? I don't know that you can even maintain this under egraph rewriting. Probably not.

sum( share(x0, x1) , body)

Linear pattern rewriting. with explicit clones.

enode containaing share(x,x) can reduce to enode containing x

break eclass into binary eclass? is binary eclass Differetn from share?
n-arity share nodes? surrree. Not really a problem.

What about eclasses represented using pointer style union find. Then enode is redajustable. Can stil have many incoming nodes.

Low level C runtime style egraph. pointer/graph rewriting engine. How do you search it though? Garbage collection style techniques.
Tags. rust enums are tags though. 
Avoid hashing


explicit sharing


## Scoped union find
Max has put forward that we need different union finds floating around indexed in some way

Ok he says that what about just a graph with lbaelled edges. connectivity in this graph is union find. Good point. The graph is then literally 
Can I use souffle choice-domain to do somethign good. It gives me an incremental spanning tree.

A more complex union find structure seems like it could be useful. We might way a full non destructive union find. Another option is a "scoped union find". Scopes form a forest. Deeper scopes get the subsequent unions of their ancestor scopes, but not vice versa. Scopes form a partially ordered set.

Options of multiple related union finds:
1. The fillaitre conchon persistent union find has an angle where you really only have one union find active at a time.
2. Using functional maps to implement union find rather than imperative arrays or refs. You don't get path compression? How important is that really? ln(n) vs inverse_ack(n) both feel close to constant.
3. just copy entire union find every time you need a new one. Updates to higher union finds don't immediately propagate to lower ones for good or bad

When you call `find` on a lower scope, you need to traverse up through the higher scopes to collect up any unions they may have accumulated since the scope was created. Is there a way to not require this?

In a sense, scope boundaries are delimitters that stop certain kinds of union find information from being propagated. Path compression is still fine (?), but calling union on a deeper scope cannot be allowed to affect disjoint sets at higher scope. The indirection of scopes never goes away unless you can explicitly collapse them for some reason (if you keep reference to all your children scopes). Collapsing merges yourself into your parent scope, and then redirects your children to your parent.

Other names:
- Marked union find
- union find trie - we could have scopes tagged with interesting info. Or not generated basted on "gensym" counter more or less, but instead looked up by key as in a trie

The pointer perspective on union find seem like it could be interesting. I wonder if literally stack techniques from delim continuations are useful? Copying stacks, markings stacks. Just carrying a scope identifier in the references? That makes sense. You could just not union beyond your current scope. Maybe depth/name?

No you do path compress. The difference is on union. You find up to the scope barrier.
No union is the same. union doesn't require a find.

You only need to refer to the equivalence classes of the scope above you. So deeper scopes could be quite small. Hmm. Hard to see how to do this.


It almost feels like it might be a "macro scale" union find, but I don't really see how to implement union on two scopes. It would be unionfind merge + ?

Maybe it's sort of keeping a stack of union finds that are implicitly being union find merged. But we hold off to share access to the upper ones.

union = keep top of scope. Perform full find. Set top of scope to this? Or just perform union at top of scope?

Copy on write optimization (COW). `Vec<Option<Box<Page>>>` Separate domain into pages. If None, assume no change compared to scope above. If deep scopes are small changes this could lead to memory savings. At the expense of even more indirection though.


```rust
type SUF =
{
  size : usize,
  ufs : Vec<ScopedUF> 
}


struct Scope(usize); // scopes are just labeled by integers into the ufs vector
struct ScopedUF {
  parent : Option<Scope>, // may be root
  uf : unionfind //Vec<usize> // maybe make Either<Vec<usize>, SimpleMap<usize,usize>>. For scopes where not much action happens, we want a sparse fast map.
}

impl ScopedUF {
  fn default(size) {
    ScopedUF {parent : None, uf : default() } //0..size.collect() }
  }
}

impl SUF {
  fn fresh_scope(self : &mut Self) {
    self.ufs.push(default(self.size))
  }
  fn build_child_scope(self, : &mut Self, s : Scope) {
    let d = default();
    d.parent = Some(s);
    self.ufs.push(d);
  }
  fn upstream(self : &mut Self, s : Scope) {
    // This is just destructive unionfind merge? So we could merge any scope s into any other s' really.
    // iterate through scope calling union on parent
    let uf = vec[s.0];
    if let Some(parent) = uf.parent {
      //let puf = vec[parent.0];
      for i in 0..self.size {
        // but we don't even need to do a full find. We only need to to a local lookup up to next scope
        self.union(i, self.find(i, s), parent);
      }
    }
  }
  fn scope_parent(self : &Self, s : Scope){
    self.ufs[s.0].parent
  }
  // fn upstream_and_destroy? Need to fix children of scope, which if we maintain we can do. Otherwise just call destroy leaf
  //
  fn upstream_range(self : &mut, start : Scope, stop : Scope) {
    // You can call upstream in a loop
    let mut s = start;
    // actually it might be important to start at the top and work down.
    while s != stop {
      self.upstream(start);
      s = self.scope_parent(s);
    }
  }
  fn find(self : &mut Self, id : usize, s : Scope){
    // ordinary path compression
    let uf = vec[s.0];
    while let Some(parent) = uf.parent {

    }
    // No: At scope boundaries, either still do ordinary micro path compression, or perhaps merge up completely to canon of next scope.
  }

  fn delimit_find(self : &mut Self, id : usize, s : Scope, stop : Scope){
    // perform find up to scope `stop`
  }
  fn scope_find(self, id, s) {
    self.delimit_find(id, s, s)
  }
  fn union(self : &mut Self, id1 : usize, id2 : usize, s : Scope) {
    // can union 
    // maybe do parallel find scope by scope so you can early stop.
    //
    

  }
  fn push_parent(){
    // it does seem possible to insert a new scope between yourself and your parent easily.
    // make your own parent pointer point to new scope, make new scope a fresh one pointing to your old parent
  }
  fn destroy_leaf_scope(self : &mut Self, s : Scope){
    // add to dead list, clear it's data. We're incrementally heading towards a memory allocator at that point.
    // But maybe that's ok.
  }
}

```

Can you get away with just tagging? I don't really see how this works.

```rust
struct SUF = {
  uf : Vec<usize>
  scopedepth : Vec<usize>
  rank : Vec<usize>
  scopename : Vec<usize>
}
```


It's reminsceint to me of marking in delimitted containution implementation

In some sense what you want are blockers or barriers to the path compression process. Path compression should only propagate up to the scoping barrier because other scope may have the same parent. - This isn't what I think anymore?
Questions:
Magic set transformation? What does backward chaining in egglog look like? Could this use a partinitioning algo?
I guess you could do it clp/chc style 
{x = y} as a set of constraints doesn't seem that interesting. It's always possible.
Well, just pattern matching doesn't need `=`
What about a static analysis of a rewriting system.
Abstract domains of all seen terms.

## Higher order rules
We could use this
[horn into eq](https://smallbone.se/papers/horn.pdf) to reflect rules themselves into the egraph?
So what if rules are just containutation of sorts. Save points? Defunctionlization kind of? Closure kind of? A little weird. Feels a bit like magic set. Feels a bit like some kind of memoization. We could flatten out all patterns into this.

```
foo(x,y,z) => bar(z,w,q) => ziz(x,z).
```

becomes
```
foo() :- rule1(x,z).
rule1(x,z), bar(z,w,q) => ziz(x,z)
```

even rwrite rule
`foo(?x) <- add(?x,add(?x,z))`
can becomes

```
chk1(q, x) :- q = add(x,y)
foo = add(x,q) :- chrk1(q, x), q = add(x,z)
```

You only need binary multipatterns in other words.


## Semantics of Egraphs

[Yihong egglogish semantics](https://necessary-taker-84c.notion.site/Egglogish-6fce65a95af542f4964db0146eb00c8a)
[egg sem remy](https://hackmd.io/@remyw/egg-sem)
[formalizing egg#](https://necessary-taker-84c.notion.site/Formalizing-egg-53b650e3d91f42058172a877caf0950a)
A Herbrand universe U is the set of all possible terms you can build out of some set of function symbols
Example:
- add(-,-), succ(-), zero , the Herbrand universe is {zero, succ(zero), add(zero,zero), succ(add(zero,zero)), ...}
- for symbols a,b,c,  the Herbrand universe is just {a,b,c}.



The powerset of the Herbrand universe $$\mathcal{P}(U)$$ is the set of all sets of terms. Sets form a [lattice](https://en.wikipedia.org/wiki/Lattice_(order)) under the operations of intersection and union.

Lattices are a useful notion in computer science. The algebraic propertiers of commutativity, associatvity, and idempotence means you don't have to be careful or can change the order of operations and everything still comes out good. 

Monotone functions are functions tha respect an ordering $$x \le y \implies f(x) \le f(y)$$.  The [Knaster-Tarki theorem](https://en.wikipedia.org/wiki/Knaster%E2%80%93Tarski_theorem) says that a fixed point of monotone operations exists.
I think a lattice homomorphism is a distinct stronger concept?


A Datalog rule defines a function between $$r : \mathcal{P}(U) \rigtharrow \mathcal{P}(U)$$. You can convert this operation to an expansive operation via the combined rule $$ X -> X \cup r(X)$$ that adds in the new pieces to the pieces that were already there.

A monotonic operation between lattices is one such tha it respects the latice ordering.

Because the Herbrand universe of datalog is finite because it only had atoms / 0-arity function symbols, no monotonic operation can expand forever.

A partition of the Herbrand universe is a subset of the powerset. It is a set of sets whose union is the entire universe and which do not intersect. A partition can be interpreted as equivalence classes.
Partitions also form a lattice. There is a partial order of coarser and finer partitions. You can find the finest partition that is coarser than two others. This is a meet of those two. The partition where every element is in it's own set is the bottom, the partition consisting of just the entire set is top.

The set of all partitions Pa(U) is a subset of the powerset of the powerset of U.

A function symbol is basically a function between n copies of the Herbrand universe.
A function symbol can be lifted to function that work over powersets $$[f] : \mathcal{P}(U)^n \rigtharrow \mathcal{P}(U)$$  $$[f](X) = \{ f(x_1,x_2,...) | x_1 \elem X_1, x_2 \elem X_2, ... \}$$.

It is a little unclear what it means to lift f to work over partitions. But this notion is where the semantics of congruence closure lie.
The problem is we are tempted to be working over one particular egraph. But this is awkward and non compositional?

$$A^B$$ is a notation to represent $$ B -> A $$. The powerset of A can be written as $$2^A$$.

A lattice annotated powerset `X : Pa(U) -> X -> L` = $$ \prod_{X : P(U)} L^X $$. X represents the egraph we are currently working in.

P(T) := True.
(a -> Prop) -> Prop


The relational model
Instead of function symbols, we have relations (over what? integers? abstract keys? Maybe this is what a categorical model might help with.)
function symbols are relation symbols with a functional dependency
Rather than being a tree, a relational term has pieces which are multi ported graphs?
Congruence becomes a secondary consideration

What if we considered each argument to the function being from a different union find 
LxLxL -> L

And then we also have like a forker and joiner or somethig.
    _______
   /
---
   \_______

L -> LxL

LxL -> L  (meet)

does `f` even do anything to the union find structure?
Yes, it has to combine them. It also performs congruence closure on f application

The image of a partition
[f](PP) = { ~f(P) | P \in PP} = { { f(p) | p in P} | P in PP }
Do I need to do some union cleanup here? I might.
{   U_p_in_P{  f(p') |  for P' in PP if p in P' for p' in P  } |  P in PP } assuming {} dedups?

concrete example
{f(a) , f(f(a))}, {a} - image -> {f(a)} { f(f(a)), f(f(f(a))) }
No this isn't a problem. Because f is injective as a term function

Okkkkk....
What we're describing is kind of just egraphs with analysis and conguence close but no rules.

So we end up with L as a fixed point of this process (the analysis normalized egraph congruent?)

So... a rule is... a pattern match is...? 
L -> {matches}

matches also form a lattice. (you can merge bindings)

L -> {matches} of a multipattern can be done in any order and merged

an "applier"

multimatch =  (pat1 | pat2 | pat3) . joinmatch . ( app1 | app2 | app3 )

rule  (searcher | searcher | searcher) . (app1 | app2 | app3) : L -> L
f* = [f] . fork : L -> L
g* = [g] . fork : L -> L
and so on

fix(f* | g* | rule1 | rule2)

Yes.
Whqat really does congruence closure. It's the combo of f with the join semantics

Patterns as egraphs
we never 

Where | has semantics of join/meet on output.

Applier...

match -> L ???

fix(meet(f.fork, g.fork, a, id)) = the whole egraph normalization


exists P : Partition, P -> L
where L is a lattice
```coq
Inductive Term : Type :=
   string -> list Herbrand -> Herbrand.
(* Is this acceptable? Not sure it will be. Anyhow *)




Definition TSet := Term -> Prop. (* Bool? *)

(*
A particular restricted herbrand universe


*)

Definition schema := list (string * nat) 
Definition valid (s : schema) : Term -> bool := fun (t : term) =>
  match t with
  | name, args => if lookup (name) == List.length args then List.forall (valid s) args else false.


Record FinPartitioning := {
  partitions : list TSet;
  complete : TSet
  disjoint : Forall2 partitions (fun x y => forall t, band (x t) (y t) == false)
}.


Record Enode := {
  head : string;
  args : list nat
}.
Definition egraph := list list Enode.
(* 

Definition egraph := { Term -> Term , with fixedpoint exists}.


*)

(** conversion to TSet *)
Definition in_eclass (egraph : egraph) (eclass : list Enode) : Term -> Bool :=

Definition denote (egraph : egraph) : FinPartitioning := 


Definition Partition := list Enode. (* A little disatisfying. *)
Definition Partition := TSet.


```







A lot of things are implicit in the egraph. They have to be because it's talking about an infintie number of terms


Implicitly f(a) = f(b) in the egraph.

Q = (Herbrand Partition -> A) is the basic lattice of the egraph analysis


f : QxQxQ -> Q  is combo congruence closure and analysis transfer function
By default things can just return bottom, so you get to choose which trasnfer functions you define


a : () -> Q

So these are semantics, but then relating this back to a finite representation (and an efficient one)








## NE graph
 See also bisimulation partition refinement
 
https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.637.1146&rep=rep1&type=pdf co-nelson opppen.
Combining theories through inequalities. Somehow condindituvie/bisimulation
analysis + opt https://dl.acm.org/doi/10.1145/565816.503298 sorin. Do dataflow while optimizing?

If you can say _nothing_ in X equals anything in U\X.
cons(a,b) != not cons

So the input to not_eq is 1 set.
a pattern describes a set.
cons(?a,?x)
Indeed this is stating a form of injectivity
Or more than that? We are also saying that f(7) = cons(7,[]) is not allowed
cons is an ocean unto itself.
{cons(?,?), f(?), g(?,?)} is stating closed universe assumption only f and g have access to constructor cons.
Ok, but nothing can call f or g either. too closed

Should this be about terms? Contexts? co-terms? objects? automata? graphs?
What Set representation to use? Is Set the wrong alley?

concrete ineuqalities as the _rules_. Patterns go in the negraph?
If we find 
can match a + b using the current negraph, we derive a pattern from it we assert as disjoint?
cons(?a,?b) is a distinct _pattern_ from all others, even it it isn't a distinct function/constructor really.
cons(cons(?a,?b), ?c) is not distinct from cons(?x,?y)
Multi match? Views? Pattern synonyms
Maybe the pattern match coverage problem is a good one? Coverage and defunctness.
| Cons(Cons(a,b)) ->
| Cons(a,b) -> 
| _ -> 


I guess when you know things have distinct types you know they can never be equal. It doesn't even make sense.
If you have equality reasoning on types that can be touh to say out of hand though (unless injective)

distinct( ?x : hom(?a,?b) )
disctinct( ?x : ob )

types are a way to 



That's interesting. A formalization of the inaccessibility of certain constructions.
private.
{ ctor1, cto2, class_method1, class_metho2,  } states a closed class definition.
Can we express subtyping? Seems hard.

But is separates off an entire mini universe.

can anything call meth1?

?ctx[ clor1(?,?) ] also describes a set of terms. Anything that contain ctor1
{ ?ctx[ match(cons(?a,?b), rhs(?a,?b))  ] , ctx } No. Doesn't really work. We don't want to say has to any rhs has to include the match form?
I guess that's eta expansion. Hmm.

_Everything_ else is seperate? We have to descibe very small equality classes.

Give X, X and U\X are distinct.
What methods of describing X are useful here.
disjoint(X)

disjoint(X,Y,Z) st XuYuZ = U or disjoint(X,Y) Z is implicit = U/X/Y
but disjoint(X) /\ disjoint(Y) is the same thing.


foo(?a) != bar(?a) is there a way to talk about this?
{ .. . .. . . foo} { .... . . . . bar} but what are all the dots?

Just contexts? "dual" to terms?





subclass {ctor1, ctor2, ctor3, meth1, meth2, meth3}
superclass {ctor1, ctor2, meth1, meth2}

->
meth3 can't access ctor1 or ctor2? That's not right.

{}  {} 

pattern matching?
f(cons(?,?))





Lambda binding:
Optimal evluation work relevant?
Compiling lambda terms relevant? Closures? Reflection into host? Uncurrying trasnformation?


[egg-smol](https://github.com/mwillsey/egg-smol)
labelled equalities
eq(l,x,y)
l could be integers, a hierarchy of eq. colors
tuples of integers?
Absract symbols 
they could be a context coming from lower egraph.

## Egraphs over programs. Program analyiys
Poinbter analysis.
https://szabta89.github.io/publications/inca-pldi2021.pdf 


unification-based point-to analysis 
VSA: interval analysis + periodic analysis 
https://www.cs.cmu.edu/~aldrich/courses/15-819O-13sp/resources/pointer.pdf Anderson vs steensGaard


kCFA is a place where datalog and lambdas meet. Could there be something good there?


.type Expr = Lit n : number | Lam symbol Expr | App Expr Expr 

init($)

subterms() :-
label($,) :- init()
context(t, Hole):- init(t)
context(x, Add(Hole, y) :: c), context(y, Add(x, Hole) :: c),  :- context(Add(x,y), c)

% literlas are equal under any context.
context(Lit(n), C1) = context(Lit(n), C2) 
% alpha renaming?
context(Var(x), C1) =? context(Var(y), C2)

Interesting idea.

Anyhow. Once you build contexts, you can do cfa
sigma(l, n) :- context(Lit(n), l)
sigma(l, t) :- context(Lam(t), l), sigma( Lam(Hole) :: l,  v )

https://dl.acm.org/doi/10.1145/3136040.3136043 quoted staged rewriting

I don't want all subterms. Equal subterms should be different things.
If egraph gets top, what does that mean? Early termination, what does that mean?

Is a union find a lattice? One that largely stays the same except when crossing binders
join of two egraphs is 1. union of symbols. 2. closure of equalities.
identity element is egraph with no terms.

avalaible expression analysis is reminscent. You could use a (persistent?) hash cons. Well you could store inefficiently, but there is almost certianly some efficient wya to do this. available equalities.

Inner scopes inherit from outer scope except when viarables are introduced.

http://moskal.me/smt/e-matching.pdf ematching for fun and profit
https://github.com/yihozhang/sdl staged datalog
https://github.com/yihozhang/egraph-sqlite egraph on sqlite


grobner bases. What is a term? a monomial? a entire polynomial in canonical order?



https://www.categoricaldata.net/cql/fmc.pdf Fast Left-Kan Extensions Using The Chase. Uniqueness quantifiation

[Sketch-Guided Equality Saturation
Scaling Equality Saturation to Complex Optimizations in Languages with Bindings](https://arxiv.org/pdf/2111.13040.pdf). De bruij indicies for lambda calculus. Guidance of rewrite rules.

<https://github.com/mwillsey/thesis> willsey's thesis

<https://twitter.com/corbinsimpson/status/1456649872182939654?s=20> corbin simpson using egg for categorical combinator.

https://github.com/gussmith23/glenside



staging ego (ocaml egraph) could be fun.
Also could try to extract the alt-ergo implementation.

<https://github.com/mwillsey/snake-egg> I approve 


bdds are both more and less than hash consed if then else trees. See fillaitre paper. Can I embed bdds into an eg raph implementation? Bddbdddb used bdds as the backing store of a datalog. Pretty clever and cool. I actually feel like this might be more powerful in many cases of interest than a relational representation.
eqrel representation is also a lattice. The join is the smallest equivalernce relation that works, that's kind of the beauty of it.
What cody wants is a custom relation domain. .decl(  )


hasahing modulo alpha equivalence
- false positives and false negatives
(x + 1) can be totally different things in different contexts
why not de bruin? ok sure. de bruijn are inefficient. false negaitvfes nad false positive
position maps of variables with structure maps.

e-summary - alhpa equivalent if same e-summary


hashlog using souffle.
If I keep the numbr of entries small, I could make a hashing function and just pretend no collisions happen. Or use cryptographic hash
hashcons + union find = egraph


module system - algebraic specification
discharge using smt? A style with which to structure smt proofs.
discharge using egglog - signatures are queries, impls are databases.
Embedded in ocaml, julia, or python.
sexify using modern looking syntax (ryst like? leanlike?)
mod Foo : {

}
semi-naive ematching
applier should return (Id,Id) tuples and also Id of new nodes generated.
This is the fresh information.

I should try building a datalog.
Maybe take a particular datalog program and write it out.


congruence closure as an axiom in the theory.
f(x,y) = f(z,w) :- f(x,y), f(z,w), x = z, y = w.
seminaive based on an update to one of the equations on the right hand side.

parents(x,f(x,z)).
parents()
 :- x = z, parents(x, f(x,X)), parents(z,f(z,X)).

all tables should only be maintained modulo equality. Stock souffle does not do this for me.



Denali and epegs - denali is almost a parallel idea to the uniqson work but from a theorem prover persepctive rather than constraint programming perspective

Egraphs and ematching and instruction selection
If I did not use any union finding, egraph is a hash cons.
Ematching is hash cons matching

egraphs for program syntehsis. That's intriguing. Armando notes and Koppel paper.


Call ematch. Record all matches
Send off to cover solver.

Combine the blindell universal function + epeg





DOUBLE EGRAPH WOAAAH
Yea. I could just maintain 2 egraphs if I want to have 2 notions of equality.

https://www.southampton.ac.uk/~gk1e17/chaseBench.pdf
The chase. EGD and TGD functional dependencies
These both seem like things i want.

memoization of patterns
I used the devie
pattern(A,B,C) :- yada
to record a pattern match. This reifies the bindings of a pattern into a table.

But we can do the same in the egraph and egglog.

zzz :- f(f(f(f(f(X))))).
patmemo(X) <- f(f(X)).
zzz :- P = patmemo(X), f(f(f(P))).

congruence closure and memofixing updates

semi-naive - just search over changes for one of the predciate.
There are many many implicit flattened predicates.


Proof scripts - query and forget egraph but push result into facts / rules if true.
?+ == copy egraph. query. insert into old egraph if true.

Allows proof scripts like this
?+ x = y.
?+ y = z.
?+ z = foo.

?+ x = y.
?+ .. = z.
?+ .. = foo.

?+ x = y.
.. = z.  // allow suppression of ?+ in presence of .. syntax
.. = foo.


?+ forall X, foo(X) => bar(X).  // make fresh X. qeury egraph. If works, insert as lemma. If fails stop program here.



Scoped rules

{
  foo(X) :- bar(Y).

}
// rule is dropped here.
:- push.
:- pop.
:- push_rules.
:- pop_rules.
:- push(egraph).
:- pop(egraph).

"calculating compilers" using an egraph approach. Is this possible?
Instruction selection as an egraph problem?


Hmm. Can you use the size of eclasses to solve combinatorial problems?
Can you implement ZDD or BDD in egrpah form? hash consing is how that kind of works

https://www.youtube.com/watch?v=W4kveStDZXI&ab_channel=YOW%21Conferences hashing moduleo alpha

hashlog - egglog without the union find
Keep a hashcons data structure. It can store all known facts and trees.
hashcons matchign
Semi naive seems more dable
Could I add hashing to souffle to get egglog back?



"egraphs" for higher homotopy? ports are edges?

egraphs for graphs
Take all subgraphs. You can compose named ports to each other
Self consistency - a loop in the g-graph. (g stands for graph) Like some kind of microscope thing
The hroizotnal and vertical composition i already noted was sort of deconstructing to alll
possible decompositions using the nameless positional encoding of string digrams
If we took a string diagrams and put every possible term decopmosition into it from the get go. Because the graph is already a caonnical form in rgards to associativity. It is not in regards to other equations.
Eclass -> [  ]


Hash consing graphs ( depends on dcomposition method, unless you hold eclasses of all possible decompes)



https://asplos-conference.org/abstracts/asplos21-paper142-extended_abstract.pdf

This is interesting sounding
Did they use egraphs for instruction selection I wonder?
Like suppose you had a rewrite rules going from ir to instruction selected ir (+ a b) = (arm_add32 a b)
But you also had IR -> IR equalities
Like (+ a b) -> (+ b a)
and I suppose MIR -> MIR rules would be possible peephole optimizations

There is a similarity between PEGs and blindell's UR and or expression graphs
Actually, what it would do is pattern matching.
Pattern selection would require a secondary optimizatioon problem

How do you instruction select over an egraph
consider a muladd instruction a*b+c
but you have a*(b+d) in your tree.
You want to egraph rewrite and then cover egraph.

Parition refinement is the "dual" of union find - a system of inequalities.
Reverse congruence. f(a) /= f(b) -> a /= b
contrapositive
the dual of an egraph

If we had observations and had to deduce what went wrong
The greatest equivalence relation consistent with the facts
as compared to egarph which is least equiavlacne relation

algebra and colagebra




Egraphs = coinductive? They are the least solution to a set of equations. The looping behavior is reminscent of CoCaml and CoLP.

Cody seems to think pasting of two pullback squares could work.
Hmm. Can I make an ematching quantifier for pullbacks?

Universal quantifier
d    b
     |
     v
a -> c

p -> b
|
v
a
for any p and two morphism such that square commutes, we get a new morphism univ_0(x,y)
with equalities associated with it. And uniqueness?
so we need to be guarded by some tough equalities
ok but fine.
But then to prove universality of the pasting
We can assert some object and some square.
But we need to deduce the universal morphism and prove that it has good triangles. and uniqueness?

What about just epi and mono. Hmm. I need exactness too
https://en.wikipedia.org/wiki/Five_lemma




An igraph - an egraph for inquality reasoning? Some smart datat structure for holding inequalities rather than union find or in addition to? Then the pieces that arec monotoonic lift inequalities through constructors.

PEGs
Convert imperative to functional program essentially. Gated-SSA. Convert back
Generalizing proofs for compiler optimizations: What? Something about 

prolog gensym
Well, gensym is possibly sufficient for my egraph thing. a map to unification variables? nah



https://www.cs.cornell.edu/~ross/publications/proofgen/
Substitution as a category. Free variable nodes. Can I substitute in an egraph? Is subgraph query doable (prob not right)? Substitution via enforcing an equality between the variable nodes and the terms that 
i'm substiution them for. Or no yeah. I could really go in an cut them out of the eclass they belong to.

https://dash.harvard.edu/bitstream/handle/1/4762396/pldi84-tristan.pdf a followup paper - translation validation
https://www.cs.cornell.edu/~ross/publications/eqsat/ equality saturation compiller optimiztion
PEGs program expression graphs
gated ssa
The nodes of loop variables represent the sequence?
Partial sums. Makes sense from an inifite series perspective.
psum f = 0 : (map f 0..10) + (psum f)
!! is indexing function.
We need to lift various things via repeat
and map under layers
data MultiList acc =
MultiList = a + [] + [[]] + [[[]]] + ...
Multilist a = Free [] a
data MultiList a = Roll [MultiList a] | Pure a
Reminsicent of fock space. Not quite. There's something there.

The sum = eval psum factoring somehow makes the de bruijnn raising and lowering more local?
psum has a short self referential definition in terms of +.
sum does not generically?
There was a comment on zulip about geometric series summation
sum g^n = 1 + g * (sum x ^ n) which is a self referential defintiion.
The endpoint moving definition is in some sense equiavalent to the psum
This is inductive rather than coinductive. Is coinductive + egraph somehow important?
sum(n, f) = f(n) + sum(n-1, f)
sum(0, f) = 0 

psum(expr) = cons(0, psum(expr))
psum(n, expr) = lift(n, )


Pawel - essetnailly algerbaic theories. string diagrams something
freelinearfunctions
linearmaps.jl linearoperators.jl
interacting hopf alggerabs

Right, The term model makes differences between schedulings
Yolk.jl, mixtape
abstractinterpeter
alternaitve exceution pipline
runtimegernated

mixtape emit function can dump out bullcrap
mccoy becker
mjolner irtools
bang bang jl 
temporal relation


IR as a category. Keno papers
Single blocks optimization

The egraph as a database.

pattern matching datalog - 
The PATTERNS are dtalog? You could reify the rebuilt egraph into a database. Sure.
But then querying a pattern agains it?


conjunctive queries - np complete
worst case partial join
schedulers.
Thousands of rules.
credit assignment

simulated annealing?
oliver flatt working on proof production in summer


Explicit contexts and context lifts using named variables.
We want local transfromations, because the ergaph gets in the way of stuff
We want thing to be semantic unless we can turn off selective congurence closure.
Explcit contextx will largely be shared.

bound variable expressions
actually extracting good linear algebra
catlab stuff.
proofs.
datalog



lift(x, e)
select(y,e)
Kind of goofy
barendregt convention. gensym new sums
sum(x, e) * sum(y,e) = sum( sum())
reqiures explcit x != y check
x and y should not be eclasses. They are explicit parameters

We can punt on alpha recovery.
sum(i,i) = sum(j,j) is ok
Not this can cause problems
sum(i,i) * sum(j,j) = sum(i,i) * sum(i,,i) 
!= sum(i, sum(i, i * i)) which may result from missing inequality guard above. 
With rearrangement, is this enough?

used variables analysis. ->
sum(x, e * b) = e * sum(x,b) if e does not contain x

Model: Important.
T = (Key -> Int) -> R
sum : key -> T -> T
sum k f = sum $ map f (\i -> \m -> if k = k' then i else m k') (1 .. 10)

x :: T
x m = toReal $ m "x"

y :: T
y m = toReal $ m "y"




Egraphs as rtelations  http://effect.systems/doc/relational.pdf
Datalog + some fields are equivalence classes?
Souffle could do this?

eclass(a,b) .equiv
eclass(a,b) :- f(x,y,z), f(x1,y1,z1),
              eclass(x,x1), eclass(), ,  // One per congruence relation
enode(n, q) 

equiv(n1,n2) :- f(n1, x, y), f(n2,x2,y2), equiv(x,x1), equiv().
equiv() :-
equiv() :-


A pattern match is a query
Q(Root, A) :- f(Root, x,y), g(X, )



Alpha equiavalence:
New enode type? Pointers down or up from binding site? Or to third kind of node?
Recover alpha equiavalence via explicit search somehow
Explicitly model contexts?  x |- x  is different from a |- x
"Hypothetical" unioning? Requires persistent backtracking of unions. Seems useful for recovering alpha.
lam x f ~? lam y g.  if x ~ y => g ~ f. Niavely requires double search for lam over egraph.
Well, what might trigger a new alpha equivalence?

Name and de bruijn in parallel? lam x g g'
de bruijn. Combinators. Same thing? The swap operator was not good. Pushing lazy raising and lowering. It had some nice properties too. raise(u,l) or something? Maybe swapping sum is a bad example to work with.
Names are nice because we have non local character so we can manipulate binders but leave alone use sites
sum_j sum_i q => sum_i sum_j q
de bruijn is nice because we have canonical names. Alpha equivalence becomes automatic.


CatEngine - get david to draw me a cat engine hybrid
Metaocaml + egraph? Ocaml egraph could be good. As cody was saying

Ematching patent application?
https://patentimages.storage.googleapis.com/26/5a/2e/9a7722870e4dbb/US8103674.pdf

Type isomoprhism reasoning in egraph
fix x, 1 + x  is list.
fix x, f x ~ f (fix x, f x) fix equations
quantifiers
semantic model is relational model of paremtricity?
Theroensm for free reasoner?
Ordinary alegebra 0 + x = x, ayda yada
+ yoneda like things from that one paper
https://homepages.inf.ed.ac.uk/wadler/papers/free-rectypes/free-rectypes.txt
lfix x,, f x ~ forall x, (f x -> x) -> x



Proof production:
Does z3 actually keep proof data in the egraph?
The extended union find gives reasons.
I guess every up link has reasons
We maintain another array registering which parent gets attached to which parent.
And an array when a root becomes a child, we record a canonical term at that time. And perhaps we only need to record an enode at that time, since we can look for older stuff? Does the array build a temrpoal order on the nodes?
The "best" enode. or a random one?
Or is it insertion time. At insertion time Eclass == ENode. Maybe we want to record that?


We need to perform an extraction every time we find a new equality? Best term in Eclass equated to best term in other eclass.


Multipatterns and datalog
Allessandro made this analog that is interesintg.
You can use the egraph as a "database" of terms
And use multipatterns to insert new terms. This is just a hash cons.
But you can also use = as a special symbol on both the left and right side of prolog rules
To create new equalities, and to check out ones
What are the syntactic restrictions on dastalog?
We aren't unifying, we're just pattern matching. Is that a problem? A difference?

Semi naive evaluation. Is it possible to only try to match terms that are updated?


Alessandro has this idea of anti rules. These are probably problematic to integrate with this search. Negation is often a funny fellow


b = c :-  b, a = b, c 







Ok, looking back. Could I make a Z3 version?
Horn solver maybe. Careful triggering.

ZX calculus?

Einsum - compile to matrix form. Then do point-free rewriting. This is spore approach
https://optimized-einsum.readthedocs.io/en/stable/
https://github.com/Jutho/TensorOperations.jl

It also feels like compiling a lambda abstraction to combinators.
The bracket abstraction http://www.cantab.net/users/antoni.diller/brackets/intro.html
lam x. b =>

\x -> E K ()

sum(i -> E) = K

Am I crazy about using explicit indices? Is that not just fine? I should have a crisp objection.


sum_i = 1^T like in probability
a_ijk b_ijk =>  dumb trasnlation of otimes with dup.

logic sound with respect to model
untyped lambda is unsound? With respect to what model?
[x]y = K y
[x]x = I



sum(i, i)
sum(i, i * x) = 
-- we need to know x contains no i. This is not clear. We could imagine an "contains i" analysis.
can contain i vs has to contain i. 



Functional programmng and rewrite systems:
Very similar. 
- pattern matching
- recursion

The dom(hom) type trick more abstractly is that you can internalize
functional programs into the egraph rules. You want to do this lightly.
I used this as both guards and to produce complex reight hand sides 
(non productive ones. Hmm Is this another connection to coninductivity?)




We don't have built in higher order functions.
You lose deterministic control
(lam )
(x :: Gamma) |- succ t = Gamma |- ts
eval t env = 

De bruijn-ish something is important. We don't get alpha renaming for free so it's a natural way
to canonicalize that
Egg went a little extrajudicical
But can you write eval for a sum?
You can, I jst don't want to expand it.
eval (Sum t) env = sum [eval t (i :: env) | i <- 1..10]
Maybe gamma should just be the integer like cody said
An outer context lift
n |- succ var 

lift n = n |-, takes an m < n dimensional function and lifts it to n-d.
We can store n as an int.
Or we could store the range in the context.
Wouldn't that be somethin.

de buijn levels would be useful.
They locally record that we don't need your shit. Uh. Wait. No this is indices
sum(succ(x)) = N * x


succ(x) * succ(y) = succ(x * y) -- can extrude lifting through simple functions
succ(x) + succ(y)
x * y = y * x
sum(x + y) => sum(x) + sum(y)
sum(succ(x) * y) = x * sum(y)
sum(1) = N 
sum(var) = N * (N -1 ) / 2, or benoulli poly 
sum(succ(succ(succ))) = b(n,N)

sum(var(1)) = 
sum(var(2))
succ(var(n)) => var(n+1)

x = x * 1


Those two rules give
sum(succ(x)) => sum (succ(x) * 1) => x * sum(1) => x * N
sum(sum(succ(var) * var)) => sum( var * sum(var) ) => sum(sum(var) * var) =>


sum(sum(x)) = sum(sum(swap(x)))
swap(var) = succ(var)
swap(succ(var)) = var
swap(x * y) == swap(x) * swap(y)
swap(x + y) == swap(x) + swap(y)
swap moves through everything like succ. (Except succ. It doesn't move through succ).
Feels very expensive.


swap(succ(x)) => succ(lift_var(x)) -- Then we don't need the extra swap(succ(var)) rule.
lift_var(var) = succ(var)
lift_var(succ(x)) => succ(x)
lift_var(x * y) = lift_var(x) * lift_var


lower(n, succ(x)) = succ(lower(n-1, x))
lower(0,succ(var)) = 
lower  
raise(0, var) = succ(var)

raise(n, lower(n-1)) ? Can't be. the intermediate state makes no sense.

de bruijn do have these useful rasing and lowering operations though


Well, it may indeed be _an_ encoding. so i dunno.

A setoid egraph. 
congruence closure could be written as a rule
x = y => f x = f y 
enumrate all f, or allow matching in the head position.
This is a guarded rule.

I've said the egraph is semantical because it builds in congruence closure. So you need to be talking about something for which congruence closure makes sense.
In particular, I think you should have a model in mind for your syntax. Given a term, you should be able to give me functions for the function symbols and elements for the constants, such that your mathemtical thing obeys your rewrite system.

Yes, yes. Peano arithmetic is fiendish. I know. But each move is meaningful.
The succ forms opens up the lazy push around.



GATs for binding
Nathanael Arkor12:26 PM
You describe everything internally: so you have a type for "contexts", a type for "types", a type for "terms", etc. Then you have operations for extending contexts, etc. This is essentially the approach of "type-theory-in-type-theory", which uses quotient-inductive-inductive types (which are closely related to GATs).

Equational metalogic Fiore http://ropas.snu.ac.kr/~gil.hur/publications/soeqlog.pdf
https://crypto.stanford.edu/~blynn/lambda/logski.html the oleg paper


Cody:
```coq

Definition Ctxt := nat.

Definition ext : Ctxt -> Ctxt := S.

Inductive Lambda : Ctxt -> Type :=
| Var : Lambda (ext 0)
| App : forall  : Ctxt, Lambda  -> Lambda  -> Lambda 
| Abs : forall  : Ctxt, Lambda (ext ) -> Lambda 
| Weak : forall  : Ctxt, Lambda  -> Lambda (ext ).

Definition id : Lambda 0 :=
Abs 0 Var.

Definition K : Lambda 0 :=
Abs 0 (Abs 1 (Weak 1 Var)).
```

I want to do sums:
N^n -> N

Realtion to quantum raising and lowering operators? Is that ludicrous?
extending the context = adding on a free parameter = (N^n -> N) -> (N^{n+1) -> N)
weakening the context = (N^n -> N) -> (N^{n-1} -> N) This makes no sense. You can evaluate it, 
sum over it, maximumize over it. You can't just wekaen it.

Jon sterling using GATs from Kris
http://www.jonmsterling.com/pdfs/algebraic-universes.pdf


Note on Mechanized Equational Reasoning for Categories with Metatheory.jl

Here's the punchline

```julia

```

I'm sure you've always wondered if that was true.

In terms of a string diagrams:
![](/assets/pairproj1proj2.png)


Read on for what this means


Compile the egg version for wasm and embed


## Metatheory and EGraphs

Alessandro Cheli has made an extremely intriguing package called Maththeory.jl. As I understand it

- Homoiconicity
- Some mumbo jumbo that I don't understand but is probably very important about using RuntimeGenerateFunctions to ge the right hand side of rules to be fast
- A tuned and feature complete EGraph implementation

I've discussed implementing EGraphs in Julia on this blog before. 
- https://www.philipzucker.com/egraph-1/
- https://www.philipzucker.com/egraph-2/
EGraphs are a data structure that efficiently achieves sharing of subterms in the presence of equality reasoning. The [egg](https://egraphs-good.github.io/) project has recently innovated and brought this technique to prominence

Here's the basics of how you use the Metatheory EGraph backend at the moment

```julia
using Metatheory
using Metatheory.EGraphs


```

I wrote my EGraph implementation with the intention of using it for this blog post.
I've previously written of my abortive attempts using the automatic theorem provers E prover and Vampire and Z3.
- https://www.philipzucker.com/notes-on-synthesis-and-equation-proving-for-catlab-jl/
- https://www.philipzucker.com/theorem-proving-for-catlab-2-lets-try-z3-this-time-nope/

I don't know why these encodings did not work. It still feels to me that they should've. The black box nature of these systems is a problem for troubleshooting.

Catlab is built around a kind of logic called Generalized Algebraic Theories (GATs). 
- https://algebraicjulia.github.io/Catlab.jl/dev/#What-is-a-GAT?
- https://ncatlab.org/nlab/show/generalized+algebraic+theory

In multi-sorted first order logic, you have terms of simple sorts like G or Int. For convenient encoding of categories you need a little more magumbo. One wants to talk about A as a term of sort Object, but then also talk about $$id(A)$$ as a term of sort $$Hom(A,A)$$. You see that the sort of `id(A)` is dependent upon a term `A`. This means that some kind of dependent type system is at play. This is possible to encode into Coq for example, but GATs are a less complicated system that has enough flexibility to do this. They're some kind of dependent types lite.

Encoding this type system correctly is tricky. I naively thought in the first post that basically the issue could be ignored, since every equation is type preserving. If the types start good, and the rules are type preserving, then everything is ok right? This has not been the correct mental model of the situation. It is highly dependent upon the exact reasoning system the degree to which types can be ignored.

A somewhat brute force way of dealing with types is to encode terms as terms tagged with types. For example, instead of merely using the term `id(A)`, we replace it with the syntax `typ(id(A), Hom(A,A))` or the equiavlaent infix syntax `id(A)::Hom(A,A)` that looks like a pun on Julia syntax or `Hom(id(A), A, A)`. The latter form was pointed out to me by James Fairbanks and it has the advantage of fusing 1 level of indirection (In a sense partially fusing the tags `typ` and `Hom`). Really we want to perform this tagging recursively like `Hom(id(Ob(A)), Ob(A), Ob(A))`. This gets incredibly verbose to type out for a human.

Some other notes

- The rewrite rule `f => id(A) \cdot f` produces A out of nowhere if types are erased. That means they either need to be reconstructed.
- Catlab overloads $\otimes$. This makes sense to Julia, because Julia is tracking the types of things and uses them for dispatch. In order to track the types for dispatch in our
- It has been argued that one could use the Julia type system to encode GAT types. The construction of Metatheory.jl makes this a light weight if possible. I argue that this is unlikely. Part of the cleverness of Catlab was to avoid abusing the host type system like is common for categorical constructions in Haskell or Agda.

Type tagging maybe be familiar to the programmer in you in that is the logical reflection of the technique of dynamic typing. Dynamic types are implemented by maintaining a tag at runtime describing how to interpret the attached data. We are in essence doing the same thing in our syntactic rewriting system. We get dispatch based on this type in the same manner that Julia or python get dispatch off of their type tags.




There is absolutely no reason to overload \otimes for both morphism product and object product from the persepctive of mechanization. I admit it is nice to have such overloadings for people, but they should be quickly stripped off for internal representations via inference. I chose to keep them syntactically distinct.

The technical connotations of "sort" vs "type"



James Fairbanks10:58 AM
GATs are a fixed point of the "X can represent Y" relation so they make a good target as a level of generality


Philip Zucker: I thought way back that this preservation would just happen if you strip the types like it does for STLC or something, but it doesn't. I think this misconception is due to my inexperience doing hand calculations of this type (which almost no one does. String diagrams rule), plus I think you "code the happy path" when doing it by hand and may not even notice a situation where just shallow pattern matching on the syntax would allow you do do something verboten, since you're smart enough to not make that dumb mistake. It's also easy unless you're being hyper careful to take big steps that seem obvious but aren't actually implied by the axioms you have.

Philip Zucker: Furthering this misconception is that for a large majority of the 30 some axioms for cartesian SMC it actually _is_ completely fine, I think. So far I've only identified about 4 or so where it's a problem, one of which is the interchange law.

Philip Zucker: And it also can only be using the axioms in particular directions too. One direction is syntactically safe, but the other direction requires checking typing conditions

Philip Zucker: Trick question: Can you apply the interchange law (f  g)  (h  k) => (f  h)  (g  k) to the term (f  id(a))  (id(b)  k)?

Philip Zucker: No you can't. In my example, f actually has type b c -> b c   and k has type c a -> c a .

Philip Zucker: The other direction is always fine though. Given (f  h)  (g  k) is well typed, so is (f  g)  (h  k)  I believe.

Philip Zucker: Again all this is obvious in string diagrams. So obvious as to be unobservable.


Egraphs as a model. Z3 uses EGraphs as models of logical statements about uninterpeted functions
We can also consider sets as models of egraphs.
An egraph is a model of a conjunction of equations if 

An interpretation of an egraph is a mapping of function symbols to functions 
and constants and equivalence classes to elements such that if there is a function symbol enode in an eclass, the function interpretation maps the interpetation of it's children classes to the class the enode lives in.
This might be easier to read in mathemtical symbolism.




### Bits and Bobbles

Where to go from here:

2. Actually integrating with Catlab. Does it scale?
3. Daniele Palombi has brought the coend calculus https://arxiv.org/pdf/1501.02503.pdf This seems like a very interesting application. I think the direct attack on this problem using Metatheory requires understanding how to deal with alpha equivalence in the EGraph Conversation here: <https://julialang.zulipchat.com/#narrow/stream/277860-metatheory.2Ejl/topic/Formal.20Proofs.20for.20CT.3F>
4. String diagrams <https://julialang.zulipchat.com/#narrow/stream/230248-catlab.2Ejl/topic/Using.20Metatheory.20for.20String.20Diagrams> There is a compelling argument that string diagrams are a preferred representation, normalizing the associativity,commutative, and unit properties of a categorical construction. It has been suggested by Alessandro and others that teneralizing the EGraph data structure in some sense may be to go.
4. Proof production. Giving 


One wonders if perhaps with my new understandings I could get z3 to work.

I had some verbose encoding I wanted to try
But I also feel like this internalized type thing could work in z3

The external z3 ematcher


Alpha conversion. Add parameters to function symbols.
sum(n, f) : (N^{n+1} -> N) -> (N^{n} -> N)
n is the number 
sum(0, f) = f ??? Eh why bother
sum(1, f)

ind(n, i) == proj(n,i) : N^n -> N
the ith projection of an n tuple. Play the roles of indices
sum(1, sum(2,  ind(2, 1) * ind(2,2)  )  )  )
sum(n, sum(n-1, ind(n, )  ))
Based on the semantic model, I feel like this makes sense.

Every operation also needs these scope tags.
These scope tags are equiavalent to marking both de bu9ijn indices and level

likewise for derivatives
(R -> R) -> (R -> R)
D(n,m, f) marking n input size and m output size
Or perhaps start with only scalar.

Huh. Maybe everything is working in fock space?
Then sigma is a lowering operator.
Fock space is a useful model of a world where we're moving around the number of variables in existence
Disjoint sum of function spaces

adag is adding on a delta function
adag(x)
adag(i,x)

sum reduces the number of free variables by summing over the leftmost one
Ok, but we want to sum over more than just the leftmost.


What about explicilty modelling homotopy functions.
f(0) = a
f(1) = b
g
g
k(1/2) = f(1) = g(0) - contingent upon a proof of f(1) = g(0) that this new function is definable?
k = compose
allowance of a move
 f(x,t) = f
 f(0,0) == 
f()
forall x : [0,1], f(x) = x
f




A decalartive rule interchange format



Could we literally use smtlib?



(set-option :verbose)
(set-option :scheduler :backoff)
(set-option :node_limit)
(set-option :type-check)
(declare-sort  )
(declare-fun  f)
(declare-fun  f)
(assert (=  expr1 expr2 ))
(assert (!=  expr1 expr2 ))
(check-sat)

(declare-rule )
(declare-eq   )
(check-eq  expr1 expr2)
(addexpr expr)
(simplify  expr :iter :timeout :node_limit :  )
(push) -- pushes rules or pushes state of egraph?
(pop)
(push-rules)
(pop-rules)
(clear-egraph)
(exit)

https://rust-cli.github.io/book/crates/index.html

We could have the thing build the corresponding rust file for you.
For guards we'd need a full programming languge
I could build in the facilities to 


There is something rather intuitinistic about the egraph.
If you had propsitions in the egraph, having 
p = True, is really more like p = Proved.
Since not having it does not imply that p is false
p = False is known falsehood
p = True is known Falsehood
neither is unknown
I guess it's more like 3 valued logic.



Condictivtely, egraph is "well-typed"
Each GUARDED rewrite rule maintains well typed-ness

Given the left hand side has matched a pattern that is in fact well typed,
we can infer some equational constraints about the types.
If these equational constraints do not fully imply the typing of the equational
rule, then these extra conditions must be added as guards.



We should 
1. Check the condition
2. Add the typing terms to the egraph so it at least know to refine these.



Definition of well typed:?



Ituitively speaking, an egraph represents a possibly infinite set of terms.
A pattern represents an infinite set of terms. A pattern `f(?a, x, ?a)` represents the set of terms that include $\{ `f(x,x,x)`, `f(y,x,y)` , f(g(c,d), x, g(c,d))...\} $, ie. all the terms . Tis descrption format of sets forms a lattice. The lattice operations of join and meet are defined via 

A (multi)rooted egraph represents a possibly infinite set of terms. It is the set of terms you can build by starting at the root and following the links.
The egraph `[x a]` where a has an edge to the class represents the terms `{x, a(x), a(a(x)), a(a(a(x))) ... }`. 

The allowance of cycles in the egraph makes it unclear to me how to precisely describe the infinite terms. There's something very coinductively at play here.

Perhaps it represents the set of terms with possibly class ids at the leaves. That feels more honest somehow. You start with the root id S_1 = {root_id}. Then you can expand one step to expand this S_2 = {a(root_id} or S_2 = {a(root_id), root_id}. Eventually a subset of these will be the finite terms. 





James made a number of good points



Maybe a highest de bruijn index analysis?
Lowest?
Bothm right,.
Then if they split

sum_examp = @theory begin
   sum( sum(a(0) * b(1))) |>  sum(a(0)) * sum(b(0)) # factoring
   sum( sum(a(0) * b(1))) |> sum( sum(a(1) * b(0))) # permute sum symbol
end

Bidning forms: de bruij indices with raising and lowering
Avoiding accidental dummy clash. Moving pieces out from 


There was that one paper that showed the equational form of the yoneda lemma
in terms of fxied point operators and stuff.

 = a^b  a * b, yada yada.




Dowke higher order unification mentions associativty as a problem like comprehension axioms.

Uniform type tagging vs inline type annotations is kind of like closure vs defunctionlaization
A uniform representation / a specliazed one. A closed world of constructors vs an open one.
Rust enums make the closed work convenient and fast.

Higher order equality reasoning. Equalities of equalities. Dijsktra style. Equality patterns.
Actually by adding equality nodes to every other node, we get equality patterns for free.
(eq(a, a)) but of course.
eq(x,y) => eq(z,w) but then we also need to propagate z = w downward into the graph. Hmm.
Mutiple kinds of equivalnce relations interacting?

Actually only is eq(x,y) is in same eclass as true should be propagate it
eq(x,y) = p  is a condition equation upon the value of p. only if we learn p is true should we propagate this info.
ForAll([x,y], (eq(x,y) == True) ==  (x == y) )
ForAll([x,y], (eq(x,y) == True) ==  (x == y) ) vs
ForAll([x,y], eq(x,y) ==  (x == y) ) ? This is still correct in a sense. It's just that I want to trigger on (eq(x,y) == True)
Can i write a trigger like that in Z3? I'm not sure i can

and(x,y) == x == y == or(x,y)
egraph equality, predicate equality, 
or(x,y) == or(y,x)
or() = asssociatve
or(x,x) == x

eq(y, y)
eq(y,x) => eq(x,y)
eq()

definitional and propositional equality.
eq() can be a question, tentative knowledge, however equailvance clases in the egraph are known knowledge.

Mathmeth
https://www.cs.cornell.edu/home/gries/Logic/Equationalhistory.html
https://www.cs.cornell.edu/home/gries/Logic/Equational.html

EGraphs that are string rewriting.
Mark out EClass 1 as special markjer for empty string.

a memoizing trie? DAG trie. Double ended trie. Links going up and down.


Take the technique of my path post but add more concrete objects.
We do I need the types. I keep getting confused.
id_A != id_B. They need to be kept as distinct objects.

Note just because I'm using z3 "Function"s does not mean we are only discussing categories of functions. Z3 uninterpeted functions are pretty much a completely syntactical construct. Well, ok in a sense we are in that vbia a Yoneda embedding really they are natural transformations between HomSet functors.


Well, I could just keep generating new canonical types.

```python
def all_objects():
  a,b,c = "a","b","c"
  for i in range(n):
    # instantiate the laws for these news objects?

```

How are we going to denote otimes? At the symbol level?
otimes( F, G  ???
F and G are natural trasnformations and otimes is a thing that takes natural transformations returns a new natural transformation
otimes(F, G, FG)
otimes( hom(-,A), hom(-,B), hom(-,AB))
otimes( hom(-,A), hom(-,B)) -> hom(-, AB)

add in all the definitions of otimes, and it's axioms

The extra bits of GATs are sort of what is needed to throw into first order logic in order to make categorical constructions more elegant, in particular the partiality of compose (compose requires that morphisms meet on the same object)
Compose can be defined as a mere relation on morphisms in first order logic, but it is clunky. compose is typically a partial function, for which you'll need to enforce extra axioms to cut that subset out of relations. Extra axioms that you need to keep using over and over increases the complexity of both interactive and automatic proof. It is worht considering if there is a way to make something so fundamental baked into your reasoning system rather than just an axiom in the system. This is perhaps the entire study of logical reasoning systems baked in a nutshell. Want to study computation? Well maybe beta reduction should be baked in your reasoning system. Or maybe not. Worth trying.
A sufficiently flexible system can also build a raft of technqiues, encodings and lemmas that the underlying fundamental reasoning system is not particularly relevant.


a variable of sort "hom(-,A)" represents an morphisms that ends on (codomain is) A



How does z3 even encode types to it's egraph. The types and arity just become part of the function symbol?
Yeah, you could intern the whole thing. arity, string and types


2 tricks:
1. Using the morphisms ~ function. Associativty axioms replaced by function composition. 
2. Using Z3 sorts as objects
3. brute force object expansion.


Interesting example: two symbo0ls a b and they commute. Each element of this monoid is isopmorphic to just a pair of 
naturals.
Can we prover (2,2) + (2,2) = (4,4)?



Metatheories ability to canonicalzie dynamically might be very useful.




# A More Naive EGraph

I know that a significant fraction of Julians worship at the shrine of performance. This is not my default shrine. I still feel like most problems I encounter in my hobbies and work are limited by how difficult the are to phrase and solve even naively, and once that is done performance is 9 times out of 10 not an issue. If it is, then it is time to tweak and reconsider. This is of course informed by my problem domains, that I'm rarely trying to build libraries for others, and the fact that I leverage a massive, mind boggling raft of tools built by people who are deeply concerned about such things, and, uncharitably, that my work does not matter to anyone.



Alessandro Cheli (who is a dynamo of energy) has been building a package called Metatheory.jl, which includes a more feature complete and optimized egraph implementation written in Julia than the one I've described in my blog posts. In addition, my understanding is he's trying to take homoiconicity seriously.

So the version of egraph I'm about to describe to you is not my recommended one, unless you're in a time crunch maybe. Nevertheless, because of it's simplcity, I think there is more conceptual clarity to it.








Or flip something out of the trie to put it in the egraph.


https://taliasplse.wordpress.com/2020/02/02/automating-transport-with-univalent-e-graphs/ 
Talia had a blog post?

Egraph notebook from allssadro

https://colab.research.google.com/drive/1tNOQijJqe5tw-Pk9iqd6HHb2abC5aRid?usp=sharing#scrollTo=zikCWjHH14YH

Ed Yang's egraph in python 
https://twitter.com/ezyang/status/1340507843292770306?s=20
https://gist.github.com/ezyang/c3db0e55a7661998c8a66ea8619f1081

Struct
head:
args:
end
 curried in maps.

Term => EClass becomes
Head => (Args => EClass)

What happens if two args becomes unified?
Is that a problem to keep them seperate?


It's kind of nice to not key on something changing under our feet.
It's not so nice that we can't resolve args without checking into the
intdisjointset
Using straight up pointers we could be pretty fast.
No. It's hard to make an indexing structure that respects the equivalence relation

If i used fixed size tuples in an array, 
Then it could be pretty fast in a certain sense.

Dict{:Symbol, Array{  Tuples{Args, Class}  }}

The product equiavlanece class? Yes.


merge(E,E) = just append arrays
normalize(  ) = for each symbol
        double loop check? Insufficient since we merge classes
        if we get a match .

Wait. That normalization is congurence colsure?
If all the arguments are equiavlanet, then the terms itself is equivalent.

0, unary and binary can get you pretty far. Maybe everywhere
With precompilation

struct Egraph
unionfind :: IntDisjointSet
constants :: Dict{Symbol, Vector{ Int64 }}} # IntSet?
unary:: Dict{Symbol, Vector{ Tuple{ Int64, Int64  }  }}
binary:: Dict{Symbol, Vector{ Tuple{ Int64, Int64  }  }}

abtriary?::Dict{ (Symbol,arity Int64)   , Array{Int64, 2}}}
parent_sym?:: Dict{Term, :Symbol}

end


Could keep the vectors weakly sorted?




Metatheroy.jl and alessandro
Matchcore
He's using Expr. Fine. I still don't really see how 
RuntimeGeneratedFunctions.jl
The world age problem - modality?


Perhaps a mistake was trying to match catlab syntax as much as possible
From haskell, I know many type parameters are inferrable.
Inferrable data in some sense should be kept out of the syntax tree. They just gum it up.
I got bit exactly by this when working with a theorem prover whose mechanism was out of my control
Forward reasoning on forall id(A).f => f because it could instantive f with a nonsenical type unless guarded.
I still can't hel but feel my original opinion in my first pose was essentially correct

Catlab annotates more than Haskell does, but it is not fully annotated either
It relies on type inferrence
We could add perhaps inferable annotations to catlab.
We also want a de-elaboration for the rewrite system?
Stupid algo: try to remove parameter, run inferrance. If it comes back something?

I feel like we don't need types. The rewrite rules respect the typing. Preservation.




id(A) the A is unnecassary. there is always an id that works.
Unification

If the translation from Catlab is nontrivial and not general purpose for all GATs, that takes a lot of bite out of working in julia in the first place. I might as well just stick to native rust.




A special patern for assocaitve rules.
Have rerwite rule comp/2( A comp/2(B,C)) => comp/3( A,B,C) # normalizaes associativty


Allow matching for neighboring positions in term
comp/n(... ,  X,Y, ... ) => 


2 obvious tasks
Make pattern matching fast
actually try to apply to catlab or some other domain?


http://taktoa.me/eqsat/Thesis.pdf remy gldschmit's bachleor's eqsat
https://gist.github.com/ezyang/c3db0e55a7661998c8a66ea8619f1081 yang python egg



In an application where we are trying to do equational reasoning, we have some pile of universally quantified equations like $\forall x. x + zero = x$ .
The e-graph is storing ground terms, ones that do not represent a universal quantification. The e-graph stores the fact $seven + zero = seven$ and $two + zero = two$ separately even though these are instantiations of the same underlying fact.

A natural approach to equational rewriting is to turn your equations into rewrite rules, which are a related but distinct thing. Rewrite rules pattern match a left hand side and replace it with a right hand side. 
Rewrite rules have an asymmetric flavor between the right and left hand side, whereas equality is more symmetric.
Applications of rewrite rules do no necessarily commute. Applying rule 1 and then rule 2 is not necessarily the same as applying 2 then 1.
One can then apply in some order the rewrite rules, hoping for the best, or maintain a set of all terms reachable.


SymbolicUtils arranges its matchers to take an expression, a dictionary of already known bindings, and a callback function that takes a new dictionary and the number of elements matched. Here's a snippet from the library for comparison.

[link](https://github.com/JuliaSymbolics/SymbolicUtils.jl/blob/cd18c76fd09078c38393b398e681329f257ecfe8/src/matchers.jl#L1)
```julia

#### Pattern matching
### Matching procedures
# A matcher is a function which takes 3 arguments
# 1. Expression
# 2. Dictionary
# 3. Callback: takes arguments Dictionary  Number of elements matched
#
function matcher(val::Any)
    function literal_matcher(next, data, bindings)
        islist(data) && isequal(car(data), val) ? next(bindings, 1) : nothing
    end
end
```


Related Libs:
* SymbolicUtils
* Match.jl
* MLStyle.jl

Make egraph generic like egg by implementing in terms of a children and istree function.
Kind of how symbiolic utils does it.

end

generic egraph over anything that implements istree yada yada. call getchildren(::T) rather than .args
children!() tpo update children
children
Basically converts it to


$\forall x. x + zero = x$ becomes 

The equations that produce these equivalences

In application to finding new rewrites, we need to be adding new equalities to the EGraph.





Duality
If you reverse all edges, DAGs remain DAGs.
Unification propagates downwards
Congruence closure propagates upwards

What if we reversed the dag and hash consed all the parents? How similar would congruence closure look like to unifacation

Hashcons Ids vs union find ids. "Fundamental" indirections. The catlab people have convinced me that there is some fundamaenetal character to the indirection that occurs via foreign keys inb databases. So should we too consider there to be a fundamental character to the Ids?
memo :: f(Id) -> Id. Set of endofunctors f on Id? 
A unification relation ~ ( term(x) , otherterm(x) ) . The signatures of the two don't have to match, but the variable sets do. Whereas the opposite is true of composition.
 f Id  <-  Id  ->  g Id  ~~~~ g Id2 <- Id2 -> h Id2.
 Pullback gives Id3 and equivalence set of Id1 Id2.
  Maybe consider the hash cons version?   Id <- Id -> Id
  the met parts perform union [ 2,6,8,1]   [2,5,7,9,5,3]
  No it makes more sense going the other way.
    Stuff -> EquivClas <- Stuff   union find
    [1 ,4,5]   
    Then 
function compose_cospan(f,g)
   unionfind = IntDisjointSet( max(f.apex, g.apex))
   for (x,y) in zip( f.right, g.left)
     merge(x,y) 
    end
    newleft = find_root.( f.left)
    NEWRIGHT = find_root(g.right)
    # maybe a normalization step to return to a range 1..Nequiv_class
    CoSpan( newapex, newleft, newright )
end

It does seem like this makes sense. I dunno what you do with it.
I mean, a pushout _is_ union right?
This is in catlab under colimit.
Does this suggest that maybe I should be implementing EGraphs as a CSet?
There is this complicated schema of Equiavlanece nodes and hash Ids.
    [1,56,7,8,  ] 


EGraphs as a CSet
objects:
EClass
FunHash

morphisms:
1 per function symbol
Maybe functors? Takes multiples EClass

congruence closure does feel like some kind of universal property. It's the largest relation under something

If function symbols are morphisms,
then They can be represented as n-d arrays on the available equaivlance classes.
It avoids the need for a hashmap. At the great expense of being able to be less lazy?
We need like a lazy sparse array. That uses 0 to denote uncomputed.
But a hash table is a lazy sparse array
I guess we could have 1 hash per function symbol. Since we always know the head.
Yea, these forms don't seem useful, but it's an interesting persepctive.

A data structure is a lot like a database

I guess the other interesting takeway that one might have is the other direwction
A hashmap can be like an avluator. memo[ f x y z ] = result.
memo[f x y z] ~ curry ~ memo[f][x y z]
So we don't have to make the correspondence morphism = array

So, where to next?
3 roads.

- implemente pattern matching in z3py
- implement pattern matching in julia
- bind to egg

Egg reference the Z3 macthing paper nad the simplify matching paper
"
E-graphs and E-matching. E-graph were originally proposed several decades ago as an efficient
data structure for maintaining congruence closure [Kozen 1977; Nelson 1980; Nelson and Oppen
1980]. E-graphs continue to be a critical component in successful SMT solvers where they are
used for combining satisfiability theories by sharing equality information [De Moura and Bjrner
2008]. A key difference between past implementations of e-graphs and eggs e-graph is our novel
rebuilding algorithm that maintains invariants only at certain critical points (Section 3). This makes
egg more efficient for the purpose of equality saturation. egg implements the pattern compilation
strategy introduced by de Moura et al. [de Moura and Bjrner 2007] that is used in state of the art
theorem provers [De Moura and Bjrner 2008]. Some provers [De Moura and Bjrner 2008; Detlefs
et al. 2005] propose optimizations like mod-time, pattern-element and inverted-path-index to find
new terms and relevant patterns for matching, and avoid redundant matches. So far, we have found
egg to be faster than several prior e-graph implementations even without these optimizations.
They are, however, compatible with eggs design and could be explored in the future. Another key
difference is eggs powerful e-class analysis abstraction and flexible interface. They empower the
programmer to easily leverage e-graphs for problems involving complex semantic reasoning
"

Term Indexing - Chapter 26 of the Handbook of Automated Reasoning
Data strucures - 
 - Trees or dags. Aggressive sharing vs hash cons. Nelson Oppejn 1980
 - Flatterms. flatten out tree into preoder traversal. Possilby with skip pointer
  - Prolog terms


  - automata based
  - Code trees 


String based indexing - idea: convert patterns into approximate string matching pattern

position sdtrings. We can lay out the terms in some sequence, let's say a preorder traversal. In addition can annotate with positions
This does actually remind me of Code2Vec

https://link.springer.com/chapter/10.1007/3-540-62950-5_59 shostak congurence as a completion algorithm

https://www.microsoft.com/en-us/research/wp-content/uploads/2016/12/krml253.pdf - leino pit claudel

It seems like having a slow but interpretable e matcher would be helpful. 


bjorner and de moura good ematching
prolog - warren machine
Avbstract machine
 pc - current instruction ? weird.
 reg[] - storing ground terms
 bstack - backtracking

```haskell
type Symbol = String
data Machine = 
  | Init Symbol Machine
  | Bind Int Symbol 

data State = State {pc :: Int, bstack :: [Machine] , reg :: [Term] }

cont (Init m) = m

run :: Machine -> State -> State
run (Init f ) { r = [Term f args]   } = s { reg = args  }

```



code trees


Path indexing