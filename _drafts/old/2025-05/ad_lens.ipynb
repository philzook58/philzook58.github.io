{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation with a ROP chain\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /tmp/stack.c\n"
     ]
    }
   ],
   "source": [
    "%%file /tmp/stack.c\n",
    "#include <stddef.h>\n",
    "#include <unistd.h>\n",
    "#include <sys/mman.h>\n",
    "#define STACK_SIZE (64 * 1024)\n",
    "#define PAGE_SIZE  sysconf(_SC_PAGESIZE)\n",
    "\n",
    "void *original_rsp;\n",
    "void *stack_top;\n",
    "/*\n",
    "// double *dz;   \n",
    "typedef struct rev_dual {\n",
    "    double x; // result\n",
    "    double* dx; // place to put reverse mode gradients.\n",
    "} rev_dual \n",
    "\n",
    "noinline // and maybe this needs a calling convention. Save none?\n",
    "void returnfromforward(res){\n",
    "    // restore normal stack\n",
    "    asm volatile(\"mov %%rsp, %0\" : \"=r\"(stacktop_rsp));\n",
    "    asm volatile(\"mov %0, %%rsp\" :: \"r\"(original_rsp));\n",
    "    return res;\n",
    "}\n",
    "\n",
    "void add(rev_dual x, rev_dual y){\n",
    "    asm volatile(\"mov %%rsp, %0\" : \"=r\"(original_rsp));\n",
    "    asm volatile (\"mov %0, %%rsp\\n\" :: \"r\"(stack_top));\n",
    "    dz = alloca(sizeof(double));\n",
    "    *dz = 0;\n",
    "    saved = alloca(sizeof(double*));\n",
    "    saved[0] = dx;\n",
    "    saved[1] = dy;\n",
    "\n",
    "    // call next forward. Doesn't return normally\n",
    "\n",
    "    // maybe I need to put stack swaps in here.\n",
    "    saved = RETURNFORWARD(x + y, dz);\n",
    "    // I mean really this is a closure I guess.\n",
    "    // do backprop\n",
    "    // hmm. it expects dx and dy to be in registers?\n",
    "    // hmm by but now saved might be clobbered. Wellllll, maybe if it isn't assumed in a reg it statically knows.\n",
    "    dx = saved[0];\n",
    "    dy = saved[1];\n",
    "    *dx += *dz\n",
    "    *dy += *dz\n",
    "    return\n",
    "}\n",
    "*/\n",
    "\n",
    "int main() {\n",
    "    // you should be doing some checking here for failure, but fuck it. All of this is awful\n",
    "    void *stack = mmap(NULL, STACK_SIZE + PAGE_SIZE, \n",
    "                       PROT_READ | PROT_WRITE,\n",
    "                       MAP_PRIVATE | MAP_ANONYMOUS, -1, 0);\n",
    "    mprotect(stack, PAGE_SIZE, PROT_NONE);\n",
    "     // Stack grows downward, so stack top is at the *end*\n",
    "    *stack_top = (char *)stack + STACK_SIZE + PAGE_SIZE;\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gcc -Wall -O2 -g -Wextra -fanalyzer /tmp/stack.c -o /tmp/stack && /tmp/stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00000000000010a0 <main>:\n",
      "    *dx += *dz\n",
      "    *dy += *dz\n",
      "}\n",
      "*/\n",
      "\n",
      "int main() {\n",
      "    10a0:\tf3 0f 1e fa          \tendbr64\n",
      "    10a4:\t53                   \tpush   %rbx\n",
      "    // you should be doing some checking here for failure, but fuck it.\n",
      "    void *stack = mmap(NULL, STACK_SIZE + PAGE_SIZE, \n",
      "    10a5:\tbf 1e 00 00 00       \tmov    $0x1e,%edi\n",
      "    10aa:\te8 e1 ff ff ff       \tcall   1090 <sysconf@plt>\n",
      "    10af:\t45 31 c9             \txor    %r9d,%r9d\n",
      "    10b2:\t41 b8 ff ff ff ff    \tmov    $0xffffffff,%r8d\n",
      "    10b8:\t31 ff                \txor    %edi,%edi\n",
      "    10ba:\tb9 22 00 00 00       \tmov    $0x22,%ecx\n",
      "    10bf:\t48 8d b0 00 00 01 00 \tlea    0x10000(%rax),%rsi\n",
      "    10c6:\tba 03 00 00 00       \tmov    $0x3,%edx\n",
      "    10cb:\te8 a0 ff ff ff       \tcall   1070 <mmap@plt>\n",
      "                       PROT_READ | PROT_WRITE,\n",
      "                       MAP_PRIVATE | MAP_ANONYMOUS, -1, 0);\n",
      "    mprotect(stack, PAGE_SIZE, PROT_NONE);\n",
      "    10d0:\tbf 1e 00 00 00       \tmov    $0x1e,%edi\n",
      "    void *stack = mmap(NULL, STACK_SIZE + PAGE_SIZE, \n",
      "    10d5:\t48 89 c3             \tmov    %rax,%rbx\n",
      "    mprotect(stack, PAGE_SIZE, PROT_NONE);\n",
      "    10d8:\te8 b3 ff ff ff       \tcall   1090 <sysconf@plt>\n",
      "    10dd:\t31 d2                \txor    %edx,%edx\n",
      "    10df:\t48 89 df             \tmov    %rbx,%rdi\n",
      "    10e2:\t48 89 c6             \tmov    %rax,%rsi\n",
      "    10e5:\te8 96 ff ff ff       \tcall   1080 <mprotect@plt>\n",
      "     // Stack grows downward, so stack top is at the *end*\n",
      "    void *stack_top = (char *)stack + STACK_SIZE + PAGE_SIZE;\n",
      "    10ea:\tbf 1e 00 00 00       \tmov    $0x1e,%edi\n",
      "    10ef:\te8 9c ff ff ff       \tcall   1090 <sysconf@plt>\n",
      "    void *original_rsp;\n",
      "    asm volatile(\"mov %%rsp, %0\" : \"=r\"(original_rsp));\n",
      "    10f4:\t48 89 e2             \tmov    %rsp,%rdx\n",
      "    void *stack_top = (char *)stack + STACK_SIZE + PAGE_SIZE;\n",
      "    10f7:\t48 8d 84 03 00 00 01 \tlea    0x10000(%rbx,%rax,1),%rax\n",
      "    10fe:\t00 \n",
      "    asm volatile (\"mov %0, %%rsp\\n\" :: \"r\"(stack_top));\n",
      "    10ff:\t48 89 c4             \tmov    %rax,%rsp\n",
      "\n",
      "    asm volatile(\"mov %0, %%rsp\" :: \"r\"(original_rsp)); // restore stack\n",
      "    1102:\t48 89 d4             \tmov    %rdx,%rsp\n",
      "\n",
      "}\n",
      "    1105:\t31 c0                \txor    %eax,%eax\n",
      "    1107:\t5b                   \tpop    %rbx\n"
     ]
    }
   ],
   "source": [
    "! objdump -dS  /tmp/stack | grep -A 50 \"<main>:\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using pytorch graph to achieve other backprop like things.\n",
    "\n",
    "wengert tapes using ROP. forth interpreter techniques.\n",
    "\n",
    "https://rufflewind.com/2016-12-30/reverse-mode-automatic-differentiation\n",
    "\n",
    "https://www.jmlr.org/papers/v18/17-468.html Automatic Differentiation in Machine Learning: a Survey\n",
    "https://arxiv.org/pdf/1811.05031 A Review of Automatic Differentiation\n",
    "and its Efficient Implementation\n",
    "https://www.autodiff.org/\n",
    "\n",
    "checkpointing\n",
    "region based memory management\n",
    "\n",
    "Composing the jacobian vs the transpose jacobian.\n",
    "\n",
    "We can take little displacements and push them into the codomain.\n",
    "Or we can take linear functionals and pull them into functionals of the codomain\n",
    "\n",
    "Connection to exact reals.\n",
    "\n",
    "Higher Order via chebfun techniques.\n",
    "Functional integeration via chebfun techniques. chebfun over pytorch values? https://pytorch.org/cppdocs/api/function_namespacetorch_1_1special_1aa8631344adab4a1cf8f70c6920732cba.html\n",
    "Differentiation of functional integeration via chebfun techniques.\n",
    "\n",
    "Higher order based on sampling. f(x) + f(1) +... under the assumption the only thing you can do to functions is eval them.\n",
    "But chebfun enables integ, diff, min, etc.\n",
    "https://github.com/chebpy/chebpy/blob/master/docs/notebook-implementation-notes.ipynb\n",
    "\n",
    "Symbolic and automatic aren't _that_ different. it's a difference of shallowness, initial vs final, meta vs object.\n",
    "\n",
    "http://conal.net/papers/higher-order-ad/higher-order-ad.pdf\n",
    "https://openreview.net/pdf?id=ryxuz9SzDB the differentiable curry\n",
    "\n",
    "https://hackage.haskell.org/package/ad-delcont continuations for ad\n",
    "\n",
    "http://blog.ezyang.com/2019/05/pytorch-internals/ \n",
    "https://pytorch.org/blog/overview-of-pytorch-autograd-engine/\n",
    "https://pytorch.org/blog/computational-graphs-constructed-in-pytorch/\n",
    "\n",
    "verilog ad\n",
    "\n",
    "\n",
    "\n",
    "pytroch fx ir https://arxiv.org/abs/2112.08429\n",
    "https://dl.acm.org/doi/abs/10.1145/3620665.3640366 Pytorch 2: Faster machine learning through dynamic python bytecode transformation and graph compilation\n",
    "\n",
    "\"No. One (torch.jit.trace) produces a (legacy) TorchScript representation while torch.fx.symbolic_trace gives you a shiny new FX one. (But if you ask, maybe torch.compile is the function you actually wantâ€¦)\"\n",
    "\n",
    "https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html\n",
    "\n",
    "triton\n",
    "https://github.com/gpu-mode/lectures/blob/main/lecture_014/A_Practitioners_Guide_to_Triton.ipynb\n",
    "https://triton-lang.org/main/index.html\n",
    "https://openai.com/index/triton/\n",
    "\n",
    "inductor\n",
    "https://peps.python.org/pep-0523/ frame evaluator api. torchdynamo\n",
    "\n",
    "\n",
    "https://github.com/NVIDIA/cutlass cuda templates for linear alg\n",
    "\n",
    "\n",
    "\n",
    "https://pytorch.org/docs/main/torch.compiler_dynamo_deepdive.html\n",
    "dynamo is a serious effort to build a reflector in python...\n",
    "also triton I guess.\n",
    "also numba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/compile.py\n"
     ]
    }
   ],
   "source": [
    "%%file /tmp/compile.py\n",
    "import torch\n",
    "\n",
    "@torch.compile\n",
    "def mse(x, y):\n",
    "    z = (x - y) ** 2\n",
    "    return z.sum()\n",
    "\n",
    "x = torch.randn(200)\n",
    "y = torch.randn(200)\n",
    "mse(x, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V1226 23:31:40.579000 2317012 torch/fx/passes/runtime_assert.py:114] [0/0] [__graph_code] TRACED GRAPH\n",
      "V1226 23:31:40.579000 2317012 torch/fx/passes/runtime_assert.py:114] [0/0] [__graph_code]  ===== pre insert_deferred_runtime_asserts __compiled_fn_1 =====\n",
      "V1226 23:31:40.579000 2317012 torch/fx/passes/runtime_assert.py:114] [0/0] [__graph_code]  <eval_with_key>.0 class GraphModule(torch.nn.Module):\n",
      "V1226 23:31:40.579000 2317012 torch/fx/passes/runtime_assert.py:114] [0/0] [__graph_code]     def forward(self, L_x_: \"\u001b[31mf32\u001b[0m\u001b[34m[200]\u001b[0m\u001b[2m\u001b[34m\u001b[0m\u001b[2m\u001b[32m\u001b[0m\", L_y_: \"\u001b[31mf32\u001b[0m\u001b[34m[200]\u001b[0m\u001b[2m\u001b[34m\u001b[0m\u001b[2m\u001b[32m\u001b[0m\"):\n",
      "V1226 23:31:40.579000 2317012 torch/fx/passes/runtime_assert.py:114] [0/0] [__graph_code]         l_x_ = L_x_\n",
      "V1226 23:31:40.579000 2317012 torch/fx/passes/runtime_assert.py:114] [0/0] [__graph_code]         l_y_ = L_y_\n",
      "V1226 23:31:40.579000 2317012 torch/fx/passes/runtime_assert.py:114] [0/0] [__graph_code]         \n",
      "V1226 23:31:40.579000 2317012 torch/fx/passes/runtime_assert.py:114] [0/0] [__graph_code]          \u001b[2m# File: /tmp/compile.py:5 in mse, code: z = (x - y) ** 2\u001b[0m\n",
      "V1226 23:31:40.579000 2317012 torch/fx/passes/runtime_assert.py:114] [0/0] [__graph_code]         sub: \"\u001b[31mf32\u001b[0m\u001b[34m[200]\u001b[0m\u001b[2m\u001b[34m\u001b[0m\u001b[2m\u001b[32m\u001b[0m\" = l_x_ - l_y_;  \u001b[2ml_x_ = l_y_ = None\u001b[0m\n",
      "V1226 23:31:40.579000 2317012 torch/fx/passes/runtime_assert.py:114] [0/0] [__graph_code]         z: \"\u001b[31mf32\u001b[0m\u001b[34m[200]\u001b[0m\u001b[2m\u001b[34m\u001b[0m\u001b[2m\u001b[32m\u001b[0m\" = sub ** \u001b[34m2\u001b[0m;  \u001b[2msub = None\u001b[0m\n",
      "V1226 23:31:40.579000 2317012 torch/fx/passes/runtime_assert.py:114] [0/0] [__graph_code]         \n",
      "V1226 23:31:40.579000 2317012 torch/fx/passes/runtime_assert.py:114] [0/0] [__graph_code]          \u001b[2m# File: /tmp/compile.py:6 in mse, code: return z.sum()\u001b[0m\n",
      "V1226 23:31:40.579000 2317012 torch/fx/passes/runtime_assert.py:114] [0/0] [__graph_code]         sum_1: \"\u001b[31mf32\u001b[0m\u001b[34m[]\u001b[0m\u001b[2m\u001b[34m\u001b[0m\u001b[2m\u001b[32m\u001b[0m\" = z.sum();  \u001b[2mz = None\u001b[0m\n",
      "V1226 23:31:40.579000 2317012 torch/fx/passes/runtime_assert.py:114] [0/0] [__graph_code]         return (sum_1,)\n",
      "V1226 23:31:40.579000 2317012 torch/fx/passes/runtime_assert.py:114] [0/0] [__graph_code]         \n",
      "V1226 23:31:40.579000 2317012 torch/fx/passes/runtime_assert.py:114] [0/0] [__graph_code] \n",
      "V1226 23:31:40.580000 2317012 torch/_dynamo/output_graph.py:1340] [0/0] [__graph_code] TRACED GRAPH\n",
      "V1226 23:31:40.580000 2317012 torch/_dynamo/output_graph.py:1340] [0/0] [__graph_code]  ===== __compiled_fn_1 =====\n",
      "V1226 23:31:40.580000 2317012 torch/_dynamo/output_graph.py:1340] [0/0] [__graph_code]  /home/philip/.local/lib/python3.10/site-packages/torch/fx/_lazy_graph_module.py class GraphModule(torch.nn.Module):\n",
      "V1226 23:31:40.580000 2317012 torch/_dynamo/output_graph.py:1340] [0/0] [__graph_code]     def forward(self, L_x_: \"\u001b[31mf32\u001b[0m\u001b[34m[200]\u001b[0m\u001b[2m\u001b[34m[1]\u001b[0m\u001b[2m\u001b[32mcpu\u001b[0m\", L_y_: \"\u001b[31mf32\u001b[0m\u001b[34m[200]\u001b[0m\u001b[2m\u001b[34m[1]\u001b[0m\u001b[2m\u001b[32mcpu\u001b[0m\"):\n",
      "V1226 23:31:40.580000 2317012 torch/_dynamo/output_graph.py:1340] [0/0] [__graph_code]         l_x_ = L_x_\n",
      "V1226 23:31:40.580000 2317012 torch/_dynamo/output_graph.py:1340] [0/0] [__graph_code]         l_y_ = L_y_\n",
      "V1226 23:31:40.580000 2317012 torch/_dynamo/output_graph.py:1340] [0/0] [__graph_code]         \n",
      "V1226 23:31:40.580000 2317012 torch/_dynamo/output_graph.py:1340] [0/0] [__graph_code]          \u001b[2m# File: /tmp/compile.py:5 in mse, code: z = (x - y) ** 2\u001b[0m\n",
      "V1226 23:31:40.580000 2317012 torch/_dynamo/output_graph.py:1340] [0/0] [__graph_code]         sub: \"\u001b[31mf32\u001b[0m\u001b[34m[200]\u001b[0m\u001b[2m\u001b[34m[1]\u001b[0m\u001b[2m\u001b[32mcpu\u001b[0m\" = l_x_ - l_y_;  \u001b[2ml_x_ = l_y_ = None\u001b[0m\n",
      "V1226 23:31:40.580000 2317012 torch/_dynamo/output_graph.py:1340] [0/0] [__graph_code]         z: \"\u001b[31mf32\u001b[0m\u001b[34m[200]\u001b[0m\u001b[2m\u001b[34m[1]\u001b[0m\u001b[2m\u001b[32mcpu\u001b[0m\" = sub ** \u001b[34m2\u001b[0m;  \u001b[2msub = None\u001b[0m\n",
      "V1226 23:31:40.580000 2317012 torch/_dynamo/output_graph.py:1340] [0/0] [__graph_code]         \n",
      "V1226 23:31:40.580000 2317012 torch/_dynamo/output_graph.py:1340] [0/0] [__graph_code]          \u001b[2m# File: /tmp/compile.py:6 in mse, code: return z.sum()\u001b[0m\n",
      "V1226 23:31:40.580000 2317012 torch/_dynamo/output_graph.py:1340] [0/0] [__graph_code]         sum_1: \"\u001b[31mf32\u001b[0m\u001b[34m[]\u001b[0m\u001b[2m\u001b[34m[]\u001b[0m\u001b[2m\u001b[32mcpu\u001b[0m\" = z.sum();  \u001b[2mz = None\u001b[0m\n",
      "V1226 23:31:40.580000 2317012 torch/_dynamo/output_graph.py:1340] [0/0] [__graph_code]         return (sum_1,)\n",
      "V1226 23:31:40.580000 2317012 torch/_dynamo/output_graph.py:1340] [0/0] [__graph_code]         \n",
      "V1226 23:31:40.580000 2317012 torch/_dynamo/output_graph.py:1340] [0/0] [__graph_code] \n"
     ]
    }
   ],
   "source": [
    "!TORCH_LOGS=graph_code python /tmp/compile.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TYPE_CHECKING': False,\n",
       " 'enable_auto_functionalized_v2': False,\n",
       " 'debug': False,\n",
       " 'disable_progress': True,\n",
       " 'verbose_progress': False,\n",
       " 'fx_graph_cache': True,\n",
       " 'fx_graph_remote_cache': None,\n",
       " 'autotune_local_cache': True,\n",
       " 'autotune_remote_cache': None,\n",
       " 'force_disable_caches': False,\n",
       " 'sleep_sec_TESTING_ONLY': None,\n",
       " 'custom_op_default_layout_constraint': 'flexible_layout',\n",
       " 'cpp_wrapper': False,\n",
       " 'abi_compatible': False,\n",
       " 'c_shim_version': '2',\n",
       " 'dce': False,\n",
       " 'static_weight_shapes': True,\n",
       " 'size_asserts': True,\n",
       " 'nan_asserts': False,\n",
       " 'pick_loop_orders': True,\n",
       " 'inplace_buffers': True,\n",
       " 'allow_buffer_reuse': True,\n",
       " 'memory_planning': False,\n",
       " 'memory_pool': 'intermediates',\n",
       " 'benchmark_harness': True,\n",
       " 'epilogue_fusion': True,\n",
       " 'epilogue_fusion_first': False,\n",
       " 'pattern_matcher': True,\n",
       " 'b2b_gemm_pass': False,\n",
       " 'post_grad_custom_pre_pass': None,\n",
       " 'post_grad_custom_post_pass': None,\n",
       " 'joint_custom_pre_pass': None,\n",
       " 'joint_custom_post_pass': None,\n",
       " 'pre_grad_custom_pass': None,\n",
       " '_pre_fusion_custom_pass': None,\n",
       " 'split_cat_fx_passes': True,\n",
       " 'efficient_conv_bn_eval_fx_passes': False,\n",
       " 'is_predispatch': False,\n",
       " 'group_fusion': False,\n",
       " 'batch_fusion': True,\n",
       " 'pre_grad_fusion_options': {'batch_linear': {},\n",
       "  'batch_linear_lhs': {},\n",
       "  'batch_layernorm': {},\n",
       "  'batch_tanh': {},\n",
       "  'batch_relu': {},\n",
       "  'batch_sigmoid': {}},\n",
       " 'post_grad_fusion_options': {},\n",
       " 'reorder_for_locality': True,\n",
       " 'dynamic_scale_rblock': True,\n",
       " 'force_fuse_int_mm_with_mul': False,\n",
       " 'use_mixed_mm': True,\n",
       " 'fx_passes_numeric_check': {'pre_grad': False,\n",
       "  'precision': 0.0001,\n",
       "  'num_iterations': 1,\n",
       "  'requires_optimizer': True},\n",
       " 'mixed_mm_choice': 'heuristic',\n",
       " 'reorder_for_compute_comm_overlap': False,\n",
       " 'reorder_for_compute_comm_overlap_passes': ['reorder_compute_for_overlap',\n",
       "  'sink_waits',\n",
       "  'raise_comms'],\n",
       " 'estimate_op_runtime': 'default',\n",
       " 'intra_node_bw': 300,\n",
       " 'inter_node_bw': 25,\n",
       " 'max_autotune': False,\n",
       " 'max_autotune_pointwise': False,\n",
       " 'max_autotune_gemm': False,\n",
       " 'force_same_precision': False,\n",
       " 'max_autotune_gemm_backends': 'ATEN,TRITON,CPP',\n",
       " 'max_autotune_conv_backends': 'ATEN,TRITON',\n",
       " 'max_autotune_gemm_search_space': 'DEFAULT',\n",
       " 'autotune_fallback_to_aten': True,\n",
       " 'unbacked_symint_fallback': 8192,\n",
       " 'search_autotune_cache': False,\n",
       " 'save_args': False,\n",
       " 'autotune_in_subproc': False,\n",
       " 'max_autotune_subproc_result_timeout_seconds': 60.0,\n",
       " 'max_autotune_subproc_graceful_timeout_seconds': 1.0,\n",
       " 'max_autotune_subproc_terminate_timeout_seconds': 2.0,\n",
       " 'autotune_multi_device': False,\n",
       " 'coordinate_descent_tuning': False,\n",
       " 'coordinate_descent_check_all_directions': False,\n",
       " 'coordinate_descent_search_radius': 1,\n",
       " 'autoheuristic_collect': '',\n",
       " 'autoheuristic_use': 'mixed_mm',\n",
       " 'autoheuristic_log_path': 'DEFAULT',\n",
       " 'layout_opt_default': '1',\n",
       " 'layout_optimization': True,\n",
       " 'force_layout_optimization': False,\n",
       " 'keep_output_stride': True,\n",
       " 'warn_mix_layout': False,\n",
       " 'realize_reads_threshold': 4,\n",
       " 'realize_opcount_threshold': 30,\n",
       " 'realize_acc_reads_threshold': 8,\n",
       " 'fallback_random': False,\n",
       " 'implicit_fallbacks': True,\n",
       " 'aggressive_fusion': False,\n",
       " 'debug_fusion': False,\n",
       " 'benchmark_fusion': False,\n",
       " 'enabled_metric_tables': '',\n",
       " 'loop_ordering_after_fusion': False,\n",
       " 'benchmark_epilogue_fusion': True,\n",
       " 'max_epilogue_benchmarked_choices': 1,\n",
       " 'max_fusion_size': 64,\n",
       " 'max_pointwise_cat_inputs': 8,\n",
       " 'unroll_reductions_threshold': 8,\n",
       " 'comment_origin': False,\n",
       " 'conv_1x1_as_mm': False,\n",
       " 'split_reductions': True,\n",
       " 'benchmark_kernel': False,\n",
       " 'constant_and_index_propagation': True,\n",
       " 'always_keep_tensor_constants': False,\n",
       " 'assert_indirect_indexing': True,\n",
       " 'compute_all_bounds': False,\n",
       " 'combo_kernels': False,\n",
       " 'benchmark_combo_kernel': False,\n",
       " 'combo_kernels_autotune': 1,\n",
       " 'combo_kernel_allow_mixed_sizes': 1,\n",
       " 'combo_kernel_foreach_dynamic_shapes': False,\n",
       " 'joint_graph_constant_folding': True,\n",
       " 'debug_index_asserts': False,\n",
       " 'emulate_precision_casts': False,\n",
       " 'is_nightly_or_source': False,\n",
       " 'developer_warnings': False,\n",
       " 'optimize_scatter_upon_const_tensor': True,\n",
       " 'worker_start_method': 'subprocess',\n",
       " '_fuse_ddp_communication': False,\n",
       " '_fuse_ddp_bucket_size': 25,\n",
       " '_fuse_ddp_communication_passes': ['fuse_ddp_with_concat_op',\n",
       "  'schedule_comm_wait'],\n",
       " '_micro_pipeline_tp': False,\n",
       " 'compile_threads': 16,\n",
       " 'global_cache_dir': None,\n",
       " 'kernel_name_max_ops': 10,\n",
       " 'shape_padding': True,\n",
       " 'comprehensive_padding': True,\n",
       " 'pad_channels_last': False,\n",
       " 'disable_padding_cpu': True,\n",
       " 'padding_alignment_bytes': 128,\n",
       " 'padding_stride_threshold': 1024,\n",
       " 'pad_outputs': False,\n",
       " 'bw_outputs_user_visible': True,\n",
       " 'force_shape_pad': False,\n",
       " 'permute_fusion': False,\n",
       " 'profiler_mark_wrapper_call': False,\n",
       " 'generate_intermediate_hooks': False,\n",
       " 'debug_ir_traceback': False,\n",
       " '_raise_error_for_testing': False,\n",
       " '_profile_var': '',\n",
       " 'profile_bandwidth': False,\n",
       " 'profile_bandwidth_regex': '',\n",
       " 'profile_bandwidth_output': None,\n",
       " 'profile_bandwidth_with_do_bench_using_profiling': False,\n",
       " 'disable_cpp_codegen': False,\n",
       " 'freezing': False,\n",
       " 'freezing_discard_parameters': False,\n",
       " 'allow_stack_allocation': False,\n",
       " 'use_minimal_arrayref_interface': False,\n",
       " 'decompose_mem_bound_mm': False,\n",
       " 'assume_aligned_inputs': False,\n",
       " 'unsafe_ignore_unsupported_triton_autotune_args': False,\n",
       " 'check_stack_no_cycles_TESTING_ONLY': False,\n",
       " 'cpp.threads': -1,\n",
       " 'cpp.no_redundant_loops': True,\n",
       " 'cpp.dynamic_threads': False,\n",
       " 'cpp.simdlen': None,\n",
       " 'cpp.min_chunk_size': 4096,\n",
       " 'cpp.cxx': (None, 'g++'),\n",
       " 'cpp.enable_kernel_profile': False,\n",
       " 'cpp.weight_prepack': True,\n",
       " 'cpp.inject_relu_bug_TESTING_ONLY': None,\n",
       " 'cpp.inject_log1p_bug_TESTING_ONLY': None,\n",
       " 'cpp.vec_isa_ok': None,\n",
       " 'cpp.descriptive_names': 'original_aten',\n",
       " 'cpp.max_horizontal_fusion_size': 16,\n",
       " 'cpp.fallback_scatter_reduce_sum': True,\n",
       " 'cpp.enable_unsafe_math_opt_flag': False,\n",
       " 'cpp.enable_floating_point_contract_flag': False,\n",
       " 'cpp.enable_tiling_heuristics': True,\n",
       " 'cpp.gemm_max_k_slices': 1,\n",
       " 'cpp.gemm_cache_blocking': None,\n",
       " 'cpp.gemm_thread_factors': None,\n",
       " 'cpp.enable_loop_tail_vec': True,\n",
       " 'triton.cudagraphs': False,\n",
       " 'triton.cudagraph_trees': True,\n",
       " 'triton.cudagraph_skip_dynamic_graphs': False,\n",
       " 'triton.slow_path_cudagraph_asserts': True,\n",
       " 'triton.cudagraph_trees_history_recording': False,\n",
       " 'triton.cudagraph_support_input_mutation': True,\n",
       " 'triton.cudagraph_unexpected_rerecord_limit': 128,\n",
       " 'triton.cudagraph_dynamic_shape_warn_limit': 50,\n",
       " 'triton.force_cudagraph_sync': False,\n",
       " 'triton.force_cudagraphs_warmup': False,\n",
       " 'triton.fast_path_cudagraph_asserts': False,\n",
       " 'triton.skip_cudagraph_warmup': False,\n",
       " 'triton.debug_sync_graph': False,\n",
       " 'triton.debug_sync_kernel': False,\n",
       " 'triton.dense_indexing': False,\n",
       " 'triton.max_tiles': 2,\n",
       " 'triton.prefer_nd_tiling': False,\n",
       " 'triton.autotune_pointwise': True,\n",
       " 'triton.autotune_cublasLt': True,\n",
       " 'triton.autotune_at_compile_time': False,\n",
       " 'triton.tiling_prevents_pointwise_fusion': True,\n",
       " 'triton.tiling_prevents_reduction_fusion': True,\n",
       " 'triton.unique_kernel_names': False,\n",
       " 'triton.descriptive_names': 'original_aten',\n",
       " 'triton.persistent_reductions': True,\n",
       " 'triton.multi_kernel': 0,\n",
       " 'triton.divisible_by_16': True,\n",
       " 'triton.min_split_scan_rblock': 256,\n",
       " 'triton.store_cubin': False,\n",
       " 'triton.spill_threshold': 16,\n",
       " 'triton.use_block_ptr': False,\n",
       " 'triton.inject_relu_bug_TESTING_ONLY': None,\n",
       " 'triton.codegen_upcast_to_fp32': True,\n",
       " 'aot_inductor.output_path': '',\n",
       " 'aot_inductor.debug_compile': False,\n",
       " 'aot_inductor.debug_dump_consts_bin': False,\n",
       " 'aot_inductor.debug_intermediate_value_printer': '0',\n",
       " 'aot_inductor.filtered_kernel_names': None,\n",
       " 'aot_inductor.serialized_in_spec': '',\n",
       " 'aot_inductor.serialized_out_spec': '',\n",
       " 'aot_inductor.use_runtime_constant_folding': False,\n",
       " 'aot_inductor.force_mmap_weights': False,\n",
       " 'aot_inductor.package': False,\n",
       " 'cuda.arch': None,\n",
       " 'cuda.version': None,\n",
       " 'cuda.compile_opt_level': '-O1',\n",
       " 'cuda.enable_cuda_lto': False,\n",
       " 'cuda.enable_ptxas_info': False,\n",
       " 'cuda.enable_debug_info': False,\n",
       " 'cuda.use_fast_math': False,\n",
       " 'cuda.cutlass_dir': '/home/philip/.local/lib/python3.10/site-packages/third_party/cutlass',\n",
       " 'cuda.cutlass_max_profiling_configs': None,\n",
       " 'cuda.cuda_cxx': None,\n",
       " 'cuda.cutlass_backend_min_gemm_size': 1,\n",
       " 'cuda.generate_test_runner': True,\n",
       " 'cuda.cutlass_op_allowlist_regex': None,\n",
       " 'cuda.cutlass_op_denylist_regex': 'pingpong',\n",
       " 'rocm.arch': [],\n",
       " 'rocm.ck_supported_arch': ['gfx90a', 'gfx940', 'gfx941', 'gfx942'],\n",
       " 'rocm.compile_opt_level': '-O2',\n",
       " 'rocm.is_debug': False,\n",
       " 'rocm.save_temps': False,\n",
       " 'rocm.use_fast_math': True,\n",
       " 'rocm.flush_denormals': True,\n",
       " 'rocm.print_kernel_resource_usage': False,\n",
       " 'rocm.rocm_home': None,\n",
       " 'rocm.ck_dir': None,\n",
       " 'rocm.n_max_profiling_configs': None,\n",
       " 'rocm.use_preselected_instances': False,\n",
       " 'cpu_backend': 'cpp',\n",
       " 'cuda_backend': 'triton',\n",
       " 'halide.cpu_target': 'host',\n",
       " 'halide.gpu_target': 'host-cuda',\n",
       " 'halide.scheduler_cuda': 'Anderson2021',\n",
       " 'halide.scheduler_cpu': 'Adams2019',\n",
       " 'halide.asserts': False,\n",
       " 'halide.debug': False,\n",
       " 'halide.scan_kernels': False,\n",
       " 'trace.enabled': False,\n",
       " 'trace.debug_dir': None,\n",
       " 'trace.debug_log': False,\n",
       " 'trace.info_log': False,\n",
       " 'trace.fx_graph': True,\n",
       " 'trace.fx_graph_transformed': True,\n",
       " 'trace.ir_pre_fusion': True,\n",
       " 'trace.ir_post_fusion': True,\n",
       " 'trace.output_code': True,\n",
       " 'trace.graph_diagram': False,\n",
       " 'trace.draw_orig_fx_graph': False,\n",
       " 'trace.dot_graph_shape': None,\n",
       " 'trace.log_url_for_graph_xform': None,\n",
       " 'trace.compile_profile': False,\n",
       " 'trace.upload_tar': None,\n",
       " 'trace.log_autotuning_results': False,\n",
       " '_save_config_ignore': ['trace.upload_tar',\n",
       "  'post_grad_custom_post_pass',\n",
       "  'post_grad_custom_pre_pass',\n",
       "  'joint_custom_pre_pass',\n",
       "  'joint_custom_post_pass',\n",
       "  'pre_grad_custom_pass'],\n",
       " '_cache_config_ignore_prefix': ['trace',\n",
       "  'cuda.cutlass_dir',\n",
       "  'compile_threads']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "def foo(x, y):\n",
    "    a = torch.sin(x)\n",
    "    b = torch.cos(y)\n",
    "    return a + b\n",
    "opt_foo1 = torch.compile(foo)\n",
    "#print(opt_foo1(torch.randn(10, 10), torch.randn(10, 10)))\n",
    "opt_foo1.get_compiler_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<AddBackward0 at 0x781e3eb38dc0>, 0)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Example computation\n",
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "y = (x + 2)**2\n",
    "y\n",
    "y.grad_fn.next_functions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "graph(%x : Float(3, strides=[1], requires_grad=0, device=cpu),\n",
       "      %y : Float(3, strides=[1], requires_grad=0, device=cpu)):\n",
       "  %2 : Long(requires_grad=0, device=cpu) = prim::Constant[value={2}]() # /tmp/ipykernel_2198523/702319095.py:2:0\n",
       "  %3 : Float(3, strides=[1], requires_grad=0, device=cpu) = aten::mul(%x, %2) # /tmp/ipykernel_2198523/702319095.py:2:0\n",
       "  %4 : int = prim::Constant[value=1]() # /tmp/ipykernel_2198523/702319095.py:2:0\n",
       "  %5 : Float(3, strides=[1], requires_grad=0, device=cpu) = aten::add(%3, %y, %4) # /tmp/ipykernel_2198523/702319095.py:2:0\n",
       "  return (%5)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def foo(x, y):\n",
    "    return 2 * x + y\n",
    "\n",
    "# Run `foo` with the provided inputs and record the tensor operations\n",
    "traced_foo = torch.jit.trace(foo, (torch.rand(3), torch.rand(3)))\n",
    "traced_foo.graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def foo(x: Tensor,\\n    y: Tensor) -> Tensor:\\n  _0 = torch.add(torch.mul(x, CONSTANTS.c0), y)\\n  return _0\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traced_foo.code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/docs/main/fx.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%2 : Long(requires_grad=0, device=cpu) = prim::Constant[value={2}]() # /tmp/ipykernel_1367663/3114623804.py:2:0\n",
      "\n",
      "%3 : Float(3, strides=[1], requires_grad=0, device=cpu) = aten::mul(%x, %2) # /tmp/ipykernel_1367663/3114623804.py:2:0\n",
      "\n",
      "%4 : int = prim::Constant[value=1]() # /tmp/ipykernel_1367663/3114623804.py:2:0\n",
      "\n",
      "%5 : Float(3, strides=[1], requires_grad=0, device=cpu) = aten::add(%3, %y, %4) # /tmp/ipykernel_1367663/3114623804.py:2:0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n in traced_foo.graph.nodes():\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kata:\n",
    "Z partition function of a string\n",
    "Action of quantum particle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import *\n",
    "xn,xs,hbar = symbols('xn xs hbar')\n",
    "\n",
    "exp(-(x - xn)**2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Expr():\n",
    "    def __init__(self, value, backward):\n",
    "        self.value = value\n",
    "        self.dx = 0\n",
    "        self.backward = backward\n",
    "        self.children = children\n",
    "    def clear():\n",
    "        self.dx = 0\n",
    "\n",
    "    def __add__(self, other):\n",
    "        def back(dx):\n",
    "            self.dx += dx\n",
    "            other.dx += dx\n",
    "        return Expr(self.value + other.value, backward=back, [self,other])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual Reverse mode AD\n",
    "It's always nice to do things manually.\n",
    "They can be shockingly straightforward.\n",
    "\n",
    "Put it into 3-address code / ANF / flatten it.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 8, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(x,y,z):\n",
    "    q = x + y\n",
    "    r = q + z\n",
    "    p = r + y\n",
    "\n",
    "    def back(dp):\n",
    "        dr = dp\n",
    "        dy = dr\n",
    "        dq = dr\n",
    "        dz = dr\n",
    "        dx = dq\n",
    "        dy += dq\n",
    "        return dx, dy, dz\n",
    "\n",
    "    return p, back\n",
    "\n",
    "res, back = f(1,2,3)\n",
    "back(4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lenses have multiple forms\n",
    "\n",
    "- s -> (t, b -> a)\n",
    "- s -> (t, e) ,  e -> b -> a. closurized form\n",
    "\n",
    "Also note a functional proraming method of encoding object orietned.\n",
    "{\n",
    "    self : e\n",
    "    getmethod1 : e -> a \n",
    "    setmethod1: e -> b -> e\n",
    "}\n",
    "So we can consider it to be rearanged as\n",
    "\n",
    "`s -> {e, getval : e -> t, back : e -> b -> a}` even though persay getval doesn't need e. \n",
    "\n",
    "\n",
    "We can do standard tricks like defunctionalization. That is how we can turn it into tape form.\n",
    "\n",
    "bundle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lens():\n",
    "    def __init__(self, get, set):\n",
    "        self.get = get\n",
    "        self.set = set\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tape.\n",
    "\n",
    "The tape is kind of keeping a record of evreything you've done.\n",
    "\n",
    "Mutable cell in python easy choice is list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 [0]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "tape = []\n",
    "def add(x,y):\n",
    "    x,dx = x\n",
    "    y,dy = y\n",
    "    dz = [0] # a cell\n",
    "    z = x + y\n",
    "    tape.append((\"add\",dz,dx,dy))\n",
    "    return (z, dz)\n",
    "\n",
    "\n",
    "# by definition tape will be in \n",
    "def backprop():\n",
    "    while tape:\n",
    "        op = tape.pop()\n",
    "        print(op)\n",
    "        match op:\n",
    "            case (\"add\",dz,dx,dy):\n",
    "                dx[0] += dz[0]\n",
    "                dy[0] += dz[0]\n",
    "\n",
    "            case _:\n",
    "                raise ValueError(\"unknown op\")\n",
    "\n",
    "def newcell(v):\n",
    "    return (v, [0])        \n",
    "\n",
    "x = newcell(1)\n",
    "y = newcell(2)\n",
    "\n",
    "res, dres  = add(add(x,y),x)\n",
    "print(res, dres)\n",
    "dres[0] = 1\n",
    "print(dres)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do multiply.\n",
    "We can check referential identity during addition to add different things to tape.\n",
    "\n",
    "This form I think can be egraphed.\n",
    "Refcells can also _forward_ union find style. Do all equality in the forward pass. Associated 1 refcell per semantic thing.\n",
    "The indirect reference thing isnt that big a deal in vectorized form.\n",
    "\n",
    "Noticing \"double\" is a smart constructor kind of thing. As I showed before, smart constructors are one step away from aegraph style.\n",
    "\n",
    "hash consing - yes. We don't want to compute x + y twice and then lose sharing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 [0]\n",
      "[1]\n",
      "('add', [1], [0], [0])\n",
      "('add', [1], [1], [0])\n",
      "(1, [2])\n",
      "(2, [1])\n",
      "(27, [0])\n",
      "('mul', [1], 3, [0], 9, [0])\n",
      "('mul', [3], 3, [9], 3, [9])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3, [27])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tape = []\n",
    "def add(x,y):\n",
    "    x,dx = x\n",
    "    y,dy = y\n",
    "    dz = [0] # a cell\n",
    "    if dx is dy:\n",
    "        z = x + y\n",
    "        tape.append((\"double\",dz,dx))\n",
    "    z = x + y\n",
    "    tape.append((\"add\",dz,dx,dy))\n",
    "    return (z, dz)\n",
    "\n",
    "def mul(x,y):\n",
    "    x,dx = x\n",
    "    y,dy = y\n",
    "    dz = [0] # a cell\n",
    "    z = x * y\n",
    "    tape.append((\"mul\",dz,x,dx,y,dy))\n",
    "    return (z, dz)\n",
    "\n",
    "# by definition tape will be in \n",
    "def backprop():\n",
    "    while tape:\n",
    "        op = tape.pop()\n",
    "        print(op)\n",
    "        match op:\n",
    "            case (\"add\",dz,dx,dy):\n",
    "                dx[0] += dz[0]\n",
    "                dy[0] += dz[0]\n",
    "            case (\"double\", dz, dx):\n",
    "                dx[0] += 2 * dz[0]\n",
    "            case (\"mul\",dz,x,dx,y,dy):\n",
    "                dx[0] += y * dz[0]\n",
    "                dy[0] += x * dz[0]\n",
    "            case _:\n",
    "                raise ValueError(\"unknown op\")\n",
    "\n",
    "def newcell(v):\n",
    "    return (v, [0])        \n",
    "\n",
    "x = newcell(1)\n",
    "y = newcell(2)\n",
    "\n",
    "res, dres  = add(add(x,y),x)\n",
    "print(res, dres)\n",
    "dres[0] = 1\n",
    "print(dres)\n",
    "\n",
    "backprop()\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "x = newcell(3)\n",
    "z = mul(x, mul(x,x))\n",
    "print(z)\n",
    "z[1][0] = 1\n",
    "backprop()\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do this with a tape stack.\n",
    "ROP chain\n",
    "\n",
    "cells are indices into the stack now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack = [  add_back, data, data]\n",
    "\n",
    "\n",
    "def push(n):\n",
    "    \n",
    "\n",
    "def ret(n):\n",
    "    # cleanup and call\n",
    "\n",
    "add_back():\n",
    "    x = stack[-1]\n",
    "    #\n",
    "    ret(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I guess nn.module is already a function object. Like duh.\n",
    "However, they are hard to integrate / minimize (? I mean, but actuall obviously they are really good). Not great for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneFun():\n",
    "    params : pytorch.tensor\n",
    "    def __call__(self, x):\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mosiac dex-lang\n",
    "stalingrad\n",
    "Higher orderr AD\n",
    "Julia ran into issues\n",
    "enzyme\n",
    "torch-mlir https://github.com/llvm/torch-mlir\n",
    "remora\n",
    "Aten https://pytorch.org/cppdocs/notes/tensor_basics.html\n",
    "pytorch7\n",
    "cudann\n",
    "alt backends? dsp cores?\n",
    "Memory model. No copy\n",
    "raytracer\n",
    "\n",
    "https://github.com/huggingface/smol-course\n",
    "smollm. intreresting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch\n",
    "\n",
    "\n",
    "What is necessary and what is not\n",
    ".pth model files\n",
    "\n",
    "I had old blog posats using pyhtroch for control and optimzing a wave function.\n",
    "Yeah, like what is fun?\n",
    "\n",
    "\n",
    "https://pytorch.org/ecosystem/\n",
    "\n",
    "executorch\n",
    "\n",
    "trochtune https://github.com/pytorch/torchtune fine tuning\n",
    "\n",
    "tensorRT. https://github.com/pytorch/TensorRT\n",
    "trochao https://github.com/pytorch/ao architecture optimization. qauntization\n",
    "\n",
    "torch.compile\n",
    "\n",
    "ONNX deployment https://onnx.ai/\n",
    "pyro\n",
    "\n",
    "https://pytorch.org/tutorials/advanced/numpy_extensions_tutorial.html I can call into scipy. If I custom implement backward?\n",
    "https://pytorch.org/tutorials/intermediate/pinmem_nonblock.html \n",
    "\n",
    "https://paperswithcode.com/\n",
    "https://github.com/yangchris11/samurai \n",
    "https://github.com/arplaboratory/learning-to-fly\n",
    "https://github.com/rl-tools\n",
    "https://github.com/rl-tools/rl-tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "# Download training data from open datasets.\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"/tmp/data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"/tmp/data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "batch_size = 64\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "device = \"cpu\"\n",
    "model = NeuralNetwork().to(device)\n",
    "print(dir(test_dataloader))\n",
    "print(model)\n",
    "dir(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# enzyme\n",
    "\n",
    "https://github.com/EnzymeAD/Enzyme-Tutorial\n",
    "https://github.com/ftynse/mlir-tutorial-docker/tree/enzyme\n",
    "tutorial at mlir winter school\n",
    "enazyme can be on llvm level or on mlir level.\n",
    "Custom tablegen\n",
    "\n",
    "```\n",
    "\n",
    "  grad_C =__enzyme_autodiff((void*)dot,\n",
    "                    enzyme_const, A,\n",
    "                    enzyme_dup, B, grad_B,\n",
    "                    enzyme_out, C,\n",
    "                    enzyme_const, n);\n",
    "```\n",
    "`../dockerscript.sh clang-12 /host/$^ -O3 -Xclang -load -Xclang /Enzyme/enzyme/build/Enzyme/ClangEnzyme-12.so -ffast-math -o /host/$@`\n",
    "\n",
    " \n",
    "jax can output mlir from python?\n",
    "\n",
    "stableHLO https://github.com/openxla/stablehlo\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# triton\n",
    "\n",
    "https://mastodon.social/@mattpd/115221696829984466 https://github.com/triton-lang/triton/blob/main/python/tutorials/gluon/01-intro.py gluon\n",
    "https://youtube.com/watch?v=5e1YKqsP8i8&t=1066s https://news.ycombinator.com/item?id=45280592\n",
    "\n",
    "Blackwell is like a puzzle to get performance?\n",
    "\n",
    "Tilus https://github.com/NVIDIA/tilus\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
