{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "320e6b50",
   "metadata": {},
   "source": [
    "---\n",
    "title: An Inequality Union Find Inspired by Atomic Asymmetric Completion\n",
    "date: 2025-12-22\n",
    "---\n",
    "\n",
    "Atomic asymmetric completion is a union find. Ground term asymmetric completion is a refinement egraph.\n",
    "\n",
    "# Abstract Completion\n",
    "\n",
    "Abstract [completion](https://en.wikipedia.org/wiki/Knuth%E2%80%93Bendix_completion_algorithm) is a generic brute force strategy for turning equations into good (convergent = confluent + terminating) rewrite rules.\n",
    "\n",
    "For any kind of data structure or mathematical object with a notion of equality, matching and replacement (strings, terms, ground terms, atoms, [polynomials](https://en.wikipedia.org/wiki/Gr%C3%B6bner_basis), [groups](https://gap-packages.github.io/kbmag/doc/chap2.html), [traces](https://en.wikipedia.org/wiki/Trace_monoid), [multisets](https://www.philipzucker.com/multiset_rw/), graphs, term graphs, drags, combos thereof) the following makes sense of something you might try to do:\n",
    "\n",
    "1. Define a well founded order on your thing (typically smaller, simpler, etc).\n",
    "2. orient your unoriented equations to simplifying rules\n",
    "3. Find all overlaps of two rule left hand sides, add those as new deduced equations\n",
    "4. goto 2\n",
    "\n",
    "Whether this battleplan is actually complete or terminating requires mathematical analysis (critical pair /diamond lemmas and other stuff), but it is obviously sound. A better version also reduces rules with respect to each other, but this also requires care to retain completeness.\n",
    "\n",
    "# Asymettric Completion\n",
    "\n",
    "\n",
    "[Asymmetric completion](https://www.iiia.csic.es/~levy/papers/jsc.pdf) is a shift on this strategy for things that aren't fundamentally bidirectional, reasoning about an inequality relationship `<=` (for examples subset relations, subtype relations, undefinedness, and refinement relations) with similar machinery as you do for a symmettric equality relationship `=`\n",
    "\n",
    "\n",
    "# How does it work\n",
    "\n",
    "The trick is you need two rewrite relations R,S rather than the single rewrite relation for equational theories.\n",
    "\n",
    "Rather than orient an equation `a = b` to turn it into a rule `a -> b` or `b -> a`, you orient an inequality `a <= b` by by placing it into either `R` or `S`. \n",
    "\n",
    "In completion, no matter how we orient, it remains true that `E = (R U R^-1)*`. In asymettric completion `LE = (R U S)*`  since facts from `LE` just get placed in `R` or `S`.\n",
    "\n",
    "For example `e17 <= e15` goes into `R` because it is reducing the identifier, whereas `e15 <= e17` would go into `S` since it is increasing the identifier.\n",
    "\n",
    "In order to prove a query `a <=? b` we do a searching reduction of `a` by `R*` and `b` by `S*`. This correspond to an intuition that you should search up from `a` at the same time you search down from `b`, but the separation of R and S removes redundancies in that search.\n",
    "\n",
    "This does not have as good of properties as a completed rewrite system, which has normal forms. We can't greedily destructively rewrite and have to retain all the things we can reach and see if the two cones emanating from `a` and `b` intersect.\n",
    "\n",
    "Nevertheless, `R` and `S` because they are oriented by eid produce eids in a monotonic orderly fashion, possibly enabling fast techniques involving min-heaps, sorted lists, and sorted merges.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e36968",
   "metadata": {},
   "source": [
    "# Code\n",
    "\n",
    "This is a basic version of the above. I use brute force python sets of tuples  `set[tuple[int,int]]` to represent R and S. Obviously, one should probably use a better data structure, maybe something more like `dict[int, set[int]]`.\n",
    "\n",
    "It is not clear that this structure is superior to the one I describe here https://www.philipzucker.com/le_find/ . Things do come out a little more orderly, but at some conceptual complexity and rebuild cost. To what benefit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed03b0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LEUF(R={(2, 0)}, S={(0, 1)}, fresh=3)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataclasses import dataclass, field\n",
    "def find_down(T, x):\n",
    "    \"\"\"\n",
    "    Transitively close downward from x. Pre apply T\n",
    "    yT*x\n",
    "    \"\"\"\n",
    "    res = set([x])\n",
    "    while True:\n",
    "        new = {y for (y,x1) in T for x in res if x == x1}\n",
    "        if len(new - res) == 0:\n",
    "            return res\n",
    "        res |= new\n",
    "\n",
    "def find_up(T, x):\n",
    "    # xT*y . Transitively close upwards. Post apply T\n",
    "    res = set([x])\n",
    "    # dumbest possible loop. could seminiave it or keep queue.\n",
    "    # new values come out monotonically\n",
    "    while True:\n",
    "        new = {y for (x1,y) in T for x in res if x == x1}\n",
    "        if len(new - res) == 0:\n",
    "            return res\n",
    "        res |= new\n",
    "\n",
    "@dataclass\n",
    "class LEUF():\n",
    "    R : set = field(default_factory=set)\n",
    "    S : set = field(default_factory=set)\n",
    "    fresh : int = 0\n",
    "    def makeset(self):\n",
    "        v = self.fresh\n",
    "        self.fresh += 1\n",
    "        return v    \n",
    "    def find_le(self, x, y):\n",
    "        zSy = find_down(self.S, y)\n",
    "        xRz = find_up(self.R, x)\n",
    "        return len(zSy & xRz) > 0\n",
    "    def assert_le(self, x, y):\n",
    "        if x == y:\n",
    "            return True\n",
    "        zSy = find_down(self.S, y)\n",
    "        xRz = find_up(self.R, x)\n",
    "        \n",
    "\n",
    "        if len(zSy & xRz) == 0:\n",
    "            if x < y:\n",
    "                self.S.add((x,y))\n",
    "            else:\n",
    "                self.R.add((x,y))\n",
    "        else:\n",
    "            return True\n",
    "    def rebuild(self):\n",
    "        # form all SR critical pairs and reduce them\n",
    "        # Or do the rebuilding immiediately in assert\n",
    "        pairs = {(x,z) for (x,y) in self.S for (y1,z) in self.R if y == y1}\n",
    "        for (x,z) in pairs:\n",
    "            if not self.find_le(x,z):\n",
    "                self.assert_le(x,z)\n",
    "        # possible remove redundant also\n",
    "\n",
    "\n",
    "find_up({(1,2), (2,3)}, 3)\n",
    "find_down({(1,2), (2,3)}, 3)\n",
    "\n",
    "uf = LEUF()\n",
    "x,y,z = uf.makeset(), uf.makeset(), uf.makeset()\n",
    "uf.assert_le(x,y)\n",
    "assert uf.find_le(x,y)\n",
    "assert not uf.find_le(y,x)\n",
    "assert not uf.find_le(x,z)\n",
    "uf.assert_le(y,z)\n",
    "assert uf.find_le(x,z)\n",
    "assert not uf.find_le(z,x)\n",
    "\n",
    "uf = LEUF()\n",
    "x,y,z = uf.makeset(), uf.makeset(), uf.makeset()\n",
    "uf.assert_le(x,y)\n",
    "uf.assert_le(z,x)\n",
    "uf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c622d109",
   "metadata": {},
   "source": [
    "# Ground Term Asymmetric Completion\n",
    "I'm fairly disturbed that Struth says ground asymettric completion fails. He has a counterexample. https://www.sciencedirect.com/science/article/pii/S1571066104002968 Knuth-Bendix Completion for Non-Symmetric Transitive Relations - Georg Struth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca66a6d",
   "metadata": {},
   "source": [
    "# Ordered Resolution as Completion\n",
    "\n",
    "Ordered Resolution and completion have a lot of similarity. Ordered resolution was designed in full knowledge of completion. I think it makes sense to think of ordered resolution as converting unordered clauses into prolog / datalog rules. This is different from standard completion in that there are many ways to orient a clause, not just two. If you have ordered all your clauses, you can then run the datalog program to construct a model. Ordered resolution combines rules in such a way to try and make this datalog run not reach an inconsistency.\n",
    "\n",
    "|          | unoriented  | oriented     |\n",
    "|----------|-------------|----------|\n",
    "| Equality | equation `=`| rewrite `->` |\n",
    "| Propositional |  clause  `c | a | not b` | rule `a :- b, not c` |\n",
    "\n",
    "A bunch of propositional clauses has many models. If they are horn clauses (each has only one positive literal), then there is a very reasonable unique notion of a minimal model. If they aren't horn, you need to say how you'd like the tie break your preference of models with a term / proposition ordering. This is sort of like prescribing an objective function to the problem. If you can eliminate all models that would be minimal, then you've eliminated all models and the thing is unsatisfiable. Requiring only search through minimal models may eliminate the need to search through some models that would obviously not be minimal (there is some other model that must also work if that one worked that would be better).\n",
    "\n",
    "The Struth article takes another road to model propositional reasoning as inequality reasoning module the axioms of a lattice. I'm not sure if this is a more or less radical perspective. https://link.springer.com/chapter/10.1007/10721975_15 An Algebra of Resolution - Georg Struth. It's very interesting that a finitely presented lattice specialization of completion gives you something like resolution akin to how specializing to finitely presented rings gives you grobner bases and Buchberger's algorithm. This really caught my eye because I've been thinking more about e-graphs modulo theories lately rather than refinement egraphs, but this shows a connection between the two topics.\n",
    "\n",
    "- Handbook of automated reasoning chapter 2 resolution https://lawrencecpaulson.github.io/papers/bachmair-hbar-resolution.pdf\n",
    "- https://rg1-teaching.mpi-inf.mpg.de/autrea-ws21/notes-3d.pdf\n",
    "- https://www.tcs.ifi.lmu.de/teaching/courses-ws-2024-25/automated-theorem-proving/slides07-more-resolution.pdf\n",
    "- https://www.philipzucker.com/superpose_datalog/ Superposition as a Super Datalog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3c7718",
   "metadata": {},
   "source": [
    "# Bits and Bobbbles\n",
    "This post is a relative of this one https://www.philipzucker.com/le_find/\n",
    "\n",
    "Resources on asymmetric completion:\n",
    "- https://www.iiia.csic.es/~levy/papers/jsc.pdf Bi-rewrite Systems - JORDI LEVY AND JAUME AGUSTI\n",
    "- https://link.springer.com/chapter/10.1007/10721975_15 An Algebra of Resolution - Georg Struth\n",
    "- https://www.sciencedirect.com/science/article/pii/S1571066104002968 Knuth-Bendix Completion for Non-Symmetric Transitive Relations - Georg Struth\n",
    "- https://people.mpi-inf.mpg.de/alumni/ag2/2011/hg/index/index7.html Look at part 2\n",
    "\n",
    "nelson Oppen for inequalities. if we have an inequality union find, can we glue theories together with a common `<=` in addition to a a common `<`? Maybe.\n",
    "\n",
    "Assymmetric completion looks to me like a good engine for subtyping.\n",
    "\n",
    "It's a bit odd that asymmetric completion doesn't seem like it went anywhere?\n",
    "\n",
    "1. `S*R* <= R*S*` iff `(R U S)* <= R*S*`\n",
    "2. `SR <= R*S*` implies `(R U S)* <= R*S*` if `R U S^-1` is wellfounded\n",
    "\n",
    "These generalize church rosser and Newman's lemma\n",
    "\n",
    "https://www.iiia.csic.es/~levy/papers/jsc.pdf Bi-rewrite Systems - JORDI LEVY AND JAUME AGUSTI\n",
    "\n",
    "I'm surprised that Dolan's https://www.cs.tufts.edu/~nr/cs257/archive/stephen-dolan/thesis.pdf discussion of birewriting and biunification contains no reference to Levy and Agusti. Is the connection not known? Is there no connection?\n",
    "\n",
    "https://arxiv.org/abs/1802.08437 Abstract Completion, Formalized\n",
    "https://www.csl.sri.com/papers/bachmairtiwari00/cade00-CC.pdf abstract congruence closure\n",
    "\n",
    "\n",
    "Writing out some small examples helped me alot. Try out some small Hasse lattices with meangingless eid labels.\n",
    "\n",
    "A second article Knuth-Bendix Completion for Non-Symmetric\n",
    "Transitive Relations struth 2001. Also discusses the ground case. Says it is not terminating even for ground. Does automata something fix that? Noticing something? Seems odd.\n",
    "`f(b) < b  R is reducing`\n",
    "`f(a) < b  S is antireducing`\n",
    "\n",
    "- Atomic completion is a union find. https://www.philipzucker.com/egraph-ground-rewrite/ https://www.philipzucker.com/egraph2024_talk_done/ \n",
    "\n",
    "- Ground Term completion is an e-graph basically.\n",
    "\n",
    "- Atomic asymmetric completion is an inequality union find. \n",
    "\n",
    "- Ground Term asymmetric completion is a refinement egraph.\n",
    "\n",
    "\n",
    "# Generalized Rewriting\n",
    "\n",
    "Generalized rewriting and generalized congruence closure is the right framework for a refinement egraph.\n",
    "It is propagation other relations besides equality up through function symbols.\n",
    "You can bake this propagation in to the farbic of the egraph.\n",
    "The refinement egraphs is a manifestation of refinement closure / monotonicity `a <= b -> f a <= f b` in the same way that the regular egraph is a manifestation or datatypification of congruence closure `a = b -> f a = f b`.\n",
    "\n",
    "Dénès, M., Mörtberg, A., Siles, V.: A refnement-based approach to computational\n",
    "algebra in coq\n",
    "\n",
    "Generaliuzed rewriting https://rocq-prover.org/doc/v8.18/refman/addendum/generalized-rewriting.html \n",
    "https://jfr.unibo.it/article/download/1574/1077/3383 A New Look at Generalized Rewriting in Type Theory Sozeau\n",
    "Subst (being <=) and diff being a fun symbol. That's a good one. since diff is `->` in some heyting sense, also makes sense.\n",
    "\n",
    "Indeed A cup B <= C  is the same as A <= C  and B <= C . Is that useful? Does that fight an AC problem?\n",
    "So set algebra. A = B, C = D,  diff, union, complement, intersection . Some of these do have anti modality. \n",
    "set constraints\n",
    "clp set\n",
    "{a} <= A  is open set with a in it.  elem(a,A) == sing(a) <= A\n",
    "\n",
    "\"mediated\" equality is very much exactly gewneralized rewriting in sozeau sense\n",
    "The variance / compatibility rules are the custom congruence rules.\n",
    "\n",
    "[Bas94] David A. Basin. Generalized Rewriting in Type Theory\n",
    "A Semi-reflexive Tactic for (Sub-)Equational Reasoning  Claudio Sacerdoti Coen\n",
    "\n",
    "Andrew McCreight. Practical tactics for separation logic\n",
    "Nick Benton and Nicolas Tabareau. Compiling Functional Types to Relational Specifications for Low Level Imperative Code. used generalized rewriting for their tactics?\n",
    "\n",
    "\n",
    "https://maude.cs.illinois.edu/w/images/0/0f/BMgrt_2003.pdf  Generalized Rewrite Theories -Roberto Bruni12 and Jos´e Mesegue . Maude. Kind of hartd to read. This might be getting at some of the same stuff. Yes.  E and R.\n",
    "\n",
    "\n",
    "\n",
    "https://www.youtube.com/watch?v=3Dh-EG6JfyU Generalized Rewriting | Jovan Gerbscheid  gcongr tactic 2023 heather macbeth Lean  https://icetcs.github.io/frocos-itp-tableaux25/slides/itp/lean4-gerbscheid.pdf\n",
    "\n",
    "```\n",
    "≤ <\n",
    "≡ [ZMOD n] n\n",
    "⊆\n",
    "→\n",
    "∣\n",
    "=ᶠ[ae μ] almost everywhere equality\n",
    "μ\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "My subsort checker database is basically a congryuence database?\n",
    "\n",
    "cong = {\n",
    "    set.Union: kd.ForAll([A,B,C,D], A <= B, C <= D, Union(A,C) <= Union(B, D))\n",
    "    set.Inter:\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Semi Unification\n",
    "\n",
    "https://archive.computerhistory.org/resources/access/text/2024/01/102805327-05-01-acc.pdf Henglein \"semi-unification\"\n",
    "\n",
    "https://pure.mpg.de/rest/items/item_1834874_3/component/file_1857517/content Is this the same or a different semi unification?\n",
    "https://www.sciencedirect.com/science/article/pii/0304397591901899 Kapur semi unification\n",
    "sig phi t = phi s\n",
    "a unification and a matching\n",
    "https://types.pl/@flippac/115102336973734248\n",
    "\n",
    "or sig t <= sig s  equivalently. \n",
    "\n",
    "https://dl.acm.org/doi/10.1145/169701.169687 type reconstruction in the presence of polymorphic recursion\n",
    "https://cormack.uwaterloo.ca/rasup.pdf  A Larger Decidable Semiunification Problem\n",
    "\n",
    "\n",
    "# graph reachability\n",
    " Amortized efficiency of a path retrieval data structure\n",
    "https://www.sciencedirect.com/science/article/pii/0304397586900988 \n",
    "\n",
    "https://chaozhang-cs.github.io/files/sigmod23-tutorial-short.pdf An Overview of Reachability\n",
    "Indexes on Graphs\n",
    "- tree cover indices\n",
    "- 2-hop indices - hmm. Kind of like a union find to root, away from root. Finding good quasi roots\n",
    "- approximate reachability\n",
    "\n",
    "This does match my paradigm of rebuild or approximate.\n",
    "\n",
    "https://drops.dagstuhl.de/storage/00lipics/lipics-vol221-sand2022/LIPIcs.SAND.2022.1/LIPIcs.SAND.2022.1.pdf Recent Advances in Fully Dynamic Graph\n",
    "Algorithms\n",
    "\n",
    "\n",
    "Inequality solvers in flow typechecker\n",
    "- https://github.com/facebook/flow/blob/main/src/typing/tvar_resolver.ml \n",
    "- https://github.com/facebook/flow/blob/d076b30cfc3635ff8c38d43bee0b08c66cb9ec4f/src/typing/context.ml#L1209 find_root? I dunno. This seems problematic.\n",
    "\n",
    "\n",
    "That lattice textbook\n",
    "\n",
    "\n",
    "Where in compilers is there an example of a contravaraint symbol?\n",
    "Inputs to function calls?\n",
    "Call site f(x,y,z)\n",
    "bind site\n",
    "def f(x,y,z):  \n",
    "\n",
    "t <= pat <= rhs\n",
    "\n",
    "`t <bisubst= pat <= rhs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "E-graph trinitarianism - hash cons <-> union find <-> egraph\n",
    "\n",
    "Ordered chaining\n",
    "https://www.philipzucker.com/le_find/\n",
    "\n",
    "```\n",
    "tSu          vSw\n",
    "-----------------\n",
    "  sig[t] S sig[w]\n",
    "```\n",
    "\n",
    "But with the usual restrction that we only need to do inferences that increase something?\n",
    "\n",
    "\n",
    "ground ordered chaining. maybe there is some path compression that can work here?\n",
    "\n",
    "\n",
    "https://inria.hal.science/inria-00073205/document Pottier thesis\n",
    "\n",
    "\n",
    "Higher order Bi-Unfication\n",
    "F(x) <= x\n",
    "\n",
    "\n",
    "icfp gilbert something knitting groupoid. Hmm.\n",
    "\n",
    "# Uses of refinement\n",
    "2 more\n",
    "- refinement reasoning in algerab of programming. Relational spec. functional implementation. The calculated typer https://people.cs.nott.ac.uk/pszgmh/typer.pdf . Program Construction: Calculating Implementation  from Specifications . Mathmeth http://www.mathmeth.com/ https://www.cs.ox.ac.uk/publications/books/PfS/ . Gries?\n",
    "- Cycluic proofs.  We get the theoremwe're trying to rpover as a inequality guarded rule. Would not numerical ineq be good enough?1\n",
    "\n",
    "https://en.wikipedia.org/wiki/Relation_algebra\n",
    "\n",
    "https://www.philipzucker.com/a-short-skinny-on-relations-towards-the-algebra-of-programming/\n",
    "\n",
    "https://www.di.uminho.pt/~jno/ps/pdbc.pdf\n",
    "\n",
    "- Gibbons book?\n",
    "- agda aop\n",
    "- Backhouse calculation book\n",
    "\n",
    "https://staff.math.su.se/anders.mortberg/papers/refinements.pdf refinement for free\n",
    "dense poly to sparse poly\n",
    "\n",
    "https://link.springer.com/content/pdf/10.1007/978-3-031-57262-3_10.pdf  Trocq: Proof Transfer for Free, With or Without Univalence 2024. Hmm. 2 versions of bitvec\n",
    "CoqEAL library https://github.com/rocq-community/coqeal\n",
    "\n",
    "Automatic and Transparent Transfer of\n",
    "Theorems along Isomorphisms\n",
    "in the Coq Proof Assistant  Theo Zimmermann1 and Hugo Herbelin 2015\n",
    "\n",
    "Data vs Program rtefinement. Cahnge the program or change the datatype.\n",
    "quot(X) = Y  fine. mod out permutations.\n",
    "canon(Y) = X , pick one somewhat arbitrarily vs\n",
    "Y -> X    . Kind of expressing containment in quoitent set. It is possible to refine Y to many possible X.\n",
    "\n",
    "\n",
    "https://en.wikipedia.org/wiki/Refinement_calculus\n",
    "\n",
    "# Algebra of Resolution\n",
    "\n",
    "\n",
    "An algebra of resolution by struth 2000. That this seems only available in the proceedings compendium stinks.\n",
    "\n",
    "A second article Knuth-Bendix Completion for Non-Symmetric\n",
    "Transitive Relations struth 2001. Also discusses the ground case. Says it is not terminating even for ground. Does automata something fix that? Noticing something? Seems odd.\n",
    "f(b) < b  R is reducing\n",
    "f(a) < b  S is antireducing\n",
    "\n",
    "`a { f { b`\n",
    "\n",
    "f^n(a) < b\n",
    "\n",
    "Can we find a good ordering though?\n",
    "create `c` such that c = b and c is in a good spot.\n",
    "\n",
    "Hmm. But we could close it out up to the currently existing terms. And then leave remaining critical pairs frozen or something.\n",
    "\n",
    "\n",
    "\n",
    "Suppose we had a question about abstract entitites \n",
    "`e17 <=? e42`\n",
    "\n",
    "Then the R half of the inequality\n",
    "`e17 R<= e13 R<= e11` Is being produced left to right\n",
    "\n",
    "`e11 S<= e14 S<= e34 S<= e42` is being searched right to left using the S half of the inequality\n",
    "\n",
    "In both cases the ids are being decreased. If the ids stop at `e0`, the process has to stop.\n",
    "\n",
    "There may be more stuff in there. `e40 S<= e42` may also be a fact around that is not useful for this particular derivation, but you can't know it's not useful immediately.\n",
    "\n",
    "That everything comes in a monotonic way probably makes for nicer usage of ordering based data structures like insorting on lists or sorted binary trees or minheaps and sorted merging.\n",
    "\n",
    "\n",
    "Huh. When you write it out, it feels obvious this works, because we just perform a transitive closure.\n",
    "But that you get to prune the transitive lcosure is non obvious.\n",
    "\n",
    "The idea of proving a <= b  by searching up from a while at the same time searching down from b is intuitive\n",
    "\n",
    "search up from a until you find b\n",
    "search down from b until you find a\n",
    "search simultaneously until you find a common connecting node https://en.wikipedia.org/wiki/Bidirectional_search\n",
    "\n",
    "If you choose to search simultaneously, you may start sending out tendrils that \"obviously\" are going far below the thing you are looking for. It's good to prune these\n",
    "\n",
    "For regular canonization, the properties of the rewrite system enable nice things.\n",
    "If it is confluent and terminating , normalizing, you can greedily simplify\n",
    "It it is just terminating but not confluent, it is possibly you need to do search, but the search will be finite.\n",
    "\n",
    "\n",
    "proof orderings. A sequence of dudes. Lexicographically shortest proofs, which isn't what we'd ask for probably.\n",
    "\n",
    "\n",
    "Relationship to subtyping\n",
    "assymmettric\n",
    "\n",
    "assymettric rewriting and linear programming. I had a spiel about working over R+ . \n",
    "I wonder if Q semiring could use linear programming somehow, N semiring use MILP \n",
    "The analog/generalization of F4 might use LP to do many semiring reductions at once\n",
    "\n",
    "Ax <= c\n",
    "Maybe R and S have a relationship to primary and dual?\n",
    "\n",
    "Equations overs R+ is a way of baking inequality in (?)\n",
    "`x**2` in grobner bases. `x**2 = linexpr` <-> `linexp >= 0`  DOes it make sense? Maybe. `x**4 = x**2`\n",
    "\n",
    "sum of sqaures is nicer. But if we make grober + birewriting, do we get a systematic treatment of \n",
    "\n",
    "It feels like the generalized / abstract KB and assymmetric KB are sometimes encodable?\n",
    "\n",
    "Fourier motzkin. normalize to largest term (with positive 1 coefficient. That part is a bit odd)\n",
    "x <= p\n",
    "p <= x\n",
    "Yeah, we don't get a normal form. that's how birewriting do\n",
    "\n",
    "I do think I can encode fourier motzkin to grobner bases by making dummy variables. They kind of track\n",
    "\n",
    "Simplex is goal driven.\n",
    "\n",
    "Analagous algorithm for polynomials. Isolate largest monomial to positive side.\n",
    "y*x**3 <= p\n",
    "p <= zx**3\n",
    "\n",
    "\n",
    "a = a -> bot\n",
    "define a recursive type constant by an equation\n",
    "X -> Y <= X' -> X' -> Y' -> <= X /\\ \n",
    "There's no way to do this without a theory of lattices?\n",
    "\n",
    "https://theory.stanford.edu/~aiken/publications/papers/popl02.pdf\n",
    "https://inria.hal.science/inria-00536817/document  Entailment of Atomic Set Constraints is\n",
    "PSPACE-Complete\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a04d8b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LEUF(R={(2, 0)}, S={(0, 1)}, fresh=3)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataclasses import dataclass, field\n",
    "def find_down(T, x):\n",
    "    # yT*x\n",
    "    res = set([x])\n",
    "    while True:\n",
    "        new = {y for (y,x1) in T for x in res if x == x1}\n",
    "        if len(new - res) == 0:\n",
    "            return res\n",
    "        res |= new\n",
    "\n",
    "def find_up(T, x):\n",
    "    # xT*y\n",
    "    res = set([x])\n",
    "    while True:\n",
    "        new = {y for (x1,y) in T for x in res if x == x1}\n",
    "        if len(new - res) == 0:\n",
    "            return res\n",
    "        res |= new\n",
    "\n",
    "@dataclass\n",
    "class LEUF():\n",
    "    R : set = field(default_factory=set)\n",
    "    S : set = field(default_factory=set)\n",
    "    fresh : int = 0\n",
    "    def makeset(self):\n",
    "        v = self.fresh\n",
    "        self.fresh += 1\n",
    "        return v    \n",
    "    def find_le(self, x, y):\n",
    "        zSy = find_down(self.S, y)\n",
    "        xRz = find_up(self.R, x)\n",
    "        return len(zSy & xRz) > 0\n",
    "    def assert_le(self, x, y):\n",
    "        if x == y:\n",
    "            return True\n",
    "        zSy = find_down(self.S, y)\n",
    "        xRz = find_up(self.R, x)\n",
    "        \n",
    "\n",
    "        if len(zSy & xRz) == 0:\n",
    "            if x < y:\n",
    "                self.S.add((x,y))\n",
    "            else:\n",
    "                self.R.add((x,y))\n",
    "        else:\n",
    "            return True\n",
    "    def rebuild(self):\n",
    "        # form all SR pairs and reduce them\n",
    "        # Or do the rebuilding immiediately in assert\n",
    "        pairs = {(x,z) for (x,y) in self.S for (y1,z) in self.R if y == y1}\n",
    "        for (x,z) in pairs:\n",
    "            if not self.find_le(x,z):\n",
    "                self.assert_le(x,z)\n",
    "        # possible remove redundant also\n",
    "\n",
    "\n",
    "find_up({(1,2), (2,3)}, 3)\n",
    "find_down({(1,2), (2,3)}, 3)\n",
    "\n",
    "uf = LEUF()\n",
    "x,y,z = uf.makeset(), uf.makeset(), uf.makeset()\n",
    "uf.assert_le(x,y)\n",
    "assert uf.find_le(x,y)\n",
    "assert not uf.find_le(y,x)\n",
    "assert not uf.find_le(x,z)\n",
    "uf.assert_le(y,z)\n",
    "assert uf.find_le(x,z)\n",
    "assert not uf.find_le(z,x)\n",
    "\n",
    "uf = LEUF()\n",
    "x,y,z = uf.makeset(), uf.makeset(), uf.makeset()\n",
    "uf.assert_le(x,y)\n",
    "uf.assert_le(z,x)\n",
    "uf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba8fd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LEUF():\n",
    "    uf : list[int]\n",
    "    R : set[tuple[int,int]] # eid decreasing part of <=\n",
    "    S : set[tuple[int,int]] # eid increasing part of <=\n",
    "    def find(self, x : int) -> int:\n",
    "        if self.uf[x] != x:\n",
    "            self.uf[x] = self.find(self.uf[x])\n",
    "        return self.uf[x]\n",
    "    def union(self, x:int, y:int):\n",
    "        x,y = self.find(x), self.find(y)\n",
    "        if x != y:\n",
    "            self.uf[max(x,y)] = min(x,y) # tie break by min.\n",
    "    def is_le(self, x, y):\n",
    "        x,y = self.find(x), self.find(y)\n",
    "        zSy = find_down(self.S, y)\n",
    "        xRz = find_up(self.R, x)\n",
    "        return len(zSy & xRz) > 0\n",
    "    def assert_le(self, x, y):\n",
    "        x,y = self.find(x), self.find(y)\n",
    "        if x == y:\n",
    "            return True\n",
    "        elif self.is_le(y, x):\n",
    "            return self.union(x,y)\n",
    "        elif self.is_le(x, y):\n",
    "            return True\n",
    "        else:\n",
    "            if x < y:\n",
    "                self.S.add((x,y))\n",
    "            else:\n",
    "                self.R.add((x,y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4200435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "@dataclass\n",
    "class BinRel():\n",
    "    xy : defaultdict = field(default_factory=lambda: defaultdict(set))\n",
    "    yx : defaultdict = field(default_factory=lambda: defaultdict(set))\n",
    "    def add(self, x, y):\n",
    "        self.xy[x].add(y)\n",
    "        self.yx[y].add(x)\n",
    "    def __getitem__(self, shape):\n",
    "        x,y = shape\n",
    "        if x == slice(None):\n",
    "            return self.yx[y]\n",
    "        elif y == slice(None):\n",
    "            return self.xy[x]\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "b = BinRel()\n",
    "b.add(0,1)\n",
    "b.add(0,2)\n",
    "b[:,1]\n",
    "b[:,2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32aa61f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LEUF():\n",
    "    # R holds x le y where y < x \n",
    "    R : dict = field(default_factory=dict)\n",
    "    # S hold y le x where y < x \n",
    "    S : dict = field(default_factory=dict) # defaultdict(set)\n",
    "    def find_up(self, x):\n",
    "    def find_down(self, x):\n",
    "\n",
    "    def find(self, x, y):\n",
    "        # normalize x le y. is_le ?\n",
    "\n",
    "    def assert_le(self, x, y):\n",
    "        x,y = self.find_le(x, y)\n",
    "        if x == y:\n",
    "            return\n",
    "        elif x < y:\n",
    "            self.R[x].add(y)\n",
    "        else:\n",
    "            self.S[y].add(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6001b229",
   "metadata": {},
   "source": [
    "I don't know\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Dustin suggests\n",
    " link-cut tree https://en.wikipedia.org/wiki/Link/cut_tree\n",
    " euler tour \n",
    "\n",
    "What about floyd warshall? but floyd wasrahjll is for graphs. \n",
    "x - y <= 5 is proof relevant inequality sure.\n",
    "\n",
    "\n",
    "reow subtyping. {a : A, b : B} <= {a : A', b : B', c : C'}\n",
    "accessor form a(X) = a(Y)? but a(X) <= a(Y) really. \n",
    "eids as records.\n",
    "This is like object stuff.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478e38d2",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=3Dh-EG6JfyU Generalized Rewriting | Jovan Gerbscheid  gcongr tactic 2023 heather macbeth  https://icetcs.github.io/frocos-itp-tableaux25/slides/itp/lean4-gerbscheid.pdf\n",
    "\n",
    "```\n",
    "≤ <\n",
    "≡ [ZMOD n] n\n",
    "⊆\n",
    "→\n",
    "∣\n",
    "=ᶠ[ae μ] almost everywhere equality\n",
    "μ\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "My subsort checker database is basically a congryuence database?\n",
    "\n",
    "cong = {\n",
    "    set.Union: kd.ForAll([A,B,C,D], A <= B, C <= D, Union(A,C) <= Union(B, D))\n",
    "    set.Inter:\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c22c35",
   "metadata": {},
   "source": [
    "Var trick for opening binders? Is it sound?\n",
    "Use original lambda as \"name\" of variable.\n",
    "lam(var(i) + 1) -->  fvar(lam(var(i) + 1)) + 1\n",
    "Don't use numbers when you don't have to\n",
    "\"Skolemization\"\n",
    "\"Use the reason it exists\"\n",
    "\"provenance\"\n",
    "\n",
    "Analog of the `let` trick.\n",
    "The free varaible was certainly \"caused\" by opening the binder.\n",
    "\n",
    "Is 0*bvar(0) = 0 a refinement? Kind of feels that way (if looking up a variable is )\n",
    "0*bvar(0) --> 0\n",
    "It errors out in strictly less contexts.\n",
    "\n",
    "\n",
    "The point of refinment/undef behavior is to enable more rewriting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b421372",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d35b99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kdrag.all import *\n",
    "State = smt.DeclareSort(\"State\")\n",
    "Rel = smt.ArraySort(State, State, smt.BoolSort()) # Pair -> Bool?\n",
    "\n",
    "comp = smt.Function(\".\", Rel, Rel, Rel)\n",
    "kd.notation.matmul.register(Rel, comp)\n",
    "id = smt.Const(\"id\", Rel)\n",
    "R,S,T = smt.Consts(\"R S T\", Rel)\n",
    "id.left = kd.axiom(smt.ForAll([R], id @ R == R))\n",
    "id.right = kd.axiom(smt.ForAll([R], R @ id == R))\n",
    "comp.assoc = kd.axiom(smt.ForAll([R, S, T], (R @ S) @ T == R @ (S @ T)))\n",
    "\n",
    "le = smt.Function(\"<=\", Rel, smt.BoolSort())\n",
    "kd.notation.le.register(Rel, le)\n",
    "\n",
    "le.trans = kd.axiom(kd.QForAll([R, S, T], le(R, S) & le(S, T), le(R, T)))\n",
    "le.refl = kd.axiom(kd.QForAll([R], le(R, R)))\n",
    "le.antisym = kd.axiom(kd.QForAll([R, S],\n",
    "    le(R, S) & le(S, R) >> (R == S)))\n",
    "\n",
    "# I could define them, but yolo\n",
    "\n",
    "Set = smt.ArraySort(State, smt.BoolSort())\n",
    "dom = smt.Function(\"dom\", Rel, Set)\n",
    "cod = smt.Function(\"cod\", Rel, Set)\n",
    "\n",
    "conv = smt.Function(\"conv\", Rel, Rel)\n",
    "conv.id = kd.axiom(smt.ForAll([R], conv(id) == id))\n",
    "conv.comp = kd.axiom(smt.ForAll([R, S], conv(R @ S) == conv(S) @ conv(R)))\n",
    "conv.inv = kd.axiom(smt.ForAll([R], conv(conv(R)) == R))\n",
    "\n",
    "join = smt.Function(\"join\", Rel, Rel, Rel)\n",
    "kd.notation._or.register(Rel, join)\n",
    "join.comm = kd.axiom(smt.ForAll([R, S], R | S == S | R))\n",
    "join.assoc = kd.axiom(smt.ForAll([R, S, T], (R | S) | T == R | (S | T)))\n",
    "join.le = kd.axiom(smt.ForAll([R, S], R <= R | S))\n",
    "join.univ = kd.axiom(smt.ForAll([R,S,T], ((R | S) <= T) == (R <= T) & (S <= T)))\n",
    "\n",
    "top = smt.Const(\"top\", Rel)\n",
    "bot = smt.Const(\"bot\", Rel)\n",
    "\n",
    "const = smt.Function(\"const\", State, Rel)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de99795d",
   "metadata": {},
   "source": [
    "# Boolean Valued Models\n",
    "\n",
    "Boolean valued models https://jdh.hamkins.org/a-gentle-introduction-to-boolean-valued-model-theory/\n",
    "a version of congurence closure with inequalites\n",
    "`[[t0 = f(s)]] /\\ [[t1 = f(x)]] <= [[t0 = t1]]``\n",
    "\n",
    "If the set / boolean algebra in question is a proof set? \n",
    "\n",
    "Is this true of separation logic or tla?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42c04c3",
   "metadata": {},
   "source": [
    "# Unification Subsumption and \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096b31da",
   "metadata": {},
   "source": [
    "# Setlog and set constraints\n",
    "See clp_set notes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542bea6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fccbadd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/stuff.pl\n"
     ]
    }
   ],
   "source": [
    "%%file /tmp/stuff.pl\n",
    "\n",
    "hello(\"world\").\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "179a04c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[w,o,r,l,d]"
     ]
    }
   ],
   "source": [
    "! scryer-prolog /tmp/stuff.pl -g 'hello(X), write(X), halt.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "76a685d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /tmp/minikan.rkt\n"
     ]
    }
   ],
   "source": [
    "%%file /tmp/minikan.rkt\n",
    "#lang racket\n",
    "(require minikanren)\n",
    "\n",
    "(run* (q) (== q 'hello-world))\n",
    "(run* (q) (fresh (x) (== x 'hello) (== q x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a98e400f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'(hello-world)\n",
      "'(hello)\n"
     ]
    }
   ],
   "source": [
    "! racket /tmp/minikan.rkt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc368f6b",
   "metadata": {},
   "source": [
    "# Refinement Closure\n",
    "---\n",
    "title: Congruence Closure becomes Refinement Closure for Refinement E-graphs\n",
    "---\n",
    "Using the term E-graph maybe is a bit of a misnomer.\n",
    "\n",
    "Even in refinement reasoning, equalities are pretty prevalent.\n",
    "\n",
    "Congruence closure generalizes to refinement closure\n",
    "\n",
    "cong close saturates\n",
    "x = y -> f(x) = f(y)\n",
    "\n",
    "\n",
    "ref closure saturates\n",
    "x <= y -> f(x) <= f(y) for monotonic\n",
    "x <= y -> f(y) <= f(x) for anti-monotonic\n",
    "\n",
    "\n",
    "x0 <=^p0 y0, x1 <=^p1 y1, ... -> f(x0,x1,x2,...) <= f(y0, y1, y2,...)\n",
    "\n",
    "\n",
    "Where pn are the polarity of position n in the function symbol f.\n",
    "\n",
    "Arrow in subtyping has polarity signature\n",
    "arr(-,+)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "parents and upper are similar beasts kind of.\n",
    "\n",
    "Is there a relationship between the dpeendent egraph and refinement egraph. Types kind of givey ou a handle on partiality.\n",
    "\n",
    "\n",
    "Yea, maybe this is a point towards egglog.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858f3cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class REGraph():\n",
    "    def __init__(self):\n",
    "        self.enodes = {}\n",
    "\n",
    "    def rebuild(self):\n",
    "        # The quadatic loop version\n",
    "        for enode1 in enodes:\n",
    "            for node2 in enodes:\n",
    "                if all(self.uf.is_le(x,y) in zip(enode.args, enodes1.args)):\n",
    "                    self.uf.assert_le(enode1, enode2)\n",
    "        \n",
    "        for enode1 in enodes:\n",
    "            # search through all upper sets of args\n",
    "            # we can prune though things with smallest args set\n",
    "            # indeed, good joins cure a lot of wounds.\n",
    "        \n",
    "        # alternative: Fill out the upper set of args\n",
    "        for enode in enodes:\n",
    "            for a in self.uf.le_set(enode.args[0]):\n",
    "                e1 = self.enodes.get_make(enode with a)\n",
    "                self.uf.assert_le(a, e1)\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a111c2",
   "metadata": {},
   "source": [
    "# Datalog Model\n",
    "Datalog / Relational / Flattened model\n",
    "\n",
    "Egglog also has datalog model. brute force eq(x,y) relation with symmettry and transitivity moves.\n",
    "\n",
    "Refinement closure can either brute fore\n",
    "\n",
    "```\n",
    "f(x,y) ~  f(x,y,res) relation\n",
    "\n",
    "% refinement closure\n",
    "\n",
    "% don't generate new symbols\n",
    " le(z,z1) :- f(x, z), le(x, y), f(y, z1)\n",
    "\n",
    "% Do generate new f ids\n",
    "set f(y) <= z :- f(x) = z, x <= y\n",
    "\n",
    "```\n",
    "\n",
    "Generalized rewriting coq\n",
    "FOLDS.\n",
    "Lessons that baking in boring congruence may not be what you want. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed5da8b",
   "metadata": {},
   "source": [
    "# Ground Ordered Chaining\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf6ad20",
   "metadata": {},
   "outputs": [],
   "source": [
    "type le = list[tuple[object,object]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9b7dee",
   "metadata": {},
   "source": [
    "## Chain UF\n",
    "\n",
    "The idea of an approximate LE via carefully picking how parents work is interesting.\n",
    "Is there a domain where this can be thought of as precise? Not general partial order.\n",
    "Maybe it can be precise if we guarantee a maximum width?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e2c421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do you best to try and store good chains. https://en.wikipedia.org/wiki/Partially_ordered_set#Derived_notions\n",
    "# chains are kind of like parent\n",
    "class LEFind():\n",
    "  chain0 : list[int]\n",
    "  chain1 : list[int]\n",
    "\n",
    "  def union(self, a, b): ...\n",
    "    # seek lowest common ancestor?\n",
    "\n",
    "  # .. chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff3d110",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac9ce36e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63584077",
   "metadata": {},
   "source": [
    "# biunify kata\n",
    "\n",
    "Everything I can do with terms\n",
    "- matching\n",
    "- unification\n",
    "- egraphs\n",
    "- knuth bendix\n",
    "- subterm\n",
    "\n",
    "\n",
    "Can i extend to \"bi\" versions?\n",
    "\n",
    "- BiProlog . BiKanren\n",
    "\n",
    "Based on the LEfind, should bi substutuitions be done as\n",
    "{eq : , upper : [] , lower : []}\n",
    "Presumably the equality case is more common (?) so should be pulled out. \n",
    "`(<= a b)` in minikanren would branch or assert?\n",
    "bi is the analog of e-unification in many respects.\n",
    "So look more towards FLP for inspiration?\n",
    "\n",
    "Would LEFind be useful for a CLP(Set)?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6876024c",
   "metadata": {},
   "outputs": [],
   "source": [
    "type bisubst = tuple[dict,dict]\n",
    "def apply(pat, bisubst): ...\n",
    "def bimatch(t, pat): ...\n",
    "def biunify(): ...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6752befc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kdrag.all import *\n",
    "def le_unify(t1, t2): \n",
    "    upsubst : dict[object, list[object]]= {} # nondet substitution\n",
    "    downsubst = {}\n",
    "    eqsubst : dict[object, object] = {} # equality substitution\n",
    "\n",
    "    todo = [[(t1,t2)]]\n",
    "    while todo:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0636b356",
   "metadata": {},
   "source": [
    "# smt egraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39dc7ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "@dataclass\n",
    "class LEFind():\n",
    "    parents : dict\n",
    "    upper : defaultdict(set)\n",
    "    lower : defaultdict(set)\n",
    "    def __init__(self):\n",
    "        self.parents = {}\n",
    "        self.upper = defaultdict(set)\n",
    "        self.lower = defaultdict(set)\n",
    "    def assert_le(self, x, y): # assert to LEFind that x <= y\n",
    "        x,y = self.find(x), self.find(y)\n",
    "        if x == y:\n",
    "            return\n",
    "        self.upper[x].add(y)\n",
    "        self.lower[y].add(x)\n",
    "        if self.is_le(y, x): # propagate antisymmettry x <= y and y <= x implies x == y\n",
    "            self.union(x, y)\n",
    "            for z in self.le_set(x) & self.ge_set(y): # anything between the two is squeezed\n",
    "                self.union(z, y)\n",
    "            for z in self.le_set(y) & self.ge_set(x): # anything between the two is squeezed. Is this redundant?\n",
    "                self.union(z, x)\n",
    "    def assert_ge(self, x, y): # assert to LEFind that x >= y\n",
    "        self.assert_le(y, x)\n",
    "    def union(self, x, y): # assert that x == y\n",
    "        x, y = self.find(x), self.find(y)\n",
    "        if x != y:\n",
    "            self.parents[x] = y # refular union find\n",
    "            self.upper[y].update(self.upper[x]) # merge upper sets\n",
    "            self.lower[y].update(self.lower[x]) # merge lower sets\n",
    "    def find(self, x : int) -> int:\n",
    "        while x in self.parents:\n",
    "            x = self.parents[x]\n",
    "        return x\n",
    "    # The next 3 functions are very similar. is_le can early stop when it hits y.\n",
    "    def is_le(self, x, y) -> bool:\n",
    "        # DFS search for y in upper set of x\n",
    "        x,y = self.find(x), self.find(y)\n",
    "        if x == y:\n",
    "            return True\n",
    "        todo = [x]\n",
    "        seen = set(todo)\n",
    "        while todo:\n",
    "            x = todo.pop() # invariant is that x is already representative\n",
    "            for z in self.upper[x]:\n",
    "                # Is there a way to use lower set for pruning?\n",
    "                z = self.find(z)   # compression could be updating z in place in upper[x]\n",
    "                if z == y:\n",
    "                    return True\n",
    "                elif z not in seen:\n",
    "                    seen.add(z)\n",
    "                    todo.append(z)\n",
    "        return False\n",
    "    def le_set(self, x) -> set[int]: # all solutions to x <= ?\n",
    "        x = self.find(x)\n",
    "        todo = [x]\n",
    "        seen = set(todo)\n",
    "        while todo:\n",
    "            x = todo.pop()\n",
    "            for z in self.upper[x]:\n",
    "                z = self.find(z)\n",
    "                if z not in seen:\n",
    "                    seen.add(z)\n",
    "                    todo.append(z)\n",
    "        return seen\n",
    "    def ge_set(self, x) -> set[int]: # all solutions to x >= ?\n",
    "        x = self.find(x)\n",
    "        todo = [x]\n",
    "        seen = set(todo)\n",
    "        while todo:\n",
    "            x = todo.pop()\n",
    "            for z in self.lower[x]:\n",
    "                z = self.find(z)\n",
    "                if z not in seen:\n",
    "                    seen.add(z)\n",
    "                    todo.append(z)\n",
    "        return seen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a86b74",
   "metadata": {},
   "source": [
    "Hmm. You know, if there is no further interpretation of le, Having the solver backed version does not help anything.\n",
    "\n",
    "FALSE. Kind of. Refinement closure. But we can't express monotonicty to smt without using quantiufiers.\n",
    "\n",
    "forall x <= y -> f(x) <= f(y)\n",
    "\n",
    "There is also an issue that I don't really know how to encode a model of undefinedness into SMT.\n",
    "The relational model? That's more like the contextual egraph right?\n",
    "The option model?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3be81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kdrag.solvers.egraph import EGraph\n",
    "\n",
    "class LEGraph(EGraph):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lefind = LEFind()\n",
    "        self.le_prim = smt.Function(\"le\", smt.IntSort(), smt.IntSort(), smt.BoolSort())\n",
    "        self.le = smt.TransitiveClosure(self.le_prim)\n",
    "    \n",
    "    def assert_le(self, x, y):\n",
    "        self.lefind.assert_le(x, y)\n",
    "        self.solver.add(self.le_prim(x, y))\n",
    "\n",
    "    def is_le(self, x, y):\n",
    "        return self.lefind.is_le(x, y)\n",
    "        with self.solver:\n",
    "            self.solver.add(smt.Not(self.le(x, y)))\n",
    "            return self.solver.check() == smt.unsat\n",
    "    def le_set(self, x):\n",
    "        return self.lefind.le_set(x)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6559ece1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EGraph():\n",
    "    lefind : LEFind\n",
    "    enodes : dict\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "philzook58.github.io",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
